{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 2.7.6\n",
      "IPython 1.2.1\n",
      "\n",
      "ete2 2.2rev1056\n",
      "biopython 1.64\n",
      "dendropy 3.12.0\n",
      "cloud 2.8.5\n",
      "numpy 1.8.2\n",
      "matplotlib 1.3.1\n",
      "\n",
      "compiler   : GCC 4.8.2\n",
      "system     : Linux\n",
      "release    : 3.13.0-36-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n",
      "host name  : amir-TECRA-W50-A\n",
      "Git hash   : aa0f118a7bb5ed880a175c6e1c5ee358f84378a6\n"
     ]
    }
   ],
   "source": [
    "%watermark -g -h -m -v -p ete2,biopython,dendropy,cloud,numpy,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reprophylo.py\n"
     ]
    }
   ],
   "source": [
    "%%file reprophylo.py\n",
    "\n",
    "##############################################################################################\n",
    "if False:\n",
    "    \"\"\"\n",
    "    ReproPhylo version 0.1 \n",
    "    \n",
    "    General purpose phylogenetics package for reproducible and experimental analysis\n",
    "    \n",
    "    Amir Szitenebrg\n",
    "    A.Szitenberg@Hull.ac.uk\n",
    "    Szitenberg@gmail.com\n",
    "    \n",
    "    David H Lunt\n",
    "    D.H.Lunt@Hull.ac.uk\n",
    "    \n",
    "    EvoHull.org\n",
    "    University of Hull\n",
    "    \n",
    "    \n",
    "    Developed with:\n",
    "    CPython 2.7.6\n",
    "    IPython 1.2.1\n",
    "    ete2 2.2rev1056\n",
    "    biopython 1.64\n",
    "    dendropy 3.12.0\n",
    "    cloud 2.8.5\n",
    "    numpy 1.8.2\n",
    "    matplotlib 1.3.1\n",
    "    pandas\n",
    "    \n",
    "    RAxML 8\n",
    "    Phylobayes\n",
    "    Trimal\n",
    "    Muscle\n",
    "    Mafft\n",
    "    Pal2nal\n",
    "    \"\"\"\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "from Bio import SeqIO\n",
    "import os, csv, sys, dendropy, re, time, random, glob, platform, warnings, rpgit, ast, gb_syn\n",
    "import HTML, inspect, shutil\n",
    "import subprocess as sub\n",
    "#import cloud.serialization.cloudpickle as pickle\n",
    "from Bio.Seq import Seq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio.Alphabet import IUPAC\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation, CompoundLocation\n",
    "from Bio.Align.Applications import MafftCommandline, MuscleCommandline\n",
    "from StringIO import StringIO \n",
    "from Bio import AlignIO \n",
    "from Bio.Phylo.Applications import RaxmlCommandline\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.SeqUtils import GC\n",
    "from ete2 import *\n",
    "from collections import Counter\n",
    "#import pandas as pd\n",
    "import math\n",
    "import __builtin__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "class Locus:\n",
    "##############################################################################################\n",
    "\n",
    "    \"\"\" Configure the loci stored in the ReproPhylo Project.\n",
    "        \n",
    "    >>> locus = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "    >>> print(locus)\n",
    "    Locus(char_type=dna, feature_type=CDS, name=coi, aliases=cox1; COX1; coi; COI; CoI)\n",
    "    \"\"\"\n",
    "\n",
    "    char_type = 'NotSet'\n",
    "    feature_type = 'NotSet'\n",
    "    name = 'NotSet'\n",
    "    aliases = []\n",
    "\n",
    "    def __init__(self, char_type=char_type, feature_type=feature_type,\n",
    "                 name=name, aliases=aliases):\n",
    "\n",
    "        \n",
    "        self.char_type = char_type\n",
    "        self.feature_type = feature_type\n",
    "        self.name = name\n",
    "        self.aliases = aliases\n",
    "        \n",
    "        valid = ['dna','prot']\n",
    "        if not self.char_type in valid:\n",
    "            raise ValueError('self.char_type should be \\'dna\\' or \\'prot\\'')\n",
    "        if not type(self.feature_type) is str:\n",
    "            raise ValueError('self.feature_type should be a string')\n",
    "        if not type(self.name) is str:\n",
    "            raise ValueError('self.name should be a string')\n",
    "        if not type(self.aliases) is list:\n",
    "            raise ValueError('self.aliases should be a list')\n",
    "        else:\n",
    "            for a in self.aliases:\n",
    "                if not type(a) is str:\n",
    "                    raise ValueError('aliases in self.aliases have to be strings')\n",
    "            \n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        aliases_str = ('; ').join(self.aliases)\n",
    "        return ('Locus(char_type='+self.char_type+', feature_type='+self.feature_type+\n",
    "                ', name='+self.name+', aliases='+aliases_str+')')\n",
    "\n",
    "    \n",
    "    \n",
    "##############################################################################################\n",
    "class Concatenation:\n",
    "##############################################################################################\n",
    "\n",
    "    \"\"\"This class is used to configure concatenations given loci and rules.\n",
    "    \n",
    "    >>> coi = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "    >>> ssu = Locus('dna', 'rRNA', '18S', ['18S rRNA','SSU rRNA'])\n",
    "    >>> bssu = Locus('dna', 'rRNA', '16S', ['16S rRNA'])\n",
    "    >>> lsu = Locus('dna', 'rRNA', '28S', ['28S rRNA', 'LSU rRNA'])\n",
    "    >>> alg11 = Locus('dna', 'CDS', 'ALG11', ['ALG11'])\n",
    "    >>> loci = [coi, ssu, bssu, lsu, alg11]\n",
    "    >>> concatenation = Concatenation(name='combined', loci=loci,\n",
    "    ...                                otu_meta='OTU_name',\n",
    "    ...                                otu_must_have_all_of=['coi'],\n",
    "    ...                                otu_must_have_one_of =[['16S','28S'],['ALG11','18S']],\n",
    "    ...                                define_trimmed_alns=[\"MuscleDefaults@dummyTrimMethod\"])\n",
    "    >>> print(str(concatenation))\n",
    "    Concatenation named combined, with loci coi,18S,16S,28S,ALG11,\n",
    "    of which coi must exist for all species\n",
    "    and at least one of each group of [ 16S 28S ][ ALG11 18S ] is represented.\n",
    "    Alignments with the following names: MuscleDefaults@dummyTrimMethod are prefered\n",
    "    \"\"\"\n",
    "    \n",
    "    otu_must_have_all_of = []\n",
    "    otu_must_have_one_of = 'any'\n",
    "    define_trimmed_alns = [] #should be Locus_name@Alignment_method_name@Trimming_mathod_name\n",
    "    \n",
    "    feature_id_dict = {}\n",
    "    \n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 loci,\n",
    "                 otu_meta,\n",
    "                 otu_must_have_all_of = otu_must_have_all_of,\n",
    "                 otu_must_have_one_of = otu_must_have_one_of,\n",
    "                 define_trimmed_alns = define_trimmed_alns):\n",
    "        self.name = name\n",
    "        self.loci = loci\n",
    "        self.otu_meta = otu_meta\n",
    "        self.otu_must_have_all_of = otu_must_have_all_of\n",
    "        self.otu_must_have_one_of = otu_must_have_one_of\n",
    "        if isinstance(otu_must_have_one_of[0],str) and not otu_must_have_one_of == 'any':\n",
    "            raise IOError('The keyword \\'otu_must_have_one_of\\' has to be a list of lists')\n",
    "        if self.otu_must_have_one_of == 'any':\n",
    "            self.otu_must_have_one_of = [[l.name for l in self.loci]]\n",
    "        self.feature_id_dict = {}\n",
    "        self.define_trimmed_alns = define_trimmed_alns\n",
    "        self.used_trimmed_alns = {}\n",
    "        seen = []\n",
    "        for locus in loci:\n",
    "            if not isinstance(locus, Locus):\n",
    "                raise TypeError(\"Expecting Locus object in loci list\")\n",
    "            if locus.name in seen:\n",
    "                raise NameError('Locus ' + locus.name + ' apears more than once in self.loci')\n",
    "            else:\n",
    "                seen.append(locus.name)\n",
    "      \n",
    "                \n",
    "                \n",
    "    def __str__(self):\n",
    "        loci_names = [i.name for i in self.loci]\n",
    "        loci_string = ''\n",
    "        for l in loci_names:\n",
    "            loci_string += l+','\n",
    "        loci_string = loci_string[:-1]\n",
    "        must_have = ''\n",
    "        for i in self.otu_must_have_all_of:\n",
    "            must_have += i+','\n",
    "        must_have = must_have[:-1]\n",
    "        trimmed_alignmnets_spec = ''\n",
    "        one_of = ''\n",
    "        for i in self.otu_must_have_one_of:\n",
    "            one_of += '[ '\n",
    "            for j in i:\n",
    "                one_of += j+' '\n",
    "            one_of += ']'\n",
    "        if (self.define_trimmed_alns) > 0:\n",
    "            for i in self.define_trimmed_alns:\n",
    "                trimmed_alignmnets_spec += i\n",
    "        return (\"Concatenation named %s, with loci %s,\\n\"\n",
    "                \"of which %s must exist for all species\\n\"\n",
    "                \"and at least one of each group of %s is represented.\\n\"\n",
    "                \"Alignments with the following names: %s are prefered\"\n",
    "                % (self.name, loci_string, must_have, one_of, trimmed_alignmnets_spec))\n",
    "        \n",
    "        \n",
    "        \n",
    "##############################################################################################\n",
    "if False:\n",
    "    \"\"\"\n",
    "    Reprophylo Project Utilities\n",
    "    \n",
    "    Used in the Project class but are not in the classe's methods\n",
    "    \"\"\"\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "__builtin__.git = False\n",
    "\n",
    "\n",
    "\n",
    "def start_git():\n",
    "    __builtin__.git = True\n",
    "    rpgit.gitInit()\n",
    "    cwd = os.getcwd()\n",
    "    import fnmatch\n",
    "    matches = []\n",
    "    for root, dirnames, filenames in os.walk(cwd):\n",
    "        for filename in fnmatch.filter(filenames, '*.py'):\n",
    "            matches.append(os.path.join(root, filename))\n",
    "        for filename in fnmatch.filter(filenames, '*.ipynb'):\n",
    "            matches.append(os.path.join(root, filename))\n",
    "    for match in matches:\n",
    "        rpgit.gitAdd(match)\n",
    "    comment = \"%i script file(s) from %s\" % (len(matches), time.asctime())\n",
    "    rpgit.gitCommit(comment)\n",
    "    \n",
    "    \n",
    "    \n",
    "def stop_git():\n",
    "    __builtin__.git = False\n",
    "    cwd = os.getcwd()\n",
    "    import fnmatch\n",
    "    matches = []\n",
    "    for root, dirnames, filenames in os.walk(cwd):\n",
    "        for filename in fnmatch.filter(filenames, '*.py'):\n",
    "            matches.append(os.path.join(root, filename))\n",
    "        for filename in fnmatch.filter(filenames, '*.ipynb'):\n",
    "            matches.append(os.path.join(root, filename))\n",
    "    for match in matches:\n",
    "        rpgit.gitAdd(match)\n",
    "    comment = \"%i script file(s) from %s\" % (len(matches), time.asctime())\n",
    "    rpgit.gitCommit(comment)\n",
    "    \n",
    "\n",
    "\n",
    "def platform_report():\n",
    "    \n",
    "    \"\"\" \n",
    "    Prints machine specs, os specs and dependencies at time of execution\n",
    "    \n",
    "    >>> isinstance(platform_report(), list)\n",
    "    True\n",
    "    \"\"\"\n",
    "    import pkg_resources\n",
    "    modules = []\n",
    "    for i in ('ete2','biopython','dendropy','cloud'):\n",
    "        try:\n",
    "            modules.append(i+' version: '+\n",
    "                                pkg_resources.get_distribution(i).version)\n",
    "        except:\n",
    "            pass\n",
    "    return(['Platform: '+platform.platform(aliased=0, terse=0),\n",
    "            'Processor: '+platform.processor(),\n",
    "            'Python build: '+platform.python_build()[0] + platform.python_build()[1],\n",
    "            'Python compiler: '+platform.python_compiler(),\n",
    "            'Python implementation: ' +platform.python_implementation(),\n",
    "            'Python version: ' + platform.python_version()]+\n",
    "             modules+\n",
    "            ['User: ' +platform.uname()[1]])\n",
    "\n",
    "\n",
    "\n",
    "def write_alns(pj, format = 'fasta'):\n",
    "    \"\"\"\n",
    "    Writes untrimmed sequence alignment files that are in pj in a biopython format\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(pj.alignments.keys()) == 0:\n",
    "        raise IOError('Align the records first')\n",
    "    else:\n",
    "        for key in pj.alignments:\n",
    "            AlignIO.write(pj.alignments[key], key+'_aln.'+format, format)\n",
    "\n",
    "\n",
    "\n",
    "def keep_feature(feature, loci):\n",
    "    \n",
    "    \"\"\" Returns true if a feature's type is in one of the loci and if the gene\n",
    "    or product qualifiers is in the aliases of one of the loci\n",
    "    \n",
    "    # making a dummy feature\n",
    "    >>> coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "    >>> location = FeatureLocation(1,100)\n",
    "    >>> feature = SeqFeature()\n",
    "    >>> feature.location = location\n",
    "    >>> feature.type = 'CDS'\n",
    "    >>> feature.qualifiers['gene'] = ['CoI']\n",
    "    \n",
    "    # testing if fits any of the Project Locus objects\n",
    "    >>> a = keep_feature(feature, [coi])\n",
    "    >>> print(a)\n",
    "    True\"\"\"\n",
    "    \n",
    "    keep = 0\n",
    "    for g in loci:\n",
    "        if not g.name in g.aliases:\n",
    "            g.aliases.append(g.name)\n",
    "        if feature.type == 'source':\n",
    "            keep = 1\n",
    "        elif feature.type == g.feature_type:\n",
    "            qual = None\n",
    "            if 'gene' in feature.qualifiers.keys():\n",
    "                qual = 'gene'\n",
    "            elif 'product' in feature.qualifiers.keys():\n",
    "                qual = 'product'\n",
    "            if qual and feature.qualifiers[qual][0] in g.aliases:\n",
    "                keep = 1\n",
    "    if keep == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "  \n",
    "\n",
    "\n",
    "def dwindle_record(record, loci):\n",
    "    \n",
    "    \"\"\" \n",
    "    Retains only features that are called by Locus objects and records with features that are\n",
    "    called by Locus objects\n",
    "    \n",
    "    # Making a dummy locus    \n",
    "    >>> coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "    \n",
    "    # Making a dummy record with a feature that fits a Locus object (kept_feature)\n",
    "    # and a feature that does not (dwindled_feature)\n",
    "    >>> location = FeatureLocation(1,100)\n",
    "    >>> kept_feature = SeqFeature()\n",
    "    >>> kept_feature.location = location\n",
    "    >>> kept_feature.type = 'CDS'\n",
    "    >>> kept_feature.qualifiers['gene'] = ['CoI']\n",
    "    >>> dwindled_feature = SeqFeature()\n",
    "    >>> dwindled_feature.location = location\n",
    "    >>> dwindled_feature.type = 'rRNA'\n",
    "    >>> dwindled_feature.qualifiers['gene'] = ['LSU']\n",
    "    >>> s = 'atgc'*1000\n",
    "    >>> record = SeqRecord(seq=Seq(s, IUPAC.ambiguous_dna), id='1', description='spam')\n",
    "    >>> record.features.append(kept_feature)\n",
    "    >>> record.features.append(dwindled_feature)\n",
    "    >>> print(len(record.features))\n",
    "    2\n",
    "    \n",
    "    # Dwindling the record\n",
    "    >>> a = dwindle_record(record, [coi])\n",
    "    >>> print(len(record.features))\n",
    "    1\n",
    "    \"\"\"\n",
    "    \n",
    "    dwindled_features = []\n",
    "    feature_count = 0\n",
    "    for feature in record.features:\n",
    "        if keep_feature(feature, loci)== True:\n",
    "            if feature.type == 'source' and not 'feature_id' in feature.qualifiers.keys():\n",
    "                feature.qualifiers['feature_id'] = [record.id + '_source']\n",
    "            elif not 'feature_id' in feature.qualifiers.keys():\n",
    "                feature.qualifiers['feature_id'] = [record.id + '_f' + str(feature_count)]\n",
    "                feature_count += 1\n",
    "            if not feature.type == 'source':\n",
    "                feature_seq = feature.extract(record.seq)\n",
    "                degen = len(feature_seq)\n",
    "                for i in ['A','T','G','C','U','a','t','g','c','u']:\n",
    "                    degen -= feature_seq.count(i)\n",
    "                feature.qualifiers['GC_content'] = [str(GC(feature_seq))]\n",
    "                feature.qualifiers['nuc_degen_prop'] = [str(float(degen)/len(feature_seq))]\n",
    "                if 'translation' in feature.qualifiers.keys():\n",
    "                    transl = feature.qualifiers['translation'][0]\n",
    "                    degen = 0\n",
    "                    for i in ['B', 'X', 'Z', 'b', 'x', 'z']:\n",
    "                        degen += transl.count(i)\n",
    "                    feature.qualifiers['prot_degen_prop'] = [str(float(degen)/len(transl))]                    \n",
    "            dwindled_features.append(feature)\n",
    "    record.features = dwindled_features\n",
    "    return record\n",
    "            \n",
    " \n",
    "    \n",
    "def is_embl_or_gb(input_filename):\n",
    "    suffixes = ['.gb','.embl']\n",
    "    gb = False\n",
    "    for s in suffixes:\n",
    "        if s in input_filename:\n",
    "            gb = True\n",
    "    return gb\n",
    "\n",
    "\n",
    "\n",
    "def parse_input(input_filename, fmt):\n",
    "    return SeqIO.parse(input_filename, fmt)\n",
    "\n",
    "\n",
    "\n",
    "def list_to_string(List):\n",
    "    \n",
    "    \"\"\"\n",
    "    Handles list printing as a nice string in the pj.write(format=\"csv\") method\n",
    "    \n",
    "    >>> L = ['a','b','b']\n",
    "    >>> print(list_to_string(L))\n",
    "    a;b;b\n",
    "    \"\"\"\n",
    "    \n",
    "    string = ''\n",
    "    for i in List:\n",
    "        if type(i) is str and '\\n' in i:\n",
    "            string += lines_to_line(i).rstrip()+';'\n",
    "        else:\n",
    "            string += str(i)+';'\n",
    "    return string[:-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lines_to_line(lines):\n",
    "    \n",
    "    \"\"\"\n",
    "    Replaces newline with space in the pj.write(format=\"csv\") method\n",
    "    \"\"\"\n",
    "    \n",
    "    lines = lines.split('\\n')\n",
    "    return (' ').join(lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def type_to_single_line_str(var):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns any type as a one line string for the pj.write(format=\"csv\") method\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(var) is str and '\\n' in var:\n",
    "        return lines_to_line(var)\n",
    "    elif type(var) is str or type(var) is int or type(var) is float:\n",
    "        return str(var)\n",
    "    elif type(var) is list and len(var) == 1:\n",
    "        return str(var[0])\n",
    "    elif type(var) is list and len(var) > 0:\n",
    "        return list_to_string(var)\n",
    "    else:\n",
    "        return var\n",
    "\n",
    "\n",
    "\n",
    "def get_qualifiers_dictionary(project, feature_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes sequence record annotation, source qualifiers and feature qualifiers and puts them\n",
    "    in a flat dictionary\n",
    "    \n",
    "        \n",
    "    # Making a dummy locus    \n",
    "    >>> coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "    \n",
    "    # Making a dummy Project\n",
    "    >>> pj = Project([coi])\n",
    "    \n",
    "    # making a dummy record\n",
    "    >>> s = 'atgc'*1000\n",
    "    >>> location = FeatureLocation(1,100)\n",
    "    >>> feature = SeqFeature()\n",
    "    >>> feature.location = location\n",
    "    >>> feature.type = 'CDS'\n",
    "    >>> feature.qualifiers['gene'] = ['CoI']\n",
    "    >>> feature.qualifiers['feature_id'] = ['12345']\n",
    "    >>> source = SeqFeature()\n",
    "    >>> source.location = FeatureLocation(0,3999)\n",
    "    >>> source.type = 'source'\n",
    "    >>> source.qualifiers['organism'] = ['Tetillda radiata']\n",
    "    >>> record = SeqRecord(seq=Seq(s, IUPAC.ambiguous_dna), id='1', description='spam')\n",
    "    >>> record.features.append(feature)\n",
    "    >>> record.features.append(source)\n",
    "    >>> record.annotations[\"evidence\"] = 'made up'\n",
    "    >>> pj.records = [record]\n",
    "    \n",
    "    # executing get_qualifiers_dictionary()\n",
    "    >>> qual_dict = get_qualifiers_dictionary(pj, '12345')\n",
    "    >>> qual_items = qual_dict.items()\n",
    "    >>> qual_items.sort(key = lambda i: i[0])\n",
    "    >>> for key, val in qual_items: print(key.ljust(20,' ') + val.ljust(20,' '))\n",
    "    annotation_evidence made up             \n",
    "    feature_id          12345               \n",
    "    gene                CoI                 \n",
    "    source_organism     Tetillda radiata    \n",
    "    \"\"\"\n",
    "    if type(feature_id) is list:\n",
    "        feature_id = feature_id[0]\n",
    "    record_id = feature_id.split('_')[0]\n",
    "    qualifiers_dictionary={}\n",
    "    for record in project.records:\n",
    "        if record.id in feature_id:\n",
    "            for annotation in record.annotations.keys():\n",
    "                qualifiers_dictionary['annotation_'+annotation]=record.annotations[annotation]\n",
    "            for feature in record.features:\n",
    "                if feature.type == 'source':\n",
    "                    for qualifier in feature.qualifiers.keys():\n",
    "                        qualifiers_dictionary['source_'+qualifier]=feature.qualifiers[qualifier][0]\n",
    "                elif feature.qualifiers['feature_id'][0] == feature_id:\n",
    "                    for qualifier in feature.qualifiers.keys():\n",
    "                        qualifiers_dictionary[qualifier]=feature.qualifiers[qualifier][0]\n",
    "    return qualifiers_dictionary\n",
    "\n",
    "def __get_qualifiers_dictionary__(project, feature_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    This will replace the public version. It uses the Project._records_dict to  pull \n",
    "    the record using the record id, instead of iterating Project.records, which is very slow.\n",
    "    \n",
    "    It requires Project.__records_list_to_dict__() to execute beforehand.\n",
    "    \"\"\"\n",
    "    if type(feature_id) is list:\n",
    "        feature_id = feature_id[0]\n",
    "    record_id = feature_id.split('_')[0]\n",
    "    record = project._records_dict[record_id]\n",
    "    qualifiers_dictionary={}\n",
    "    for annotation in record.annotations.keys():\n",
    "        qualifiers_dictionary['annotation_'+annotation]=record.annotations[annotation]\n",
    "    for feature in record.features:\n",
    "        if feature.type == 'source':\n",
    "            for qualifier in feature.qualifiers.keys():\n",
    "                qualifiers_dictionary['source_'+qualifier]=feature.qualifiers[qualifier][0]\n",
    "        elif feature.qualifiers['feature_id'][0] == feature_id:\n",
    "            for qualifier in feature.qualifiers.keys():\n",
    "                qualifiers_dictionary[qualifier]=feature.qualifiers[qualifier][0]\n",
    "    return qualifiers_dictionary\n",
    "\n",
    "def seq_format_from_suffix(suffix):\n",
    "    \n",
    "    \"\"\"\n",
    "    Guesses input format from suffix\n",
    "    \n",
    "    >>> print(seq_format_from_suffix('gb'))\n",
    "    genbank\n",
    "    \"\"\"\n",
    "    \n",
    "    suffixes = {'fasta': ['fas','fasta','fa','fna'],\n",
    "                'genbank': ['gb','genbank'],\n",
    "                'embl': ['embl']}\n",
    "    found = False\n",
    "    for key in suffixes.keys():\n",
    "        if suffix in suffixes[key]:\n",
    "            found = True\n",
    "            return key\n",
    "    if not found:\n",
    "        raise RuntimeError(suffix+' is not a recognised suffix of an unaligned sequence file')\n",
    "\n",
    "\n",
    "\n",
    "def read_feature_quals_from_tab_csv(csv_filename):\n",
    "    import re\n",
    "    header = open(csv_filename, 'r').readlines()[0].rstrip().split('\\t')\n",
    "    feature_id_col = header.index('feature_id')\n",
    "    taxonomy_col = header.index('taxonomy')\n",
    "    seq_col = header.index('seq')\n",
    "    translation_col = None\n",
    "    if 'translation' in header:\n",
    "        translation_col = header.index('translation')\n",
    "    csv_info = {}\n",
    "    for line in [l.rstrip().split('\\t') for l in open(csv_filename, 'r').readlines()[1:]]:\n",
    "        if not line[0] in csv_info.keys():\n",
    "            csv_info[line[0]] = {'source':{},\n",
    "                                 'taxonomy':[],\n",
    "                                 'features':{}\n",
    "                                 }\n",
    "        if csv_info[line[0]]['taxonomy'] == []:\n",
    "            csv_info[line[0]]['taxonomy'] = line[taxonomy_col].split(';')\n",
    "        csv_info[line[0]]['features'][line[feature_id_col]] = {}\n",
    "        get_source = False\n",
    "        if csv_info[line[0]]['source'] == {}:\n",
    "            get_source = True\n",
    "        for i in range(len(header)):\n",
    "            if get_source and 'source:_' in header[i]:\n",
    "                qual_name = re.sub('source:_','',header[i])\n",
    "                if not line[i] == 'null' and not line[i] == '':\n",
    "                    csv_info[line[0]]['source'][qual_name] = line[i].split(';')\n",
    "            elif (not 'source:_' in header[i] and not line[i] == 'null' and not line[i] == '' and\n",
    "                  not i in [seq_col, translation_col, taxonomy_col, feature_id_col]):\n",
    "                csv_info[line[0]]['features'][line[feature_id_col]][header[i]] = line[i].split(';')\n",
    "    return csv_info\n",
    "\n",
    "def count_positions(aln_column):\n",
    "    counts = {}\n",
    "    for i in aln_column:\n",
    "        if i in counts.keys():\n",
    "            counts[i] += 1\n",
    "        else:\n",
    "            counts[i] = 1\n",
    "    return counts\n",
    "\n",
    "def global_aln_stats(aln_obj):\n",
    "    total_gaps = 0\n",
    "    prop_list = []\n",
    "    non_uniform_count = aln_obj.get_alignment_length()\n",
    "    parsimony_informative = 0\n",
    "    for i in range(aln_obj.get_alignment_length()):\n",
    "        total_gaps += aln_obj[:, i].count('-')\n",
    "        prop_list.append(aln_obj[:, i].count('-')/float(len(aln_obj)))\n",
    "        if len(count_positions(aln_obj[:, i]).keys()) == 1:\n",
    "            non_uniform_count -= 1\n",
    "        elif (len(count_positions(aln_obj[:, i]).keys()) == 2 and\n",
    "              '-' in  count_positions(aln_obj[:, i]).keys()):\n",
    "            non_uniform_count -= 1\n",
    "        if len([p for p in count_positions(aln_obj[:, i]).keys() if (p != '-' and  count_positions(aln_obj[:, i])[p] > 1)]) > 1:\n",
    "            parsimony_informative += 1\n",
    "    mean_gap_prop = sum(prop_list)/aln_obj.get_alignment_length()\n",
    "    return (mean_gap_prop, non_uniform_count, parsimony_informative)\n",
    "\n",
    "def count_undetermined_lines(aln_obj, cutoff=0):\n",
    "    count = 0\n",
    "    ids = []\n",
    "    \n",
    "    if aln_obj.get_alignment_length() < cutoff*2:\n",
    "        warnings.warn('The cutoff to exclude a sequence is more than half of the alignmnet length')\n",
    "    elif aln_obj.get_alignment_length() <= cutoff:\n",
    "        raise RuntimeWarning('The cutoff to exclude a sequence is as long or longer than the alignment')\n",
    "    \n",
    "    for seq in aln_obj:\n",
    "        if str(seq.seq).count('-') >= aln_obj.get_alignment_length()-cutoff:\n",
    "            count += 1\n",
    "            ids.append(seq.id)\n",
    "    return count, ids\n",
    "\n",
    "def count_collapsed_aln_seqs(aln_obj):\n",
    "    count = 1\n",
    "    seen_seqs = [str(aln_obj[0].seq)]\n",
    "    for seq in aln_obj[1:]:\n",
    "        str_seq = str(seq.seq)\n",
    "        if len([s for s in seen_seqs if (str_seq in s or s in str_seq or s == str_seq)]) == 0:\n",
    "            count += 1\n",
    "        seen_seqs.append(str_seq)\n",
    "    return count\n",
    "\n",
    "def aln_summary(aln_obj, cutoff=0):\n",
    "    lines = [\"Alignment length: %i\" % aln_obj.get_alignment_length(),\n",
    "             \"Number of rows: %i\" % len(aln_obj),\n",
    "             \"Unique sequences: %i\"%count_collapsed_aln_seqs(aln_obj),\n",
    "             \"Average gap prop.: %f\\nVariable columns: %i\\nParsimony informative: %i\"\n",
    "             %global_aln_stats(aln_obj),\n",
    "             \"Undetermined sequences: %i\"%(count_undetermined_lines(aln_obj, cutoff=cutoff)[0]),\n",
    "             \"Undetermined sequence cutoff: %i\"%cutoff\n",
    "             ]\n",
    "    return [lines, len(aln_obj), count_undetermined_lines(aln_obj, cutoff=cutoff), count_collapsed_aln_seqs(aln_obj)]        \n",
    "            \n",
    "\n",
    "def loci_list_from_csv(loci):\n",
    "    if any(len(line.split(',')) >= 4 for line in open(loci, 'r').readlines()):\n",
    "        pass\n",
    "    else:\n",
    "        raise IOError(\"File %s has no valid loci of format char_type,feature_type,name,aliases\"%loci)\n",
    "        \n",
    "        \n",
    "    loci_dict = {}\n",
    "    loci_list = []\n",
    "    for line in [line.rstrip() for line in open(loci, 'r').readlines() if len(line.rstrip()) > 0]:\n",
    "        if len(line.split(',')) < 4:\n",
    "            raise IOError(\"The line %s in file %s is missing arguments. Needs at least char_type,feature_type,name,aliases\"%\n",
    "                          (line.rstrip(), loci))\n",
    "        else:\n",
    "            group = None\n",
    "            try:\n",
    "                group = int(line.rstrip().split(',')[-1])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if group:\n",
    "                locus_exists = False\n",
    "                for name in loci_dict:\n",
    "                    if 'group' in loci_dict[name].keys() and loci_dict[name]['group'] == group:\n",
    "                        loci_dict[name]['aliases'] += line.split(',')[3:-1]\n",
    "                        locus_exists = True\n",
    "                if not locus_exists:\n",
    "                    loci_dict[line.split(',')[2]] = {'group': int(line.rstrip().split(',')[-1]),\n",
    "                                                     'char_type': line.split(',')[0],\n",
    "                                                     'feature_type': line.split(',')[1],\n",
    "                                                     'aliases': line.split(',')[3:-1]\n",
    "                                                     }\n",
    "            else:\n",
    "                loci_dict[line.split(',')[2]] = {'group': None,\n",
    "                                                 'char_type': line.split(',')[0],\n",
    "                                                 'feature_type': line.split(',')[1],\n",
    "                                                 'aliases': line.split(',')[3:]\n",
    "                                                 }\n",
    "                \n",
    "            \n",
    "            \n",
    "    for name in loci_dict:\n",
    "        loci_list.append(Locus(loci_dict[name]['char_type'],\n",
    "                               loci_dict[name]['feature_type'],\n",
    "                               name,\n",
    "                               loci_dict[name]['aliases']))\n",
    "    return loci_list\n",
    "\n",
    "def parse_paup_charset(nexus_filename):\n",
    "    \n",
    "    \"\"\" \n",
    "    Takes a nexus file with PAUP style charset commands.\n",
    "    Returns a dictionary with partition names as keys and a list of\n",
    "    integers representing the start and end of the partition as a value.\n",
    "    Position count starts from 0.\n",
    "    \n",
    "    Handles paup commands of the following format:\n",
    "    CHARSET  locus_name=1-129;\n",
    "    or\n",
    "    charset locus_name = 1 - 129 ;\n",
    "    \"\"\"\n",
    "    \n",
    "    charsets = {}\n",
    "    charset_lines = [l for l in open(nexus_filename,'r').readlines() if \n",
    "                     (l.startswith('CHARSET') or l.startswith('charset'))]\n",
    "    if len(charset_lines) == 0:\n",
    "        raise IOError(\"There are no CHARSET commands in %s\"%nexus_filename)\n",
    "    for line in charset_lines:\n",
    "        try:\n",
    "            info = line.split()[1].split(';')[0]\n",
    "            locus_name, range = info.split('=')\n",
    "            locus_name = locus_name.strip().rstrip()\n",
    "            start = int(range.split('-')[0].strip().rstrip())-1\n",
    "            end = int(range.split('-')[1].strip().rstrip())-1\n",
    "            charsets[locus_name] = [start,end]\n",
    "        except:\n",
    "            raise IOError('Expects \"charset set_name = start_int - end_int;\"'+\n",
    "                          ' (case insensitive, spaces around the \"=\" or \"-\" not mandatory). Got %s'%line)\n",
    "    return charsets\n",
    "        \n",
    "def pj_from_nexus_w_charset(nexus_filename, output_dir, char_type, feature_type, project=False):\n",
    "    \n",
    "    \"\"\" \n",
    "    Takes a nexus file with PAUP style charset commands as input.\n",
    "    Creates a separate fasta file for each alignment partition\n",
    "    Returns a list of fasta filenames and a list of Locus objects\n",
    "    If project==True, returns a Project instance with the loci, alignments and records instead\n",
    "    \"\"\"\n",
    "    \n",
    "    from reprophylo import Locus\n",
    "    from Bio import AlignIO\n",
    "    \n",
    "    charsets = parse_paup_charset(nexus_filename)\n",
    "    \n",
    "    alignment =  AlignIO.read(nexus_filename, 'nexus')\n",
    "    filenames = []\n",
    "    loci_list = []\n",
    "    for locus_name in charsets:\n",
    "        s = charsets[locus_name][0]\n",
    "        e = charsets[locus_name][1]\n",
    "        outname = \"%s/%s.fasta\"%(output_dir,locus_name)\n",
    "        AlignIO.write(alignment[:, s:e], outname, 'fasta')\n",
    "        filenames.append(outname)\n",
    "        loci_list.append(Locus(char_type, feature_type, locus_name, [locus_name]))\n",
    "    \n",
    "    if project:\n",
    "        from reprophylo import Project\n",
    "        pj = Project(loci_list)\n",
    "        for f in filenames:\n",
    "            locus_name = f.split('/')[-1].split('.')[0]\n",
    "            print '%i/%i reading %s'%(i,len(filenames), locus_name)\n",
    "            i += 1\n",
    "            pj.read_alignment(f, char_type, feature_type, locus_name)\n",
    "        return pj\n",
    "            \n",
    "    else:  \n",
    "        return filenames, loci_list\n",
    "\n",
    "##############################################################################################\n",
    "class Project:\n",
    "##############################################################################################\n",
    "    \n",
    "    \"\"\"\n",
    "    The Project class contians all the data and has methods to analyze it. It allows for\n",
    "    experimental analysis by running alternative analyses and formally comparing the \n",
    "    outputs. The pickle_pj() function allows to pickle the project, including the data,\n",
    "    intermediates and results, as well as a description of the methods.It allows for a rerun\n",
    "    of the whole analysis as is, as well as for a reconfiguration of the analysis or addition\n",
    "    of data. If git is installed, it can be called by 'import rpgit'. As a result, git can be \n",
    "    initiated using start_git(). A git repository will be created in the CWD, if it doesn't already exist. \n",
    "    Input datasets, .py, .ipynb and .pkl files in the CWD will be version controlled. \n",
    "    Version control can be paused in the middle of the script\n",
    "    by calling stop_git() and restarted by calling start_git() again.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loci):\n",
    "        \n",
    "        \"\"\"\n",
    "        # making dummy loci\n",
    "        >>> coi = Locus('dna','CDS','coi',['COX1','cox1'])\n",
    "        >>> ssu = Locus('dna','rRNA','18S',['18S','SSU'])\n",
    "        \n",
    "        # Making a Project object\n",
    "        >>> pj = Project([coi,ssu])\n",
    "        >>> print(str(pj))\n",
    "        Project object with the loci coi,18S,\n",
    "        \"\"\"\n",
    "        self.records = []\n",
    "        self._records_dict = {}\n",
    "        self.starttime = str(time.asctime())\n",
    "        self.user = None\n",
    "        if os.path.isfile('USER'):\n",
    "            self.user = []\n",
    "            for line in open('USER','r').readlines():\n",
    "                key, arg = line.rstrip().split('=')\n",
    "                self.user.append([key, arg])\n",
    "        self.loci = loci\n",
    "        self.records_by_locus = {}\n",
    "        self.concatenations = []\n",
    "        self.alignments = {}\n",
    "        self.trimmed_alignments = {}\n",
    "        self.trees = {}\n",
    "        self.used_methods = []\n",
    "        self.sets = {}\n",
    "        \n",
    "\n",
    "        self.defaults = {'raxmlHPC': programspath+'raxmlHPC-PTHREADS-SSE3',\n",
    "                         'mafft': 'mafft',\n",
    "                         'muscle': programspath+'muscle',\n",
    "                         'trimal': programspath+'trimal',\n",
    "                         'pb': programspath+'pb',\n",
    "                         'bpcomp': programspath+'bpcomp',\n",
    "                         'tracecomp': programspath+'tracecomp',\n",
    "                         'fasttree': programspath+'FastTreeMP',\n",
    "                         'pal2nal': programspath+'pal2nal.pl'}\n",
    "    \n",
    "        seen = []\n",
    "        if isinstance(loci,list):\n",
    "            for locus in loci:\n",
    "                if not isinstance(locus, Locus):\n",
    "                    raise TypeError(\"Expecting Locus object in loci list. \"+locus+\n",
    "                                    \" not a Locus object\")\n",
    "                if locus.name in seen:\n",
    "                    raise NameError('Locus ' + locus.name + ' apears more than once in self.loci')\n",
    "                else:\n",
    "                    seen.append(locus.name)\n",
    "        elif isinstance(loci,str):\n",
    "            self.loci = loci_list_from_csv(loci)\n",
    "            #print 'Read the following loci from file %s:'%loci\n",
    "            #for l in self.loci:\n",
    "                #print str(l)\n",
    "        self.aln_summaries = []\n",
    "                \n",
    "                \n",
    "    def __str__(self):\n",
    "        loci_string = ''\n",
    "        for i in self.loci:\n",
    "            loci_string += i.name+','\n",
    "        return 'Project object with the loci '+loci_string\n",
    "\n",
    "\n",
    "    def __records_list_to_dict__(self):\n",
    "        self._records_dict = SeqIO.to_dict(self.records)\n",
    "        \n",
    "    ###################################\n",
    "    # Project methods for reading data\n",
    "    ###################################  \n",
    "\n",
    "\n",
    "\n",
    "    def read_embl_genbank(self, input_filenames_list):\n",
    "        \n",
    "        \"\"\"\n",
    "        Read a file from Genbank of EMBL\n",
    "        \n",
    "        >>> input_filenames = ['test-data/test.gb']\n",
    "        >>> locus = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "        >>> pj = Project([locus])\n",
    "        >>> print(len(pj.records))\n",
    "        0\n",
    "        >>> pj.read_embl_genbank(input_filenames)\n",
    "        >>> print(len(pj.records))\n",
    "        89\n",
    "        \"\"\"\n",
    "        \n",
    "        if __builtin__.git:\n",
    "            import rpgit\n",
    "        else:\n",
    "            warnings.warn('Version control off')    \n",
    "        generators = []\n",
    "        for input_filename in input_filenames_list:\n",
    "            if __builtin__.git:\n",
    "                import rpgit\n",
    "                rpgit.gitAdd(input_filename)\n",
    "            generators.append(parse_input(input_filename, 'gb'))\n",
    "            for generator in generators:\n",
    "                for record in generator:\n",
    "                    dwindled_record = dwindle_record(record, self.loci)\n",
    "                    if len(record.features) > 1:\n",
    "                        self.records.append(dwindled_record)\n",
    "                    elif len(record.features) == 1 and not record.features[0].type == 'source':\n",
    "                        self.records.append(dwindled_record)\n",
    "        if __builtin__.git:\n",
    "            import rpgit\n",
    "            comment = \"%i genbank/embl data file(s) from %s\" % (len(input_filenames_list), time.asctime()) \n",
    "            for filename in input_filenames_list:\n",
    "                rpgit.gitAdd(filename)\n",
    "            cwd = os.getcwd()\n",
    "            import fnmatch\n",
    "            matches = []\n",
    "            for root, dirnames, filenames in os.walk(cwd):\n",
    "                for filename in fnmatch.filter(filenames, '*.py'):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "                for filename in fnmatch.filter(filenames, '*.ipynb'):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "            for match in matches:\n",
    "                rpgit.gitAdd(match)    \n",
    "            rpgit.gitCommit(comment) \n",
    "            \n",
    "        \n",
    "        \n",
    "    def read_denovo(self, input_filenames, char_type, format = 'fasta'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Include records from a fasta file. Fasta file records will be given record ids \n",
    "        of the form 'denovo1'. The record.id and record.description will be placed in a\n",
    "        source feature under the 'original_id' and 'original_desc' qualifiers. Denovo sequences\n",
    "        require the use of the add_feature_to_record() method in order to be included in the\n",
    "        anaysis.\n",
    "        \n",
    "\n",
    "        >>> input_filenames = ['test-data/test.gb']\n",
    "        >>> locus = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "        >>> pj = Project([locus])\n",
    "        >>> print(len(pj.records))\n",
    "        0\n",
    "        >>> pj.read_embl_genbank(input_filenames)\n",
    "        >>> print(len(pj.records))\n",
    "        89\n",
    "        >>> input_filenames = ['test-data/test.fasta']\n",
    "        >>> pj.read_denovo(input_filenames, 'dna')\n",
    "        1\n",
    "        >>> print(len(pj.records))\n",
    "        90\n",
    "        \n",
    "        # Since the denovo sequence has no feature it is not included\n",
    "        >>> pj.extract_by_locus()\n",
    "        >>> print(len(pj.records_by_locus['coi']))\n",
    "        89\n",
    "        \n",
    "        # Making a feature for the denovo record.\n",
    "        >>> pj.add_feature_to_record('denovo0', 'CDS',  qualifiers={'gene': 'coi'})\n",
    "        'denovo0_f0'\n",
    "        >>> pj.extract_by_locus()\n",
    "        >>> print(len(pj.records_by_locus['coi']))\n",
    "        90\n",
    "        \"\"\"\n",
    "        \n",
    "        if __builtin__.git:\n",
    "            import rpgit\n",
    "        else:\n",
    "            warnings.warn('Version control off')\n",
    "        \n",
    "        count = 0\n",
    "        # start the counter where it stoped the last time we read denovo things\n",
    "        for record in self.records:\n",
    "            if 'denovo' in record.id:\n",
    "                serial = int(record.id[6:])\n",
    "                if serial >= count:\n",
    "                    count = serial+1\n",
    "        for input_filename in input_filenames:\n",
    "            if __builtin__.git:\n",
    "                import rpgit\n",
    "                rpgit.gitAdd(input_filename)\n",
    "            denovo = SeqIO.parse(input_filename, format)\n",
    "            for record in denovo:\n",
    "                source = SeqFeature(FeatureLocation(0, len(record.seq)), type='source', strand=1)\n",
    "                source.qualifiers['original_id'] = [record.id]\n",
    "                source.qualifiers['original_desc'] = [(' ').join(record.description.split()[1:])]\n",
    "                record.id = 'denovo'+str(count)\n",
    "                record.name = record.id\n",
    "                source.qualifiers['feature_id'] = [record.id+'_source']\n",
    "                record.features = [source]\n",
    "                if '-' in str(record.seq):\n",
    "                    record.seq = Seq(str(record.seq).replace('-',''))\n",
    "                    warnings.warn(\"Reseting gaps in records from %s\"%input_filename)\n",
    "                if char_type == 'prot':\n",
    "                    record.seq.alphabet = IUPAC.protein\n",
    "                    #feature = SeqFeature(FeatureLocation(0, len(record.seq)), type='Protein', strand=1)\n",
    "                    #feature.qualifiers['translation'] = [str(record.seq)]\n",
    "                    #feature.qualifiers['protonly']=['true']\n",
    "                elif char_type == 'dna':\n",
    "                    record.seq.alphabet = IUPAC.ambiguous_dna\n",
    "                count += 1\n",
    "                self.records.append(record)\n",
    "                \n",
    "        if __builtin__.git:\n",
    "            import rpgit\n",
    "            comment = \"%i denovo data file(s) from %s\" % (len(input_filenames), time.asctime())\n",
    "            for filename in input_filenames:\n",
    "                rpgit.gitAdd(filename)\n",
    "            cwd = os.getcwd()\n",
    "            import fnmatch\n",
    "            matches = []\n",
    "            for root, dirnames, filenames in os.walk(cwd):\n",
    "                for filename in fnmatch.filter(filenames, '*.py'):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "                for filename in fnmatch.filter(filenames, '*.ipynb'):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "            for match in matches:\n",
    "                rpgit.gitAdd(match)\n",
    "            rpgit.gitCommit(comment)\n",
    "        return count       \n",
    "    \n",
    "    def read_alignment(self, filename, char_type, feature_type, locus_name, format=\"fasta\", aln_method_name = \"ReadDirectly\", exclude=[]):\n",
    "    \n",
    "        if __builtin__.git:\n",
    "            import rpgit\n",
    "        else:\n",
    "            warnings.warn('Version control off')\n",
    "            \n",
    "        if not any([locus.name == locus_name for locus in self.loci]):\n",
    "            raise RuntimeError(\"Locus %s does not exist\"%locus_name)\n",
    "        elif not [locus for locus in self.loci if locus.name == locus_name][0].char_type == char_type:\n",
    "            raise RuntimeError(\"%s is not a %s locus\"%(locus_name, char_type))\n",
    "        elif not [locus for locus in self.loci if locus.name == locus_name][0].feature_type == feature_type:\n",
    "            raise RuntimeError(\"The feature_type %s is not %s\"%(locus_name, feature_type))\n",
    "            \n",
    "        count = 0\n",
    "        # start the counter where it stoped the last time we read denovo things\n",
    "        for record in self.records:\n",
    "            if 'denovo' in record.id:\n",
    "                serial = int(record.id[6:])\n",
    "                if serial >= count:\n",
    "                    count = serial+1\n",
    "        # Read the alignment:\n",
    "        raw_aln_input = list(AlignIO.read(filename, format))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # make records\n",
    "        records = []\n",
    "        aln_records = []\n",
    "        for record in raw_aln_input:\n",
    "            total_seq_len = len(record.seq)\n",
    "            if (str(record.seq).count('-') + str(record.seq).count('.') + \n",
    "                str(record.seq).count('?') + str(record.seq).count('N') +\n",
    "                str(record.seq).count('n') + str(record.seq).count('X') + \n",
    "                str(record.seq).count('x')) == total_seq_len:\n",
    "                print 'dropping seq %s in locus %s: missing data'%(locus_name,record.id)\n",
    "            elif not record.id in exclude:\n",
    "                # remove gaps\n",
    "                new_record = SeqRecord(seq=Seq(str(record.seq).replace('-','').replace('.','')))\n",
    "                aln_record = SeqRecord(seq=Seq(str(record.seq).replace('.','-')))\n",
    "                \n",
    "                #set alphabet\n",
    "                if char_type == 'prot':\n",
    "                    new_record.seq.alphabet = IUPAC.protein\n",
    "                    aln_record.seq.alphabet = IUPAC.protein\n",
    "                    \n",
    "                elif char_type == 'dna':\n",
    "                    new_record.seq.alphabet = IUPAC.ambiguous_dna\n",
    "                    aln_record.seq.alphabet = IUPAC.ambiguous_dna\n",
    "                    \n",
    "                # set denovo record id\n",
    "                new_record.id = 'denovo%i'%count\n",
    "                new_record.name = new_record.id\n",
    "                \n",
    "                # set source and first feature\n",
    "                source = SeqFeature(FeatureLocation(0, len(new_record.seq)), type='source', strand=1)\n",
    "                source.qualifiers['original_id'] = [record.id]\n",
    "                source.qualifiers['original_desc'] = [(' ').join(record.description.split()[1:])]\n",
    "                source.qualifiers['feature_id'] = [new_record.id+'_source']\n",
    "                feature = SeqFeature(FeatureLocation(0, len(new_record.seq)), type=feature_type, strand=1)\n",
    "                feature.qualifiers['feature_id'] = [new_record.id+'_f0']\n",
    "                feature.qualifiers['gene'] = [locus_name]\n",
    "                feature_seq = feature.extract(new_record.seq)\n",
    "                degen = len(feature_seq)\n",
    "                if char_type == 'dna':    \n",
    "                    for i in ['A','T','G','C','U','a','t','g','c','u']:\n",
    "                        degen -= feature_seq.count(i)\n",
    "                    feature.qualifiers['GC_content'] = [str(GC(feature_seq))]\n",
    "                    feature.qualifiers['nuc_degen_prop'] = [str(float(degen)/len(feature_seq))]\n",
    "                    warnings.warn(\"To get translations, add a feature manually\")\n",
    "                elif char_type == 'prot': \n",
    "                    degen = 0\n",
    "                    for i in ['B', 'X', 'Z', 'b', 'x', 'z']:\n",
    "                        degen += feature_seq.count(i)\n",
    "                    feature.qualifiers['prot_degen_prop'] = [str(float(degen)/len(feature_seq))]   \n",
    "                new_record.features = [source, feature]\n",
    "                \n",
    "                aln_record.id = 'denovo%i_f0'%count\n",
    "                \n",
    "                count += 1\n",
    "                \n",
    "                records.append(new_record)\n",
    "                aln_records.append(aln_record)\n",
    "            \n",
    "        token = \"%s@%s\"%(locus_name, aln_method_name)\n",
    "        \n",
    "        if token in self.alignments.keys():\n",
    "            warnings.warn(\"Replacing alignment %s\"%token)\n",
    "        # need to add the denovo id's inside the alignment    \n",
    "        self.alignments[token] = MultipleSeqAlignment(aln_records)\n",
    "        self.records += records\n",
    "        self.extract_by_locus()\n",
    "        \n",
    "        if __builtin__.git:\n",
    "            import rpgit\n",
    "            comment = \"Alignment file %s\" % (time.asctime())\n",
    "            rpgit.gitAdd(filename)\n",
    "            cwd = os.getcwd()\n",
    "            import fnmatch\n",
    "            matches = []\n",
    "            for root, dirnames, filenames in os.walk(cwd):\n",
    "                for filename in fnmatch.filter(filenames, '*.py'):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "                for filename in fnmatch.filter(filenames, '*.ipynb'):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "            for match in matches:\n",
    "                rpgit.gitAdd(match)\n",
    "            rpgit.gitCommit(comment)\n",
    "\n",
    "    def add_feature_to_record(self, record_id, feature_type, location='full', qualifiers={}):\n",
    "    \n",
    "        \"\"\"\n",
    "        # Making a dummy locus    \n",
    "        >>> coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "    \n",
    "        # Making a dummy Project\n",
    "        >>> pj = Project([coi])\n",
    "    \n",
    "        # making a dummy record\n",
    "        >>> s = 'atgc'*1000\n",
    "        >>> source = SeqFeature()\n",
    "        >>> source.location = FeatureLocation(0,3999)\n",
    "        >>> source.type = 'source'\n",
    "        >>> record = SeqRecord(seq=Seq(s, IUPAC.ambiguous_dna), id='1', description='spam')\n",
    "        >>> record.features.append(source)\n",
    "        >>> pj.records = [record]\n",
    "        >>> print(len(pj.records[0].features))\n",
    "        1\n",
    "        \n",
    "        # adding a feature to a record in the pj\n",
    "        >>> import warnings\n",
    "        >>> warnings.simplefilter(\"ignore\")\n",
    "        >>> pj.add_feature_to_record('1', 'CDS', qualifiers={'gene': 'madeuplocus'})\n",
    "        '1_f0'\n",
    "        >>> print(len(pj.records[0].features))\n",
    "        2\n",
    "        \"\"\"\n",
    "    \n",
    "        for record in self.records:\n",
    "            if record.id == record_id:\n",
    "                #determine new feature id\n",
    "                feature_id = None\n",
    "                serials = []\n",
    "                if len(record.features) > 0:\n",
    "                    for feature in record.features:\n",
    "                        if 'feature_id' in feature.qualifiers.keys():\n",
    "                            if '_f' in feature.qualifiers['feature_id'][0]:\n",
    "                                f = feature.qualifiers['feature_id'][0]\n",
    "                                serials.append(int(f.split('_')[1][1:]))\n",
    "                    serials.sort(reverse = True)\n",
    "                if len(serials) > 0:\n",
    "                    feature_id = record.id + '_f' + str(serials[0]+1)\n",
    "                else:\n",
    "                    feature_id = record.id + '_f0'\n",
    "                feature = None\n",
    "                if location == 'full':\n",
    "                    feature = SeqFeature(FeatureLocation(0, len(record.seq)),\n",
    "                                         type=feature_type,\n",
    "                                         strand=1)\n",
    "                elif isinstance(location, list):\n",
    "                    for i in location:\n",
    "                        if not isinstance(i, list):\n",
    "                            raise RuntimeError('\\'location\\' takes either \\'full\\' or a list of lists')\n",
    "                    if len(location) == 1:\n",
    "                        feature = SeqFeature(FeatureLocation(int(location[0][0])-1,int(location[0][1])),\n",
    "                                             type=feature_type, strand=int(location[0][2]))\n",
    "                    elif len(location) > 1:\n",
    "                        list_of_locations = []\n",
    "                        for i in location:\n",
    "                            start = int(i[0]-1)\n",
    "                            end = int(i[1])\n",
    "                            strand = int(i[2])\n",
    "                            list_of_locations.append(FeatureLocation(start,end,strand=strand))\n",
    "                        feature = SeqFeature(CompoundLocation(list_of_locations),type=feature_type)\n",
    "                feature.qualifiers['feature_id'] = [feature_id]\n",
    "                \n",
    "                \n",
    "                \n",
    "                if len(qualifiers.keys()) > 0:\n",
    "                    for key in qualifiers.keys():\n",
    "                        feature.qualifiers[key] = [qualifiers[key]]\n",
    "                if (('codon_start' in qualifiers.keys()) and\n",
    "                    ('transl_table' in qualifiers.keys())):\n",
    "                    cds = feature.extract(record.seq)\n",
    "                    if str(qualifiers['codon_start']) == '2':\n",
    "                        cds = cds[1:]\n",
    "                    elif str(qualifiers['codon_start']) == '3':\n",
    "                        cds = cds[2:]\n",
    "                    translation = cds.translate(table=int(qualifiers['transl_table']))\n",
    "                    if len(translation)*3 < float(0.9)*len(cds):\n",
    "                        raise RuntimeWarning('The translation of feature '+feature_id+' uses less than 90%'+\n",
    "                                             ' of the coding sequence')\n",
    "                    feature.qualifiers['translation'] = [str(translation)]\n",
    "                \n",
    "                \n",
    "                feature_seq = feature.extract(record.seq)\n",
    "                degen = len(feature_seq)\n",
    "                for i in ['A','T','G','C','U','a','t','g','c','u']:\n",
    "                    degen -= feature_seq.count(i)\n",
    "                feature.qualifiers['GC_content'] = [str(GC(feature_seq))]\n",
    "                feature.qualifiers['nuc_degen_prop'] = [str(float(degen)/len(feature_seq))]\n",
    "                if 'translation' in feature.qualifiers.keys():\n",
    "                    transl = feature.qualifiers['translation'][0]\n",
    "                    degen = 0\n",
    "                    for i in ['B', 'X', 'Z', 'b', 'x', 'z']:\n",
    "                        degen += transl.count(i)\n",
    "                    feature.qualifiers['prot_degen_prop'] = [str(float(degen)/len(transl))]   \n",
    "                \n",
    "                record.features.append(feature)\n",
    "                return feature_id\n",
    "\n",
    "\n",
    "\n",
    "    ##############################################\n",
    "    # Project methods for managing concatenations\n",
    "    ##############################################  \n",
    "\n",
    "\n",
    " \n",
    "    def add_concatenation(self, concatenation_object):\n",
    "        \n",
    "        \"\"\"\n",
    "        add a Concatenation object to the Project\n",
    "        \n",
    "        # making dummy loci\n",
    "        >>> coi = Locus('dna','CDS','coi',['COX1','cox1'])\n",
    "        >>> ssu = Locus('dna','rRNA','18S',['18S','SSU'])\n",
    "        >>> lsu = Locus('dna','rRNA','28S',['28S','LSU'])\n",
    "        >>> loci = [coi,ssu,lsu]\n",
    "        \n",
    "        # making dummy Concatenation\n",
    "        >>> combined = Concatenation(name='combined', \n",
    "        ...                          loci=loci, otu_meta='OTU_dict',\n",
    "        ...                          otu_must_have_all_of=['coi'],\n",
    "        ...                          otu_must_have_one_of =[['18S','28S']],\n",
    "        ...                          define_trimmed_alns=[\"MafftLinsi@Gappyout\"])\n",
    "        >>> print(str(combined))\n",
    "        Concatenation named combined, with loci coi,18S,28S,\n",
    "        of which coi must exist for all species\n",
    "        and at least one of each group of [ 18S 28S ] is represented.\n",
    "        Alignments with the following names: MafftLinsi@Gappyout are prefered\n",
    "        \n",
    "        # making a dummy Project\n",
    "        >>> pj = Project(loci)\n",
    "        \n",
    "        # Including the Concatenation in the Project\n",
    "        >>> pj.add_concatenation(combined)\n",
    "        >>> print(len(pj.concatenations))\n",
    "        1\n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(concatenation_object, Concatenation):\n",
    "            # correct characters offending raxml, back up original values\n",
    "            meta = concatenation_object.otu_meta\n",
    "            self.copy_paste_within_feature(meta, \"original_%s\"%meta)\n",
    "            offensive = ['\\t','\\r','\\n', \"'\", '\"', ',', ' ',\n",
    "                         ';', ':', ']','[','(',')','/']\n",
    "            for r in self.records:\n",
    "                for f in r.features:\n",
    "                    if meta in f.qualifiers.keys():\n",
    "                        for o in offensive:\n",
    "                            if o in f.qualifiers[meta][0]:\n",
    "                                print ((\"found raxml offensive char %s in OTU %s. Replacing with '_ro_'.\"+\n",
    "                                        \"Backing up original in the qualifier original_%s.\")%(o, f.qualifiers[meta][0], meta))\n",
    "                                f.qualifiers[meta][0] = f.qualifiers[meta][0].replace(o,'_ro_')\n",
    "            \n",
    "            seen = []\n",
    "            for s in self.concatenations:\n",
    "                seen.append(s.name)\n",
    "            if concatenation_object.name in seen:\n",
    "                raise NameError('Concatenation ' + concatenation_object.name +\n",
    "                                ' apears more than once in self.concatenations')\n",
    "            else:\n",
    "                self.concatenations.append(concatenation_object)\n",
    "        else:\n",
    "            raise TypeError(\"Expecting Concatenation object\")\n",
    "\n",
    "    \n",
    "    #def make_concatenation_alignments(self):\n",
    "        \n",
    "    #    \"\"\"\n",
    "    #    Concatenates a trimmed alignment based on each of the Concatenation objects and adds them\n",
    "    #    to the pj.trimmed_alignments dictionary. While a trimmed alignment of an individual locus will have a key\n",
    "    #    following the patten \"locus_name@alignment_method_name@trimming_method_name, the key for a concatenated\n",
    "    #    trimmed alignment will be the Concatenation object name attribute.\n",
    "    #    \"\"\"\n",
    "    #    for s in self.concatenations:\n",
    "    #        \n",
    "    #        # get a non-redundant list of OTUs stored in 'meta', such as voucher specimen\n",
    "    #        meta = s.otu_meta\n",
    "    #        OTU_list = []\n",
    "    #        for record in self.records:\n",
    "    #            for feature in record.features:\n",
    "    #                if not feature.type == 'source':\n",
    "    #                    qualifiers_dictionary = get_qualifiers_dictionary(self,\n",
    "    #                                                                      feature.qualifiers['feature_id'])\n",
    "    #                    if (meta in qualifiers_dictionary.keys() and\n",
    "    #                        not qualifiers_dictionary[meta] in OTU_list):\n",
    "    #                        OTU_list.append(qualifiers_dictionary[meta])\n",
    "    #                        \n",
    "    #        \n",
    "    #    \n",
    "    #\n",
    "    #        included_individuals = {} #included_individuals[otu][locus]=feautre_id\n",
    "    #        \n",
    "    #        #Get the correct trimmed alignment tokens\n",
    "    #        keys_of_trimmed_alignments_to_use_in_concat = []\n",
    "    #        for locus in s.loci:\n",
    "    #            trimmed_aln = None\n",
    "    #            all_locus_trimmed_alns_in_pj = []\n",
    "    #            for key in self.trimmed_alignments.keys():\n",
    "    #                if locus.name == key.split('@')[0]:\n",
    "    #                    all_locus_trimmed_alns_in_pj.append(key)\n",
    "    #            if len(all_locus_trimmed_alns_in_pj) == 1:\n",
    "    #                trimmed_aln = all_locus_trimmed_alns_in_pj[0]\n",
    "    #            elif len(all_locus_trimmed_alns_in_pj) == 0:\n",
    "    #                raise RuntimeError('Locus '+locus.name+' have no trimmed alignments')\n",
    "    #            else:\n",
    "    #                s.define_trimmed_alns.sort(key = lambda i: i.count('@'), reverse=True)\n",
    "    #                for definition in s.define_trimmed_alns:\n",
    "    #                    if definition.count('@') == 2 and locus.name == definition.split('@')[0]:\n",
    "    #                        trimmed_aln = definition\n",
    "    #                    elif definition.count('@') == 1 and any([definition in i for i in all_locus_trimmed_alns_in_pj]):\n",
    "    #                        trimmed_aln = locus.name+'@'+definition\n",
    "    #                    else:\n",
    "    #                        raise RuntimeError(\"Could not determine which alignment/trimming alternative to use for locus '\"+\n",
    "    #                                            locus.name+\"' out of \"+str(all_locus_trimmed_alns_in_pj))\n",
    "    #            if trimmed_aln:\n",
    "    #                keys_of_trimmed_alignments_to_use_in_concat.append(trimmed_aln)\n",
    "    #            else:\n",
    "    #                raise RuntimeError('Could not find trimmed aln for locus '+locus.name+' given the rulls '+str(s.define_trimmed_alns))\n",
    "    #        \n",
    "    #        #print \"%i individuals will be included in the concatenations %s\"%(len(included_individuals.keys()), s.name)\n",
    "    #        \n",
    "    #        #if len(included_individuals.keys()) < 4:\n",
    "    #        #    raise RuntimeError(\"Concatenation %s has less than 4 OTUs and cannot be analyzed\"%s.name)\n",
    "    #        for otu in OTU_list:\n",
    "    #            otu_features = {}\n",
    "    #            use = True\n",
    "    #            \n",
    "    #            # Check first rule\n",
    "    #            for locus in s.otu_must_have_all_of:\n",
    "    #                token = [t for t in keys_of_trimmed_alignments_to_use_in_concat if \"%s@\"%locus in t][0]\n",
    "    #                feature_ids = [r.id for r in self.trimmed_alignments[token]]\n",
    "    #                feature_found = False\n",
    "    #                count = 0\n",
    "    #                for feature_id in feature_ids:\n",
    "    #                    qualifiers = get_qualifiers_dictionary(self, feature_id)\n",
    "    #                    if meta in qualifiers.keys() and otu == qualifiers[meta]:\n",
    "    #                        count += 1\n",
    "    #                        feature_found = True\n",
    "    #                        otu_features[locus] = feature_id\n",
    "    #                if count > 1:\n",
    "    #                    raise RuntimeError(\"%s is not unique in %s\"%(otu, locus))\n",
    "    #                if not feature_found:\n",
    "    #                    use = False\n",
    "    #                    \n",
    "    #            # Check second rule\n",
    "    #            if use:\n",
    "    #                for group in s.otu_must_have_one_of:\n",
    "    #                    if isinstance(group,str):\n",
    "    #                        raise IOError('The keyword \\'otu_must_have_one_of\\' has to be a list of lists')\n",
    "    #                    feature_found = False\n",
    "    #                    for locus in group:\n",
    "    #                        token = [t for t in keys_of_trimmed_alignments_to_use_in_concat if \"%s@\"%locus in t][0]\n",
    "    #                        feature_ids = [r.id for r in self.trimmed_alignments[token]]\n",
    "    #                        count = 0\n",
    "    #                        for feature_id in feature_ids:\n",
    "    #                            qualifiers = get_qualifiers_dictionary(self, feature_id)\n",
    "    #                            if meta in qualifiers.keys() and otu == qualifiers[meta]:\n",
    "    #                                count += 1\n",
    "    #                                feature_found = True\n",
    "    #                                otu_features[locus] = feature_id\n",
    "    #                        if count > 1:\n",
    "    #                            raise RuntimeError(\"%s is not unique in %s\"%(otu, locus))\n",
    "    #                    if not feature_found:\n",
    "    #                        use = False\n",
    "    #            if use:\n",
    "    #                included_individuals[otu] = otu_features\n",
    "    #        \n",
    "    #        # printing a table of the alignment\n",
    "    #        included_indivduals_table = ''\n",
    "    #        loci_names = [l.name for l in s.loci]\n",
    "    #        line = 'OTU'.ljust(30,' ')\n",
    "    #        for name in loci_names:\n",
    "    #            line += name.ljust(20,' ')\n",
    "    #        included_indivduals_table += line+'\\n'\n",
    "    #        for otu in included_individuals.keys():\n",
    "    #            line = otu.ljust(30,' ')\n",
    "    #            for locus_name in loci_names:\n",
    "    #                if locus_name in included_individuals[otu].keys():\n",
    "    #                    line += included_individuals[otu][locus_name].ljust(15,' ')\n",
    "    #                else:\n",
    "    #                    line += ''.ljust(15,' ')\n",
    "    #            included_indivduals_table += line+'\\n'\n",
    "    #        print \"Concatenation %s will have the following data\"%s.name\n",
    "    #        print included_indivduals_table\n",
    "    #        \n",
    "    #        # remove partitions with less then 4 sequences\n",
    "    #        for name in loci_names:\n",
    "    #            if len([otu for otu in included_individuals.keys() if name in included_individuals[otu].keys()]) < 4:\n",
    "    #                print ((\"Locus %s has less then 4 sequences in concatenation %s and where excluded \"+\n",
    "    #                                     \"from the concatenation\")%(name,s.name))\n",
    "    #                for key in keys_of_trimmed_alignments_to_use_in_concat:\n",
    "    #                    if name in key:\n",
    "    #                        keys_of_trimmed_alignments_to_use_in_concat.remove(key)\n",
    "    #                        \n",
    "    #                                    \n",
    "    #        \n",
    "    #        # build alignment\n",
    "    #        # concat_records = []\n",
    "    #        alignment = []\n",
    "    #        for individual in included_individuals.keys():\n",
    "    #            sequence = ''\n",
    "    #            for key in keys_of_trimmed_alignments_to_use_in_concat:\n",
    "    #                locus_name = key.split('@')[0]                    \n",
    "    #                length = len(self.trimmed_alignments[key][0])\n",
    "    #                s.used_trimmed_alns[key] = length\n",
    "    #                if locus_name in included_individuals[individual].keys():\n",
    "    #                    for record in self.trimmed_alignments[key]:\n",
    "    #                        if record.id == included_individuals[individual][locus_name]:\n",
    "    #                            sequence += str(record.seq)\n",
    "    #                else:\n",
    "    #                    sequence += '?'*length\n",
    "    #            concat_sequence = SeqRecord(seq = Seq(sequence), id = individual, description = '')\n",
    "    #            alignment.append(concat_sequence)\n",
    "    #        self.trimmed_alignments[s.name] = MultipleSeqAlignment(alignment)                \n",
    "    #        s.feature_id_dict = included_individuals    \n",
    "\n",
    "    def make_concatenation_alignments(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Concatenates a trimmed alignment based on each of the Concatenation objects and adds them\n",
    "        to the pj.trimmed_alignments dictionary. While a trimmed alignment of an individual locus will have a key\n",
    "        following the patten \"locus_name@alignment_method_name@trimming_method_name, the key for a concatenated\n",
    "        trimmed alignment will be the Concatenation object name attribute.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.__records_list_to_dict__()\n",
    "        \n",
    "        for s in self.concatenations:\n",
    "            \n",
    "            # get a non-redundant list of OTUs stored in 'meta', such as voucher specimen\n",
    "            meta = s.otu_meta\n",
    "            OTU_list = []\n",
    "            for record in self.records:\n",
    "                for feature in record.features:\n",
    "                    if not feature.type == 'source':\n",
    "                        qualifiers_dictionary = __get_qualifiers_dictionary__(self,\n",
    "                                                                          feature.qualifiers['feature_id'])\n",
    "                        if (meta in qualifiers_dictionary.keys() and\n",
    "                            not qualifiers_dictionary[meta] in OTU_list):\n",
    "                            OTU_list.append(qualifiers_dictionary[meta])\n",
    "                            \n",
    "            \n",
    "        \n",
    "    \n",
    "            included_individuals = {} #included_individuals[otu][locus]=feautre_id\n",
    "            \n",
    "            #Get the correct trimmed alignment tokens\n",
    "            keys_of_trimmed_alignments_to_use_in_concat = []\n",
    "            for locus in s.loci:\n",
    "                trimmed_aln = None\n",
    "                all_locus_trimmed_alns_in_pj = []\n",
    "                for key in self.trimmed_alignments.keys():\n",
    "                    if locus.name == key.split('@')[0]:\n",
    "                        all_locus_trimmed_alns_in_pj.append(key)\n",
    "                if len(all_locus_trimmed_alns_in_pj) == 1:\n",
    "                    trimmed_aln = all_locus_trimmed_alns_in_pj[0]\n",
    "                elif len(all_locus_trimmed_alns_in_pj) == 0:\n",
    "                    raise RuntimeError('Locus '+locus.name+' have no trimmed alignments')\n",
    "                else:\n",
    "                    s.define_trimmed_alns.sort(key = lambda i: i.count('@'), reverse=True)\n",
    "                    for definition in s.define_trimmed_alns:\n",
    "                        if definition.count('@') == 2 and locus.name == definition.split('@')[0]:\n",
    "                            trimmed_aln = definition\n",
    "                        elif definition.count('@') == 1 and any([definition in i for i in all_locus_trimmed_alns_in_pj]):\n",
    "                            trimmed_aln = locus.name+'@'+definition\n",
    "                        else:\n",
    "                            raise RuntimeError(\"Could not determine which alignment/trimming alternative to use for locus '\"+\n",
    "                                                locus.name+\"' out of \"+str(all_locus_trimmed_alns_in_pj))\n",
    "                if trimmed_aln:\n",
    "                    keys_of_trimmed_alignments_to_use_in_concat.append(trimmed_aln)\n",
    "                else:\n",
    "                    raise RuntimeError('Could not find trimmed aln for locus '+locus.name+' given the rulls '+str(s.define_trimmed_alns))\n",
    "            \n",
    "            #print \"%i individuals will be included in the concatenations %s\"%(len(included_individuals.keys()), s.name)\n",
    "            \n",
    "            #if len(included_individuals.keys()) < 4:\n",
    "            #    raise RuntimeError(\"Concatenation %s has less than 4 OTUs and cannot be analyzed\"%s.name)\n",
    "            for otu in OTU_list:\n",
    "                otu_features = {}\n",
    "                use = True\n",
    "                \n",
    "                # Check first rule\n",
    "                for locus in s.otu_must_have_all_of:\n",
    "                    token = [t for t in keys_of_trimmed_alignments_to_use_in_concat if \"%s@\"%locus in t][0]\n",
    "                    feature_ids = [r.id for r in self.trimmed_alignments[token]]\n",
    "                    feature_found = False\n",
    "                    count = 0\n",
    "                    for feature_id in feature_ids:\n",
    "                        qualifiers = __get_qualifiers_dictionary__(self, feature_id)\n",
    "                        if meta in qualifiers.keys() and otu == qualifiers[meta]:\n",
    "                            count += 1\n",
    "                            feature_found = True\n",
    "                            otu_features[locus] = feature_id\n",
    "                    if count > 1:\n",
    "                        raise RuntimeError(\"%s is not unique in %s\"%(otu, locus))\n",
    "                    if not feature_found:\n",
    "                        use = False\n",
    "                        \n",
    "                # Check second rule\n",
    "                if use:\n",
    "                    for group in s.otu_must_have_one_of:\n",
    "                        if isinstance(group,str):\n",
    "                            raise IOError('The keyword \\'otu_must_have_one_of\\' has to be a list of lists')\n",
    "                        feature_found = False\n",
    "                        for locus in group:\n",
    "                            token = [t for t in keys_of_trimmed_alignments_to_use_in_concat if \"%s@\"%locus in t][0]\n",
    "                            feature_ids = [r.id for r in self.trimmed_alignments[token]]\n",
    "                            count = 0\n",
    "                            for feature_id in feature_ids:\n",
    "                                qualifiers = __get_qualifiers_dictionary__(self, feature_id)\n",
    "                                if meta in qualifiers.keys() and otu == qualifiers[meta]:\n",
    "                                    count += 1\n",
    "                                    feature_found = True\n",
    "                                    otu_features[locus] = feature_id\n",
    "                            if count > 1:\n",
    "                                raise RuntimeError(\"%s is not unique in %s\"%(otu, locus))\n",
    "                        if not feature_found:\n",
    "                            use = False\n",
    "                if use:\n",
    "                    included_individuals[otu] = otu_features\n",
    "            \n",
    "            # printing a table of the alignment\n",
    "            included_indivduals_table = ''\n",
    "            loci_names = [l.name for l in s.loci]\n",
    "            line = 'OTU'.ljust(30,' ')\n",
    "            for name in loci_names:\n",
    "                line += name.ljust(20,' ')\n",
    "            included_indivduals_table += line+'\\n'\n",
    "            for otu in included_individuals.keys():\n",
    "                line = otu.ljust(30,' ')\n",
    "                for locus_name in loci_names:\n",
    "                    if locus_name in included_individuals[otu].keys():\n",
    "                        line += included_individuals[otu][locus_name].ljust(15,' ')\n",
    "                    else:\n",
    "                        line += ''.ljust(15,' ')\n",
    "                included_indivduals_table += line+'\\n'\n",
    "            print \"Concatenation %s will have the following data\"%s.name\n",
    "            print included_indivduals_table\n",
    "            \n",
    "            # remove partitions with less then 4 sequences\n",
    "            for name in loci_names:\n",
    "                if len([otu for otu in included_individuals.keys() if name in included_individuals[otu].keys()]) < 4:\n",
    "                    print ((\"Locus %s has less then 4 sequences in concatenation %s and where excluded \"+\n",
    "                                         \"from the concatenation\")%(name,s.name))\n",
    "                    for key in keys_of_trimmed_alignments_to_use_in_concat:\n",
    "                        if name in key:\n",
    "                            keys_of_trimmed_alignments_to_use_in_concat.remove(key)\n",
    "                            \n",
    "                                        \n",
    "            \n",
    "            # build alignment\n",
    "            # concat_records = []\n",
    "            alignment = []\n",
    "            for individual in included_individuals.keys():\n",
    "                sequence = ''\n",
    "                for key in keys_of_trimmed_alignments_to_use_in_concat:\n",
    "                    locus_name = key.split('@')[0]                    \n",
    "                    length = len(self.trimmed_alignments[key][0])\n",
    "                    s.used_trimmed_alns[key] = length\n",
    "                    if locus_name in included_individuals[individual].keys():\n",
    "                        for record in self.trimmed_alignments[key]:\n",
    "                            if record.id == included_individuals[individual][locus_name]:\n",
    "                                sequence += str(record.seq)\n",
    "                    else:\n",
    "                        sequence += '?'*length\n",
    "                concat_sequence = SeqRecord(seq = Seq(sequence), id = individual, description = '')\n",
    "                alignment.append(concat_sequence)\n",
    "            self.trimmed_alignments[s.name] = MultipleSeqAlignment(alignment)                \n",
    "            s.feature_id_dict = included_individuals \n",
    "    \n",
    "        self._records_dict = {}\n",
    "        \n",
    "    ###################################################\n",
    "    # Project methods for modifying feature qualifiers\n",
    "    ###################################################\n",
    "\n",
    "\n",
    "\n",
    "    def write(self, filename, format = 'genbank'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Write the records in the project in any Biopython format, as csv or as nexml.\n",
    "        \n",
    "        The csv table has a line for each feature (ie multiplt lines for records\n",
    "        with multiple non-source featue). Each line will include the records annotations,\n",
    "        the source feature qualifiers and the qualifiers of the feature itself. (ie, in a\n",
    "        record with several features, the record annotations and the source feature qualifiers\n",
    "        will be repeted in several lines, once for each non-source feature in the record.\n",
    "        The csv file is primarily usefull for reviewing and editing feature qualifiers\n",
    "        \n",
    "        The nexml format only includes the trees from the pj.trees dictionary, but all \n",
    "        the metadata is included as leaf metadata, including the sequences and the\n",
    "        trimmed and alligned sequences for each leaf.\n",
    "        \"\"\"\n",
    "        if format == 'nexml':\n",
    "            self.write_nexml(filename)\n",
    "        elif format == 'genbank' or format == 'embl':\n",
    "            SeqIO.write(self.records, filename, format)\n",
    "        elif format == 'csv':\n",
    "            # get titles for source and othe features\n",
    "            source_qualifiers = []\n",
    "            feature_qualifiers = []\n",
    "            for record in self.records:\n",
    "                for feature in record.features:\n",
    "                    for key in feature.qualifiers.keys():\n",
    "                        if feature.type == 'source' and not key in source_qualifiers:\n",
    "                            source_qualifiers.append(key)\n",
    "                        elif not feature.type == 'source' and not key in feature_qualifiers:\n",
    "                            feature_qualifiers.append(key)\n",
    "            \n",
    "            with open(filename, 'wb') as csvfile:\n",
    "                linewriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                        quotechar='\"',\n",
    "                                        quoting=csv.QUOTE_MINIMAL)\n",
    "                linewriter.writerow(['record_id','seq']+['source:_'+q for q in source_qualifiers]+['taxonomy']+feature_qualifiers)\n",
    "                for record in self.records:\n",
    "                    seq = ''\n",
    "                    if len(record.seq) <= 10:\n",
    "                        seq = str(record.seq)[0:11]\n",
    "                    else:\n",
    "                        seq = str(record.seq)[0:6] + '...' + str(record.seq)[-5:]\n",
    "                    \n",
    "                    \n",
    "                    line_start = [record.id, seq]\n",
    "                    source = None\n",
    "                    for feature in record.features:\n",
    "                        if feature.type == 'source':\n",
    "                            source = feature\n",
    "                    if not source == None:\n",
    "                        for qual in source_qualifiers:\n",
    "                            if qual in source.qualifiers.keys():\n",
    "                                line_start.append(type_to_single_line_str(source.qualifiers[qual]))\n",
    "                            else:\n",
    "                                line_start.append('null')\n",
    "                    elif source == None:\n",
    "                        for qual in source_qualifiers:\n",
    "                            line_start.append('null')\n",
    "                    if 'taxonomy' in record.annotations.keys():\n",
    "                        line_start.append(type_to_single_line_str(record.annotations['taxonomy']))\n",
    "                    else:\n",
    "                        line_start.append(['null'])\n",
    "                    for feature in record.features:\n",
    "                        if not feature.type == 'source':\n",
    "                            line = list(line_start)\n",
    "                            for qual in feature_qualifiers:\n",
    "                                if qual in feature.qualifiers.keys() and qual == 'translation':\n",
    "                                    trans = feature.qualifiers[qual][0]\n",
    "                                    if len(trans)<=10:\n",
    "                                        line.append(trans)\n",
    "                                    else:\n",
    "                                        line.append(trans[:6] + '...' + trans[-5:])\n",
    "                                elif qual in feature.qualifiers.keys():\n",
    "                                    line.append(type_to_single_line_str(feature.qualifiers[qual]))\n",
    "                                else:\n",
    "                                    line.append('null')\n",
    "                            linewriter.writerow(line)\n",
    "        \n",
    "        if __builtin__.git:\n",
    "            import rpgit\n",
    "            comment = \"Records %s text file from %s\" % (format, time.asctime())\n",
    "            rpgit.gitAdd(filename)\n",
    "            cwd = os.getcwd()\n",
    "            import fnmatch\n",
    "            matches = []\n",
    "            for root, dirnames, filenames in os.walk(cwd):\n",
    "                for filename in fnmatch.filter(filenames, '*.py'):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "                for filename in fnmatch.filter(filenames, '*.ipynb'):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "            for match in matches:\n",
    "                rpgit.gitAdd(match)\n",
    "            rpgit.gitCommit(comment)\n",
    "\n",
    "    def correct_metadata_from_file(self,csv_file):\n",
    "        metadata = read_feature_quals_from_tab_csv(csv_file)\n",
    "        new_records = []\n",
    "        for record in self.records:\n",
    "            if record.id in metadata.keys():\n",
    "                #print 'making new record'\n",
    "                new_record = record\n",
    "                record_corrected_metadata = metadata[record.id]\n",
    "                new_record.annotations['taxonomy'] = metadata[record.id]['taxonomy']\n",
    "                for feature in new_record.features:\n",
    "                    if feature.type == 'source':\n",
    "                        feature.qualifiers = metadata[record.id]['source']\n",
    "                    else:\n",
    "                        feature_id = feature.qualifiers['feature_id']\n",
    "                        translation = None\n",
    "                        if 'translation' in feature.qualifiers.keys():\n",
    "                            translation = feature.qualifiers['translation']\n",
    "                        feature.qualifiers = metadata[record.id]['features'][feature_id[0]]\n",
    "                        feature.qualifiers['feature_id'] = feature_id\n",
    "                        if translation:\n",
    "                            feature.qualifiers['translation'] = translation\n",
    "                new_records.append(new_record)\n",
    "            else:\n",
    "                #print 'using old records'\n",
    "                new_records.append(record)\n",
    "        \n",
    "        self.records = new_records\n",
    "        \n",
    "        if __builtin__.git:\n",
    "            import rpgit\n",
    "            comment = \"Corrected metadata CSV file from %s\" % (time.asctime())\n",
    "            rpgit.gitAdd(csv_file)\n",
    "            cwd = os.getcwd()\n",
    "            import fnmatch\n",
    "            matches = []\n",
    "            for root, dirnames, filenames in os.walk(cwd):\n",
    "                for filename in fnmatch.filter(filenames, '*.py'):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "                for filename in fnmatch.filter(filenames, '*.ipynb'):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "            for match in matches:\n",
    "                rpgit.gitAdd(match)\n",
    "            rpgit.gitCommit(comment)\n",
    "                \n",
    "    def if_this_then_that(self, IF_THIS, IN_THIS, THEN_THAT, IN_THAT, mode = 'whole'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Searches pj.records for features that have the value IF_THIS in the qualifier IN_THIS\n",
    "        and places the value THEN_THAT in the qualifier IN_THAT, which either exists or is new.\n",
    "        \n",
    "        The IF_THIS value can either match completely (mode = 'whole') or just to a part (mode = 'part')\n",
    "        of the target qualifier value\n",
    "        \n",
    "        The following demonstartes all the feature qualifier editing methods\n",
    "        \n",
    "        # Make a dummy pj with a locus and with records\n",
    "        >>> input_filenames = ['test-data/test.gb']\n",
    "        >>> locus = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "        >>> pj = Project([locus])\n",
    "        >>> pj.read_embl_genbank(input_filenames)\n",
    "        \n",
    "        # copying a source qualifier into the feature qualifiers so that it\n",
    "        # will be available for editing (source qualifiers are kept imutable)\n",
    "        >>> pj.add_qualifier_from_source('organism')\n",
    "        \n",
    "        # populate a new qualifier based on the data in another\n",
    "        # Here we will take only the the genus name from the organism qualifier\n",
    "        # and put it in a new qualifier\n",
    "        # We use mode='part' because our search phrase (the genus name)\n",
    "        # fits only the start of the organism name\n",
    "        >>> tetillid_genera = ['Tetilla', 'Cinachyrella', 'Craniella']\n",
    "        >>> for g in tetillid_genera:\n",
    "        ...     pj.if_this_then_that(g, 'organism', g, 'genus', mode='part')\n",
    "        \n",
    "        # Now we will add a sample id to all the sequences which belong to\n",
    "        # sample TAU_25617\n",
    "        >>> pj.add_qualifier(['JX177913.1_f0',\n",
    "        ...                  'JX177935.1_f0',\n",
    "        ...                  'JX177965.1_f0'],\n",
    "        ...                  'specimen_voucher',\n",
    "        ...                  'TAU_25617')\n",
    "    \n",
    "        # We are using a copy paste approch to unite the data from \n",
    "        # differen qualifiers under on qualifiers\n",
    "        >>> pj.copy_paste_within_feature('variant', 'strain_or_variant')\n",
    "        >>> pj.copy_paste_within_feature('strain', 'strain_or_variant')\n",
    "        \n",
    "        # Now we print the qualifier of a random feature as an example\n",
    "        >>> qual_dict = get_qualifiers_dictionary(pj, 'JX177913.1_f0')\n",
    "        >>> qual_items = qual_dict.items()\n",
    "        >>> qual_items.sort(key = lambda i: i[0])\n",
    "        >>> for key, val in qual_items:\n",
    "        ...     print(key.ljust(40,' ') + type_to_single_line_str(val)[:5]+'...')\n",
    "        GC_content                              37.28...\n",
    "        annotation_accessions                   JX177...\n",
    "        annotation_data_file_division           INV...\n",
    "        annotation_date                         05-SE...\n",
    "        annotation_gi                           39293...\n",
    "        annotation_keywords                     ...\n",
    "        annotation_organism                     Cinac...\n",
    "        annotation_references                   locat...\n",
    "        annotation_sequence_version             1...\n",
    "        annotation_source                       mitoc...\n",
    "        annotation_taxonomy                     Eukar...\n",
    "        codon_start                             2...\n",
    "        db_xref                                 GI:39...\n",
    "        feature_id                              JX177...\n",
    "        gene                                    cox1...\n",
    "        genus                                   Cinac...\n",
    "        nuc_degen_prop                          0.0...\n",
    "        organism                                Cinac...\n",
    "        product                                 cytoc...\n",
    "        prot_degen_prop                         0.0...\n",
    "        protein_id                              AFM91...\n",
    "        source_country                          Panam...\n",
    "        source_db_xref                          taxon...\n",
    "        source_feature_id                       JX177...\n",
    "        source_identified_by                    Ilan,...\n",
    "        source_mol_type                         genom...\n",
    "        source_note                             autho...\n",
    "        source_organelle                        mitoc...\n",
    "        source_organism                         Cinac...\n",
    "        source_specimen_voucher                 DH_S2...\n",
    "        specimen_voucher                        TAU_2...\n",
    "        transl_table                            4...\n",
    "        translation                             MIGSG...\n",
    "        \n",
    "        # Note that GC content and the porportion of degenerate positions\n",
    "        # have been automatically included. They will be plotted in the report\n",
    "        \"\"\"\n",
    "        \n",
    "        for record in self.records:\n",
    "            for feature in record.features:\n",
    "                if not feature.type == 'source':\n",
    "                    if IN_THIS in feature.qualifiers.keys():\n",
    "                        if not type(feature.qualifiers[IN_THIS]) is list:\n",
    "                            feature.qualifiers[IN_THIS] = [feature.qualifiers[IN_THIS]]\n",
    "                        for i in feature.qualifiers[IN_THIS]:\n",
    "                            if mode == 'whole':\n",
    "                                if i == IF_THIS:\n",
    "                                    feature.qualifiers[IN_THAT] = [THEN_THAT]\n",
    "                            elif mode == 'part':\n",
    "                                if IF_THIS in i:\n",
    "                                    feature.qualifiers[IN_THAT] = [THEN_THAT]\n",
    "    \n",
    "\n",
    "\n",
    "    def add_qualifier(self, feature_ids, name, value):\n",
    "        if not type(value) is list:\n",
    "                    value = [value]\n",
    "        for record in self.records:\n",
    "            for feature in record.features:\n",
    "                if feature.qualifiers['feature_id'][0] in feature_ids:\n",
    "                    feature.qualifiers[name] = value\n",
    "\n",
    "\n",
    "\n",
    "    def add_qualifier_from_source(self, qualifier):\n",
    "        for record in self.records:\n",
    "            source = None\n",
    "            for feature in record.features:\n",
    "                if feature.type == 'source':\n",
    "                    source = feature\n",
    "            value = None\n",
    "            if not source == None:\n",
    "                if qualifier in source.qualifiers.keys():\n",
    "                    value = source.qualifiers[qualifier]       \n",
    "            if not value == None:\n",
    "                if not type(value) is list:\n",
    "                    value = [value]\n",
    "                for feature in record.features:\n",
    "                    if not feature.type == 'source':\n",
    "                        feature.qualifiers[qualifier] = value\n",
    "    \n",
    "\n",
    "\n",
    "    def copy_paste_within_feature(self, from_qualifier, to_qualifier):\n",
    "        for record in self.records:\n",
    "            for feature in record.features:\n",
    "                if not feature.type == 'source':\n",
    "                    if from_qualifier in feature.qualifiers.keys():\n",
    "                        feature.qualifiers[to_qualifier] = feature.qualifiers[from_qualifier]\n",
    "                  \n",
    "                        \n",
    "                        \n",
    "    def copy_paste_from_features_to_source(self, from_feature_qual, to_source_qual):\n",
    "        for record in self.records:\n",
    "            source = None\n",
    "            values_from_features = []\n",
    "            for feature in record.features:\n",
    "                if not feature.type == 'source':\n",
    "                    if from_feature_qual in feature.qualifiers.keys():\n",
    "                        if not feature.qualifiers[from_feature_qual] in values_from_features:\n",
    "                            values_from_features += feature.qualifiers[from_feature_qual]\n",
    "                else:\n",
    "                    source = feature\n",
    "            if source == None:\n",
    "                source = SeqFeature(FeatureLocation(0, len(record.seq)), type='source', strand=1)\n",
    "                source.qualifiers['feature_id'] = record.id + '_source'\n",
    "                record.features = [source] + record.features\n",
    "            if not to_source_qual in source.qualifiers.keys():\n",
    "                source.qualifiers[to_source_qual] = values_from_features\n",
    "           \n",
    "                \n",
    "                \n",
    "    def species_vs_loci(self, outfile_name):\n",
    "        \n",
    "        \"\"\"\n",
    "        Makes a csv file showing the count of each unique value in the source_organism qualifier\n",
    "        for each locus\n",
    "        \"\"\"\n",
    "        \n",
    "        species_vs_loci = {}\n",
    "        for record in self.records:\n",
    "            organism = 'undef'\n",
    "            for feature in record.features:\n",
    "                if feature.type == 'source':\n",
    "                    if 'organism' in feature.qualifiers.keys():\n",
    "                        organism = feature.qualifiers['organism'][0]\n",
    "            if not organism in species_vs_loci.keys():\n",
    "                species_vs_loci[organism] = {}    \n",
    "            for feature in record.features:\n",
    "                if not feature.type == 'source':\n",
    "                    for locus in self.loci:\n",
    "                        if not locus.name in locus.aliases:\n",
    "                            locus.aliases.append(locus.name)\n",
    "                        if 'gene' in feature.qualifiers.keys():\n",
    "                            if feature.qualifiers['gene'][0] in locus.aliases:\n",
    "                                if not locus.name in species_vs_loci[organism].keys():\n",
    "                                    species_vs_loci[organism][locus.name] = 1\n",
    "                                else:\n",
    "                                    species_vs_loci[organism][locus.name] += 1\n",
    "                        elif 'product' in feature.qualifiers.keys():\n",
    "                            if feature.qualifiers['product'][0] in locus.aliases:\n",
    "                                if not locus.name in species_vs_loci[organism].keys():\n",
    "                                    species_vs_loci[organism][locus.name] = 1\n",
    "                                else:\n",
    "                                    species_vs_loci[organism][locus.name] += 1\n",
    "        with open(outfile_name, 'wb') as csvfile:\n",
    "            linewriter = csv.writer(csvfile, delimiter='\\t',\n",
    "                                    quotechar='|',\n",
    "                                    quoting=csv.QUOTE_MINIMAL)\n",
    "            loci_names = []\n",
    "            for g in self.loci:\n",
    "                loci_names.append(g.name)\n",
    "            linewriter.writerow(['species']+loci_names)\n",
    "            for organism in sorted(list(species_vs_loci.keys())):\n",
    "                line = [organism]\n",
    "                for name in loci_names:\n",
    "                    if name in species_vs_loci[organism].keys():\n",
    "                        line.append(str(species_vs_loci[organism][name]))\n",
    "                    else:\n",
    "                        line.append('0')\n",
    "                linewriter.writerow(line)\n",
    "                \n",
    "                \n",
    "                \n",
    "    ######################################                \n",
    "    # Project methods to analyze the data\n",
    "    ######################################\n",
    "    \n",
    "    \n",
    "        \n",
    "    def extract_by_locus(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        >>> input_filenames = ['test-data/test.gb']\n",
    "        >>> coi = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "        >>> lsu = Locus('dna', 'rRNA', '28S', ['28s','28S','LSU rRNA','28S ribosomal RNA','28S large subunit ribosomal RNA'])\n",
    "        >>> pj = Project([coi, lsu])\n",
    "        >>> pj.read_embl_genbank(input_filenames)\n",
    "        >>> pj.extract_by_locus()\n",
    "        >>> print(len(pj.records_by_locus['coi']))\n",
    "        89\n",
    "        >>> print(len(pj.records_by_locus['28S']))\n",
    "        48\n",
    "        \"\"\"\n",
    "        \n",
    "        data_by_locus = {}\n",
    "        for locus in self.loci:\n",
    "            if not locus.name in locus.aliases:\n",
    "                locus.aliases.append(locus.name)\n",
    "                locus.aliases.append(locus.name.replace('_',' '))\n",
    "            records = []\n",
    "            for record in self.records:\n",
    "                for feature in record.features:\n",
    "                    if ((feature.type == locus.feature_type or\n",
    "                         # equate CDS and Protein feature types to allow reading protein sequences from \n",
    "                         # a mix of DNA gb files and protein fasta files that were given a Protein feature\n",
    "                         # type\n",
    "                         (feature.type in ['CDS','Protein'] and locus.feature_type in ['CDS','Protein'])) and\n",
    "                        \n",
    "                        (('gene' in feature.qualifiers.keys() and\n",
    "                          feature.qualifiers['gene'][0] in locus.aliases) \n",
    "                         or\n",
    "                         ('product' in feature.qualifiers.keys() and \n",
    "                          feature.qualifiers['product'][0] in locus.aliases))\n",
    "                        \n",
    "                        ):\n",
    "                        if locus.char_type == 'dna':\n",
    "                            if not type(record.seq.alphabet) == IUPAC.IUPACProtein:\n",
    "                                S = feature.extract(record.seq)\n",
    "                            else:\n",
    "                                raise RuntimeError('Trying to read DNA from protein only records')\n",
    "                        elif locus.char_type == 'prot':\n",
    "                            if not type(record.seq.alphabet) == IUPAC.IUPACProtein:\n",
    "                                if 'translation' in feature.qualifiers.keys():\n",
    "                                    S = Seq(feature.qualifiers['translation'][0], IUPAC.protein)\n",
    "                                else:\n",
    "                                    raise RuntimeError('Trying to read protein from DNA records without translation info')\n",
    "                            else:\n",
    "                                S = feature.extract(record.seq)\n",
    "                        feature_record = SeqRecord(seq = S, id = feature.qualifiers['feature_id'][0],\n",
    "                                                   description = '')\n",
    "                        records.append(feature_record)\n",
    "            data_by_locus[locus.name] = records\n",
    "        self.records_by_locus = data_by_locus\n",
    "\n",
    "    def exclude(self, start_from_max=True, **kwargs):\n",
    "        keep_safe = self.records_by_locus\n",
    "        self.extract_by_locus()\n",
    "        locus_names = [i.name for i in self.loci]\n",
    "        for key, value in kwargs.iteritems():\n",
    "            if key in locus_names:\n",
    "                if value == 'all':\n",
    "                    self.records_by_locus[key] = []\n",
    "                else:\n",
    "                    subset = []\n",
    "                    locus_feature_ids = [i.id.split('_')[0] for i in self.records_by_locus[key]]\n",
    "                    if not all(i.split('_')[0] in locus_feature_ids for i in value):\n",
    "                        print [i.split('_')[0] for i in value if not i.split('_')[0] in locus_feature_ids]\n",
    "                        warnings.warn('Not all records to exclude exist in locus. Typos?')\n",
    "                    if start_from_max:\n",
    "                        for record in keep_safe[key]:\n",
    "                            if not record.id.split('_')[0] in [i.split('_')[0] for i in value]:\n",
    "                                subset.append(record)\n",
    "                        self.records_by_locus[key] = subset\n",
    "                    else:\n",
    "                        for record in self.records_by_locus[key]:\n",
    "                            if not record.id.split('_')[0] in [i.split('_')[0] for i in value]:\n",
    "                                subset.append(record)\n",
    "                        self.records_by_locus[key] = subset\n",
    "            else:\n",
    "                warnings.warn('Locus name %s not recognised'%key)\n",
    "\n",
    "    def include(self, start_from_null=True, **kwargs):\n",
    "        keep_safe = self.records_by_locus\n",
    "        self.extract_by_locus()\n",
    "        locus_names = [i.name for i in self.loci]\n",
    "        for key, value in kwargs.iteritems():\n",
    "            if key in locus_names:\n",
    "                if value == 'all':\n",
    "                    pass\n",
    "                else:\n",
    "                    subset = []\n",
    "                    locus_feature_ids = [i.id.split('_')[0] for i in self.records_by_locus[key]]\n",
    "                    if not all(i.split('_')[0] in locus_feature_ids for i in value):\n",
    "                        print [i.split('_')[0] for i in value if not i.split('_')[0] in locus_feature_ids]\n",
    "                        warnings.warn('Not all records to include exist in locus. Typos?')\n",
    "                    for record in self.records_by_locus[key]:\n",
    "                        if record.id.split('_')[0] in [i.split('_')[0] for i in value]:\n",
    "                            subset.append(record)\n",
    "                    self.records_by_locus[key] = subset\n",
    "                    if not start_from_null:\n",
    "                        self.records_by_locus[key] = subset+keep_safe[key]\n",
    "            else:\n",
    "                warnings.warn('Locus name %s not recognised'%key)\n",
    "\n",
    "    def filter_by_seq_length(self, locus_name, min_length=0, max_length=None):\n",
    "        if self.records_by_locus == {}:\n",
    "            self.extract_by_locus()\n",
    "        subset = [r for r in self.records_by_locus[locus_name] if len(r) >= min_length]\n",
    "        if max_length:\n",
    "            subset = [r for r in subset if len(r) <= max_length]\n",
    "        self.records_by_locus[locus_name] = subset\n",
    "        \n",
    "    def filter_by_gc_content(self, locus_name, min_percent_gc=0, max_percent_gc=None):\n",
    "        if self.records_by_locus == {}:\n",
    "            self.extract_by_locus()\n",
    "        subset = [r for r in self.records_by_locus[locus_name] if GC(r.seq) >= min_percent_gc]\n",
    "        if max_percent_gc:\n",
    "            subset = [r for r in subset if GC(r.seq) <= max_percent_gc]\n",
    "        self.records_by_locus[locus_name] = subset\n",
    "                \n",
    "\n",
    "    def write_by_locus(self, format = 'fasta'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Write the unaligned sequences into file in any Biopython format, one file per locus\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.records_by_locus == {}:\n",
    "            self.extract_by_locus\n",
    "        for key in self.records_by_locus.keys():\n",
    "            SeqIO.write(self.records_by_locus[key], key+'.'+format, format)\n",
    "\n",
    "\n",
    "\n",
    "    def align(self, alignment_methods=[], pal2nal='defaults'):\n",
    "        \n",
    "            \"\"\"\n",
    "            Configured by an AlnConf object\n",
    "            \"\"\"\n",
    "            if pal2nal == 'defaults':\n",
    "                pal2nal = self.defaults['pal2nal']\n",
    "            seen_loci = []\n",
    "            for method in alignment_methods:\n",
    "                method.timeit.append(time.time())\n",
    "                method.platform = platform_report()\n",
    "                if method.program_name == 'muscle':\n",
    "                    method.platform.append('Program and version: '+os.popen(method.cmd + ' -version').read())\n",
    "                elif method.program_name == 'mafft':\n",
    "                    p = sub.Popen(method.cmd+\" --version\", shell=True, stderr=sub.PIPE, stdout=sub.PIPE)\n",
    "                    method.platform.append('Program and version: '+p.communicate()[1].splitlines()[3])\n",
    "                for locus in method.loci:\n",
    "                    if locus.name in seen_loci:\n",
    "                        #raise RuntimeError('locus '+locus.name+' is in more than one AlnConf objects')\n",
    "                        pass\n",
    "                    else:\n",
    "                        seen_loci.append(locus.name)\n",
    "                    stdout, stderr = method.command_lines[locus.name]()\n",
    "                    align = AlignIO.read(StringIO(stdout), \"fasta\",  alphabet=IUPAC.protein)\n",
    "                    if method.CDSAlign and locus.feature_type == 'CDS' and locus.char_type == 'dna':\n",
    "                        for seq in align:\n",
    "                            found = 0\n",
    "                            for s in method.CDS_in_frame[locus.name]:\n",
    "                                if s.id == seq.id:\n",
    "                                    found = 1\n",
    "                            if found == 0:\n",
    "                                raise RuntimeError(seq.id + ' is not in the CDS sequences')\n",
    "                        for s in method.CDS_in_frame[locus.name]:\n",
    "                            found = 0\n",
    "                            for seq in align:\n",
    "                                if s.id == seq.id:\n",
    "                                    found = 1\n",
    "                            if found == 0:\n",
    "                                raise RuntimeError(seq.id + ' is not in the protein sequences')\n",
    "                        for seq in method.CDS_in_frame[locus.name]:    \n",
    "                            for prot in align:\n",
    "                                if prot.id == seq.id:\n",
    "                                    i = 0\n",
    "                                    for p in str(prot.seq):\n",
    "                                        if not p == '-':\n",
    "                                            i += 1\n",
    "                                    if not i*3 == len(seq.seq):\n",
    "                                        raise RuntimeError('nuc and prot seqs have unmatched lengths for '+seq.id)\n",
    "                        aln_filename = method.id+'_'+locus.name+'.aln'\n",
    "                        AlignIO.write(align, aln_filename, 'fasta')\n",
    "                        cds_filename = method.id+'_CDS_in_frame_'+locus.name+'.fasta'\n",
    "                        stdout = os.popen(pal2nal+' '+aln_filename+' '+cds_filename + ' -nostderr -codontable %i'%method.codontable).read()\n",
    "                        align = AlignIO.read(StringIO(stdout), \"clustal\",  alphabet=IUPAC.ambiguous_dna)\n",
    "                        #from Bio import CodonAlign\n",
    "                        #codon_aln = CodonAlign.build(align, method.CDS_in_frame[locus.name])\n",
    "                        #align = codon_aln\n",
    "                    method_files = glob.glob(method.id+'_*')\n",
    "                    [summary_lines, num_lines, num_undeter, num_collapsed_aln_seqs] = aln_summary(align)\n",
    "                    summary = 'Alignment name: '+locus.name+'@'+method.method_name+'\\n'\n",
    "                    for line in summary_lines:\n",
    "                        summary += line+'\\n'\n",
    "                    if num_lines < 4:\n",
    "                        line = 'Alignment %s has less than 4 sequences and will be dropped'%(locus.name+'@'+method.method_name)\n",
    "                        print line\n",
    "                        summary += line+'\\n'\n",
    "                    elif num_undeter[0] > 0:\n",
    "                        line = 'Alignment %s has undetermined sequences and will be dropped'%(locus.name+'@'+method.method_name)\n",
    "                        print line\n",
    "                        summary += line+'\\n'\n",
    "                    elif num_collapsed_aln_seqs < 4:\n",
    "                        line = 'Alignment %s has less than 4 unique sequences and will be dropped'%(locus.name+'@'+method.method_name)\n",
    "                        print line\n",
    "                        summary += line+'\\n'\n",
    "                        \n",
    "                    else:\n",
    "                        self.alignments[locus.name+'@'+method.method_name] = align\n",
    "                    self.aln_summaries.append(summary)\n",
    "                method.timeit.append(time.time())\n",
    "                method.timeit.append(method.timeit[2]-method.timeit[1])\n",
    "                for f in method_files:\n",
    "                    os.remove(f)\n",
    "            self.used_methods += alignment_methods\n",
    "\n",
    "\n",
    "\n",
    "    def write_alns(self, id=['feature_id'], format = 'fasta'):\n",
    "        filenames = []\n",
    "        if len(self.alignments.keys()) == 0:\n",
    "            raise IOError('Align the records first')\n",
    "        else:\n",
    "            for key in self.alignments:\n",
    "                aln = self.alignments[key]\n",
    "                records = []\n",
    "                for s in aln:\n",
    "                    qualifiers = get_qualifiers_dictionary(self, s.id)\n",
    "                    new_id = \"\"\n",
    "                    for i in id:\n",
    "                        if i in qualifiers.keys():\n",
    "                            new_id += qualifiers[i]+'_'\n",
    "                    if new_id == \"\":\n",
    "                        new_id = s.id\n",
    "                    else:\n",
    "                        new_id = new_id[:-1]\n",
    "                    records.append(SeqRecord(seq=s.seq, id=new_id))\n",
    "                SeqIO.write(records, key+'_aln.'+format, format)\n",
    "                filenames.append(key+'_aln.'+format)\n",
    "        return filenames\n",
    "\n",
    "\n",
    "\n",
    "    def write_trimmed_alns(self, id=['feature_id'], format = 'fasta'):\n",
    "        filenames = []\n",
    "        if len(self.trimmed_alignments.keys()) == 0:\n",
    "            raise IOError('Align and trimmed the records first')\n",
    "        else:\n",
    "            for key in self.trimmed_alignments.keys():\n",
    "                aln = self.trimmed_alignments[key]\n",
    "                records = []\n",
    "                for s in aln:\n",
    "                    qualifiers = get_qualifiers_dictionary(self, s.id)\n",
    "                    new_id = \"\"\n",
    "                    for i in id:\n",
    "                        if i in qualifiers.keys():\n",
    "                            new_id += qualifiers[i]+'_'\n",
    "                    if new_id == \"\":\n",
    "                        new_id = s.id\n",
    "                    else:\n",
    "                        new_id = new_id[:-1]\n",
    "                    records.append(SeqRecord(seq=s.seq, id=new_id))\n",
    "                SeqIO.write(records, key+'_trimmed_aln.'+format, format)\n",
    "                filenames.append(key+'_trimmed_aln.'+format)\n",
    "        return filenames\n",
    "\n",
    "    def show_aln(self, token, id=['feature_id']):\n",
    "        aln_obj=None\n",
    "        if token in self.alignments.keys():\n",
    "            aln_obj = self.alignments[token]\n",
    "        elif token in self.trimmed_alignments.keys():\n",
    "            aln_obj = self.trimmed_alignments[token]\n",
    "        locus_name = token.split('@')[0]\n",
    "        char_type = [locus.char_type for locus in self.loci if locus.name == locus_name][0]\n",
    "        \n",
    "        records = []\n",
    "        for s in aln_obj:\n",
    "            qualifiers = get_qualifiers_dictionary(self, s.id)\n",
    "            new_id = \"\"\n",
    "            for i in id:\n",
    "                if i in qualifiers.keys():\n",
    "                    new_id += qualifiers[i]+'_'\n",
    "            if new_id == \"\":\n",
    "                new_id = s.id\n",
    "            else:\n",
    "                new_id = new_id[:-1]\n",
    "            records.append(SeqRecord(seq=s.seq, id=new_id))\n",
    "            \n",
    "        title_length = max([len(r.id) for r in records])+2\n",
    "        \n",
    "        # colors\n",
    "        dna_colors = {'a':'green',\n",
    "                      'A':'green',\n",
    "                      'T':'red',\n",
    "                      't':'red',\n",
    "                      'U':'red',\n",
    "                      'u':'red',\n",
    "                      'g':'gray',\n",
    "                      'G':'gray',\n",
    "                      'c':'blue',\n",
    "                      'C':'blue'\n",
    "                      }\n",
    "        protein_colors = {'R':'blueviolet',\n",
    "                          'r':'blueviolet',\n",
    "                          \n",
    "                          'K':'cornflowerblue',\n",
    "                          'k':'cornflowerblue',\n",
    "                          \n",
    "                          'E':'red',\n",
    "                          'e':'red',\n",
    "                          \n",
    "                          'D':'crimson',\n",
    "                          'd':'crimson',\n",
    "                          \n",
    "                          'I':'gold',\n",
    "                          'i':'gold',\n",
    "                          \n",
    "                          'L':'yellow',\n",
    "                          'l':'yellow',\n",
    "                          \n",
    "                          'V':'moccasin',\n",
    "                          'v':'moccasin',\n",
    "                          \n",
    "                          'A':'lemonchiffon',\n",
    "                          'a':'lemonchiffon',\n",
    "                          \n",
    "                          'C':'palegreen',\n",
    "                          'c':'palegreen',\n",
    "                          \n",
    "                          'H':'paleturquoise',\n",
    "                          'h':'paleturquoise',\n",
    "                          \n",
    "                          'M':'hotpink',\n",
    "                          'm':'hotpink',\n",
    "                          \n",
    "                          'N':'pink',\n",
    "                          'n':'pink',\n",
    "                          \n",
    "                          'Q':'yellow',\n",
    "                          'q':'yellow',\n",
    "                          \n",
    "                          'F':'darkseagreen',\n",
    "                          'f':'darkseagreen',\n",
    "                          \n",
    "                          'Y':'darkcyan',\n",
    "                          'y':'darkcyan',\n",
    "                          \n",
    "                          'W':'steelblue',\n",
    "                          'w':'steelblue',\n",
    "                          \n",
    "                          \n",
    "                          'S':'thistle',\n",
    "                          's':'thistle',\n",
    "                          \n",
    "                          'T':'lavender',\n",
    "                          't':'lavender',\n",
    "                          \n",
    "                          'G':'darkgray',\n",
    "                          'g':'darkgray',\n",
    "                          \n",
    "                          'P':'gainsboro',\n",
    "                          'p':'gainsboro',\n",
    "                          }\n",
    "        colors = None\n",
    "        if char_type == 'dna':\n",
    "            colors = dna_colors\n",
    "        elif char_type == 'prot':\n",
    "            colors = protein_colors\n",
    "        linelength = (len(records[0].seq)+len(records[0].id)+3)*10\n",
    "        html_string = '<html><head></head>\\n'\n",
    "        html_string += '<body><pre><font face=\"Courier New\">\\n'\n",
    "        for r in records:\n",
    "            html_string += r.id.ljust(title_length, '.')\n",
    "            for p in str(r.seq):\n",
    "                c = 'white'\n",
    "                if p in colors.keys():\n",
    "                    c = colors[p]\n",
    "                html_string += '<font style=\"BACKGROUND-COLOR: %s\">%s</font>'%(c, p)\n",
    "            html_string += \"<br>\"\n",
    "        html_string +=  '</font></pre></body></html>' \n",
    "        \n",
    "        import webbrowser\n",
    "        path = os.path.abspath(\"%s.html\"%token)\n",
    "        url = 'file://' + path\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(html_string)\n",
    "        webbrowser.open_new_tab(url)\n",
    "        \n",
    "    \n",
    "    def tree(self, raxml_methods, bpcomp='default', bpcomp_burnin=0.2, bpcomp_step=10, bpcomp_cutoff=0.01):\n",
    "        # to do: determine the program used and the resulting expected tree file name\n",
    "        \n",
    "        if bpcomp == 'default':\n",
    "            bpcomp = self.defaults['bpcomp']\n",
    "        \n",
    "        for raxml_method in raxml_methods:\n",
    "            raxml_method.timeit.append(time.time())\n",
    "            raxml_method.platform = platform_report()\n",
    "            if isinstance(raxml_method, RaxmlConf):\n",
    "                raxml_method.platform.append('Program and version: '+ raxml_method.cmd + ': ' +\n",
    "                                             os.popen(raxml_method.cmd + ' -version').readlines()[2])\n",
    "            elif isinstance(raxml_method, PbConf):\n",
    "                p = sub.Popen(raxml_method.cmd+\" -v\", shell=True, stderr=sub.PIPE, stdout=sub.PIPE)\n",
    "                raxml_method.platform.append('Program and version: '+p.communicate()[1].splitlines()[-1])\n",
    "            for trimmed_alignment in raxml_method.command_lines.keys():\n",
    "                for cline in raxml_method.command_lines[trimmed_alignment]:\n",
    "                    if isinstance(raxml_method, RaxmlConf):\n",
    "                        stdout, stderr = cline()\n",
    "                    elif isinstance(raxml_method, PbConf):\n",
    "                        sub.call(cline, shell=True)\n",
    "                t = None\n",
    "                if isinstance(raxml_method, RaxmlConf):\n",
    "                    if raxml_method.preset == 'fa':\n",
    "                        t = Tree('RAxML_bipartitions.'+raxml_method.id+'_'+trimmed_alignment+'0')\n",
    "                    elif raxml_method.preset == 'fD_fb':\n",
    "                        t = Tree('RAxML_bipartitions.'+raxml_method.id+'_'+trimmed_alignment+'1')\n",
    "                    elif raxml_method.preset == 'fd_b_fb':\n",
    "                        t = Tree('RAxML_bipartitions.'+raxml_method.id+'_'+trimmed_alignment+'2')\n",
    "                    elif raxml_method.preset == 'fd_fJ' or raxml_method.preset == 'fF_fJ':\n",
    "                        tree_filename = 'RAxML_fastTreeSH_Support.'+raxml_method.id+'_'+trimmed_alignment+'1'\n",
    "                        t = Tree(open(tree_filename,'r').read().replace('[','[&&NHX:support='))\n",
    "                        \n",
    "                elif isinstance(raxml_method, PbConf):\n",
    "                    base_name = \"%s_%s\"%(raxml_method.id, trimmed_alignment)\n",
    "                    trees_file = \"%s.1.treelist\"%base_name\n",
    "                    trees_per_chain = len(open(trees_file,'r').readlines())\n",
    "                    # find the number of chains\n",
    "                    nchains = raxml_method.cline_args['nchain'].split()[0]\n",
    "                    chain_names = ''\n",
    "                    for i in range(1,str(nchains)+1):\n",
    "                        chain_names += \"%s.%i \"%(base_name, i)\n",
    "                    chain_names = chain_names[:-1]\n",
    "                    bpcomp_cline = \"%s -c %f -x %i %i %s\"%(bpcomp,\n",
    "                                                           bpcomp_cutoff,\n",
    "                                                           int(trees_per_chain*bpcomp_burnin),\n",
    "                                                           int(bpcomp_step),\n",
    "                                                           chain_names)\n",
    "                    sub.call(bpcomp_cline, shell=True)\n",
    "                    t = Tree(\"bpcomp.con.tre\")\n",
    "                    for l in t:\n",
    "                        l.support = 0\n",
    "                    \n",
    "            \n",
    "                for n in t.traverse():\n",
    "                    n.add_feature('tree_method_id', str(raxml_method.id)+'_'+trimmed_alignment)\n",
    "                t.dist = 0\n",
    "                t.add_feature('tree_method_id', str(raxml_method.id)+'_'+trimmed_alignment)\n",
    "                \n",
    "           \n",
    "                loci_names = [i.name for i in  self.loci]       \n",
    "                concat_names = [c.name for c in self.concatenations]\n",
    "                if trimmed_alignment.partition('@')[0] in loci_names:\n",
    "                        \n",
    "                        for leaf in t:\n",
    "                            records = self.records\n",
    "                            feature = ''\n",
    "                            feature_source = ''\n",
    "                            record = ''\n",
    "                            for r in records:\n",
    "                                if r.id in leaf.name:\n",
    "                                    record = r\n",
    "                                    for f in r.features:\n",
    "                                        if f.type == 'source':\n",
    "                                            feature_source = f\n",
    "                                        elif f.qualifiers['feature_id'][0] == leaf.name:\n",
    "                                            feature = f\n",
    "                            for a in record.annotations.keys():\n",
    "                                label = 'annotation_'+a\n",
    "                                leaf.add_feature(label, record.annotations[a])\n",
    "                            for f_source_qual in feature_source.qualifiers.keys():\n",
    "                                label = 'source_'+f_source_qual\n",
    "                                leaf.add_feature(label, feature_source.qualifiers[f_source_qual][0])\n",
    "                            for f_qual in feature.qualifiers.keys():\n",
    "                                leaf.add_feature(f_qual, feature.qualifiers[f_qual][0])\n",
    "                        for l in t:\n",
    "                            t.add_feature('tree_id', trimmed_alignment+'@'+raxml_method.method_name)\n",
    "                        self.trees[trimmed_alignment+'@'+raxml_method.method_name] = [t,t.write(features=[])]\n",
    "                        \n",
    "                elif trimmed_alignment in concat_names:\n",
    "                        s = filter(lambda i: i.name == trimmed_alignment, self.concatenations)[0]\n",
    "                        for leaf in t:\n",
    "                            records = self.records\n",
    "                            feature = ''\n",
    "                            feature_source = ''\n",
    "                            record = ''\n",
    "                            for r in records:\n",
    "                                for feature in r.features:\n",
    "                                    if not feature.type == 'source':\n",
    "                                        qual_dict = get_qualifiers_dictionary(self, feature.qualifiers['feature_id'])\n",
    "                                        if s.otu_meta in qual_dict.keys() and qual_dict[s.otu_meta] == leaf.name:\n",
    "                                            for key in qual_dict.keys():\n",
    "                                                leaf.add_feature(key, qual_dict[key])\n",
    "                        for l in t:\n",
    "                            t.add_feature('tree_id', s.name+'@mixed@mixed@'+raxml_method.method_name)\n",
    "                        self.trees[s.name+'@mixed@mixed@'+raxml_method.method_name] = [t,t.write(features=[])]\n",
    "            raxml_method.timeit.append(time.time())\n",
    "            raxml_method.timeit.append(raxml_method.timeit[2]-raxml_method.timeit[1])\n",
    "            if not raxml_method.keepfiles:\n",
    "                for file_name in os.listdir(os.curdir):\n",
    "                            if raxml_method.id.partition('_')[0] in file_name:\n",
    "                                os.remove(file_name)\n",
    "        self.used_methods += raxml_methods\n",
    "\n",
    "\n",
    "\n",
    "    def clear_tree_annotations(self):\n",
    "        for tree in self.trees.keys():\n",
    "            t = Tree(self.trees[tree][1])\n",
    "            t.dist = 0\n",
    "            self.trees[tree][0] = t\n",
    "\n",
    "\n",
    "\n",
    "    def write_nexml(self, output_name):\n",
    "        D = dendropy.DataSet()\n",
    "        tree_list = []\n",
    "        \n",
    "        loci_names = []\n",
    "        for locus in self.loci:\n",
    "            loci_names.append(locus.name)\n",
    "        \n",
    "        for tree_name in self.trees.keys():\n",
    "            #get aligned and trimmd aligned sequences as leaf features\n",
    "            t = self.trees[tree_name][0]\n",
    "            for l in t:\n",
    "                loc_name = tree_name.split('@')[0]\n",
    "                trim_aln_name = tree_name.rpartition('@')[0]\n",
    "                aln_name = None\n",
    "                if loc_name in loci_names:\n",
    "                    aln_name = tree_name.rsplit('@',2)[0]\n",
    "                else:\n",
    "                    trim_aln_name = trim_aln_name.split('@')[0]\n",
    "                \n",
    "                otu_feature = 'feature_id'\n",
    "                if not aln_name: # then it is a concatenation\n",
    "                    for c in self.concatenations:\n",
    "                        if c.name == loc_name:\n",
    "                            otu_feature = c.otu_meta\n",
    "                            \n",
    "                if aln_name: # Then it is a locus\n",
    "                    leaf_feature_value = getattr(l, otu_feature)\n",
    "                    alignment = self.alignments[aln_name]\n",
    "                    for record in alignment:\n",
    "                        if record.id == leaf_feature_value:\n",
    "                            l.add_feature('aligned_sequence',str(record.seq))\n",
    "                t_aln = self.trimmed_alignments[trim_aln_name]\n",
    "                    \n",
    "                leaf_feature_value = getattr(l, otu_feature)\n",
    "                for record in t_aln:\n",
    "                    if record.id == leaf_feature_value:\n",
    "                        l.add_feature('aligned_trimmed_sequence',str(record.seq))\n",
    "                    \n",
    "            tree_string = self.trees[tree_name][0].write(features=[])\n",
    "            tree = dendropy.Tree()\n",
    "            tree.read_from_string(tree_string, schema='newick', extract_comment_metadata = True)\n",
    "            tree_list.append(tree)\n",
    "        TL = dendropy.TreeList(tree_list)    \n",
    "        D.add_tree_list(TL)\n",
    "            \n",
    "        D.write_to_path(\n",
    "            output_name,\n",
    "            'nexml',\n",
    "            suppress_annotations=False,\n",
    "            annotations_as_nhx=False,\n",
    "            exclude_trees=False)\n",
    "\n",
    "\n",
    "\n",
    "    def annotate(self, fig_folder,\n",
    "    \n",
    "                 root_meta,\n",
    "                 root_value,\n",
    "    \n",
    "                 leaf_labels_txt_meta,\n",
    "                 leaf_node_color_meta=None,\n",
    "                 leaf_label_colors=None,\n",
    "    \n",
    "                 node_bg_meta=None,\n",
    "                 node_bg_color=None,\n",
    "                 \n",
    "                 node_support_dict=None,\n",
    "                 \n",
    "                 heat_map_meta = None, #list\n",
    "                 heat_map_colour_scheme=2,\n",
    "                 \n",
    "                 multifurc=None,\n",
    "                 \n",
    "                 scale = 1000,\n",
    "                 \n",
    "                 html = None\n",
    "                 ): \n",
    "            \n",
    "            stdout = sys.stdout\n",
    "            if html:\n",
    "                sys.stdout = open(html, 'wt')\n",
    "                \n",
    "            print '<html>'\n",
    "            ts = TreeStyle()\n",
    "            ts.show_leaf_name = False\n",
    "            ts.scale = scale\n",
    "            if node_support_dict:\n",
    "                ts.legend_position=1\n",
    "                ts.legend.add_face(TextFace('Node support: ', fsize=10), column=0)\n",
    "                i = 1\n",
    "                for color in sorted(node_support_dict.items(),key=lambda i: i[1][0], reverse=True):\n",
    "                    ts.legend.add_face(CircleFace(radius = 4, color = color[0]), column=i)\n",
    "                    i +=1 \n",
    "                    ts.legend.add_face(TextFace(' '+str(node_support_dict[color[0]][0])+'-'+str(node_support_dict[color[0]][1]),\n",
    "                                                fsize=10), column=i)\n",
    "                    i += 1\n",
    "                \n",
    "            for tree in self.trees.keys():\n",
    "                                       \n",
    "                # set outgroup leaves, labels and label colors\n",
    "                outgroup_list = []\n",
    "                all_heatmap_profile_values = []\n",
    "                leaves_for_heatmap = []\n",
    "                \n",
    "                for leaf in self.trees[tree][0]:\n",
    "                    qualifiers_dictionary = get_qualifiers_dictionary(self, leaf.feature_id)\n",
    "                    leaf_label = ''\n",
    "                    for meta in leaf_labels_txt_meta:\n",
    "                        leaf_label += qualifiers_dictionary[meta]+' '\n",
    "                    leaf_label = leaf_label[:-1]\n",
    "                    fgcolor = 'black'\n",
    "                    if leaf_label_colors:\n",
    "                        for colour_name in leaf_label_colors.keys():\n",
    "                            if colour_name in qualifiers_dictionary[leaf_node_color_meta]:\n",
    "                                fgcolor = leaf_label_colors[colour_name]\n",
    "                    leaf_face = TextFace(leaf_label, fgcolor=fgcolor)\n",
    "                    leaf.add_face(leaf_face,0)\n",
    "                    if not root_value == 'mid' and root_meta in qualifiers_dictionary.keys() and root_value in qualifiers_dictionary[root_meta]:\n",
    "                        outgroup_list.append(leaf)\n",
    "                        \n",
    "                    if heat_map_meta:\n",
    "                        include = True\n",
    "                        for i in heat_map_meta:\n",
    "                            if not i in qualifiers_dictionary:\n",
    "                                include = False\n",
    "                        if include:\n",
    "                            profile = []\n",
    "                            deviation = []\n",
    "                            for meta in heat_map_meta:\n",
    "                                if meta in qualifiers_dictionary.keys():\n",
    "                                    profile.append(float(qualifiers_dictionary[meta]))\n",
    "                                    all_heatmap_profile_values.append(float(qualifiers_dictionary[meta]))\n",
    "                                    deviation.append(0.0)\n",
    "                            leaf.add_features(profile=profile)\n",
    "                            leaf.add_features(deviation=deviation)\n",
    "                            leaves_for_heatmap.append(leaf)\n",
    "                for leaf in leaves_for_heatmap:\n",
    "                    leaf.add_face(ProfileFace(max_v=float(max(all_heatmap_profile_values)),\n",
    "                                              min_v=float(min(all_heatmap_profile_values)), \n",
    "                                              center_v=float(float(max(all_heatmap_profile_values)+min(all_heatmap_profile_values))/2),\n",
    "                                              width=50, height=30,\n",
    "                                              style='heatmap',\n",
    "                                              colorscheme=heat_map_colour_scheme),\n",
    "                                    column=1, position=\"aligned\")\n",
    "                        \n",
    "                        \n",
    "                #set outgroup\n",
    "                if outgroup_list == ['mid']:\n",
    "                    try:\n",
    "                        R = self.trees[tree][0].get_midpoint_outgroup()\n",
    "                        self.trees[tree][0].set_outgroup(R)\n",
    "                        print 'rooting tree '+tree+' at midpoint'\n",
    "                    except:\n",
    "                        print 'root in '+tree+' already set correctly?'\n",
    "                    \n",
    "                elif len(outgroup_list) == 1:\n",
    "                    try:\n",
    "                        self.trees[tree][0].set_outgroup(outgroup_list[0])\n",
    "                    except:\n",
    "                        print 'root in '+tree+' already set correctly?'\n",
    "                elif len(outgroup_list) > 1:\n",
    "                    try:\n",
    "                        R = self.trees[tree][0].get_common_ancestor(outgroup_list)\n",
    "                        self.trees[tree][0].set_outgroup(R)\n",
    "                    except:\n",
    "                        print 'root in '+tree+' already set correctly?'\n",
    "                elif len(outgroup_list)==0:\n",
    "                    try:\n",
    "                        R = self.trees[tree][0].get_midpoint_outgroup()\n",
    "                        self.trees[tree][0].set_outgroup(R)\n",
    "                        print 'rooting tree '+tree+' at midpoint'\n",
    "                    except:\n",
    "                        print 'root in '+tree+' already set correctly?'\n",
    "    \n",
    "                # ladderize\n",
    "                self.trees[tree][0].ladderize()\n",
    "            \n",
    "                ns = NodeStyle()\n",
    "                ns['size'] = 0\n",
    "                ns['fgcolor'] = 'black'\n",
    "                ns['vt_line_width'] = 2\n",
    "                ns['hz_line_width'] = 2\n",
    "                ns['hz_line_color'] = 'DimGray'\n",
    "                ns['vt_line_color'] = 'DimGray'\n",
    "                for n in self.trees[tree][0].traverse():\n",
    "                    n.set_style(ns)\n",
    "                self.trees[tree][0].set_style(ns)\n",
    "            \n",
    "                if multifurc:\n",
    "                    for n in self.trees[tree][0].traverse():\n",
    "                        if n.support < multifurc and not n.is_leaf():\n",
    "                            n.delete()\n",
    "    \n",
    "                # node bg colors\n",
    "                if node_bg_color:\n",
    "                    for key in node_bg_color.keys():\n",
    "                        for node in self.trees[tree][0].get_monophyletic(values=[key], target_attr=node_bg_meta):\n",
    "                            ns = NodeStyle(bgcolor=node_bg_color[key])\n",
    "                            ns['size']=0\n",
    "                            ns['fgcolor']='black'\n",
    "                            ns['vt_line_width'] = 2\n",
    "                            ns['hz_line_width'] = 2\n",
    "                            ns['hz_line_color'] = 'DimGray'\n",
    "                            ns['vt_line_color'] = 'DimGray'\n",
    "                            node.set_style(ns)\n",
    "                \n",
    "    \n",
    "                # node support\n",
    "                if node_support_dict:\n",
    "                    for node in self.trees[tree][0].traverse():\n",
    "                        for key in node_support_dict.keys():\n",
    "                            if (node.support <= node_support_dict[key][0] and\n",
    "                                node.support > node_support_dict[key][1]):\n",
    "                                node.add_face(CircleFace(radius = 5, color = key),column=0, position = \"float\")             \n",
    "                    \n",
    "                self.trees[tree][0].render(fig_folder + \"/\"+self.trees[tree][0].get_leaves()[0].tree_method_id+'.png',w=1000, tree_style=ts)\n",
    "                print('<A href='+\n",
    "                       fig_folder + \"/\" + self.trees[tree][0].get_leaves()[0].tree_method_id+'.png'+\n",
    "                       '>'+self.trees[tree][0].get_leaves()[0].tree_method_id+\n",
    "                       '</A><BR>')\n",
    "            print '</html>'\n",
    "            print fig_folder\n",
    "            sys.stdout = stdout\n",
    "     \n",
    "            \n",
    "    def trim(self, list_of_Conf_objects, cutoff=0):\n",
    "        for m in list_of_Conf_objects:\n",
    "            m.timeit.append(time.time())\n",
    "            m.platform = platform_report() \n",
    "            m.platform.append('Program and version: '+ m.cmd + ': ' +\n",
    "                               os.popen(m.cmd + ' --version').readlines()[1])\n",
    "            if isinstance(m, TrimalConf):\n",
    "                import subprocess as sub\n",
    "                for aln in m.command_lines.keys():\n",
    "                    p = sub.Popen(m.command_lines[aln], shell=True, stdout=sub.PIPE)\n",
    "                    stdout, stderr = p.communicate()\n",
    "                    #stdout = os.system(m.command_lines[aln]).stdout\n",
    "                    alphabet = IUPAC.ambiguous_dna\n",
    "                    locus_name = aln.split('@')[0]\n",
    "                    for locus in self.loci:\n",
    "                        if locus.name == locus_name and locus.char_type == 'prot':\n",
    "                            alphabet = IUPAC.protein\n",
    "                    align = AlignIO.read(StringIO(stdout), \"fasta\",  alphabet=alphabet)\n",
    "                    [summary_lines, num_lines, num_undeter, num_collapsed_aln_seqs] = aln_summary(align,\n",
    "                                                                                                  cutoff=cutoff)\n",
    "                    summary = 'Alignment name: '+aln+'\\n'\n",
    "                    for line in summary_lines:\n",
    "                        summary += line+'\\n'\n",
    "                    if num_lines < 4:\n",
    "                        line = 'Alignment %s has less than 4 sequences and will be dropped'%aln\n",
    "                        print line\n",
    "                        summary += line+'\\n'\n",
    "                    elif num_undeter[0] > 0:\n",
    "                        line = 'Alignment %s has undetermined sequences (%i bp or less) which will be dropped: %s'%(aln, cutoff+1, num_undeter[1])\n",
    "                        print line\n",
    "                        summary += line+'\\n'\n",
    "                        records_wo_undeter = []\n",
    "                        for record in align:\n",
    "                            if not record.id in num_undeter[1]:\n",
    "                                records_wo_undeter.append(record)\n",
    "                        align =  MultipleSeqAlignment(records_wo_undeter)\n",
    "                        self.trimmed_alignments[aln] = align\n",
    "                    elif num_collapsed_aln_seqs < 4:\n",
    "                        line = 'Alignment %s has less than 4 unique sequences and will be dropped'%aln\n",
    "                        print line\n",
    "                        summary += line+'\\n'\n",
    "                    else:\n",
    "                        self.trimmed_alignments[aln] = align\n",
    "                    self.aln_summaries.append(summary)\n",
    "            for file_name in os.listdir(os.curdir):\n",
    "                if m.id.partition('_')[0] in file_name:\n",
    "                    os.remove(file_name)\n",
    "            m.timeit.append(time.time())\n",
    "            m.timeit.append(m.timeit[2]-m.timeit[1])\n",
    "            self.used_methods.append(m)\n",
    "\n",
    "\n",
    "    def report_seq_stats(self):        \n",
    "        if len(self.records_by_locus.keys())>0:\n",
    "\n",
    "            # This will make a list of seq length for each locus. Seq length are calced\n",
    "            # using the record.seq in 'pj.records_by_locus'. 'pj.records_by_locus is a\n",
    "            # dict with loci names as keys, and lists of SeqReocrd objects as values\n",
    "            lengths_dict = {}\n",
    "            for locus_name in self.records_by_locus.keys():\n",
    "                lengths_dict[locus_name] = []\n",
    "                for record in self.records_by_locus[locus_name]:\n",
    "                    lengths_dict[locus_name].append(len(record.seq))\n",
    "                    \n",
    "            print \"Distribution of sequence lengths\".title()\n",
    "            draw_boxplot(lengths_dict, 'Seq length (bp)', 'inline')\n",
    "            \n",
    "            \n",
    "            for stat in ['GC_content']:#, 'nuc_degen_prop', 'prot_degen_prop'):\n",
    "                title = 'Distribution of sequence statistic \\\"'+stat+'\\\"'\n",
    "                print title.title()\n",
    "                # This will make a dict with loci as keys and a list of stat values as\n",
    "                # dict values.\n",
    "                stat_dict = {}\n",
    "                ylabel = 'GC ontent (%)'\n",
    "                if not stat == 'GC_content':\n",
    "                    ylabel = 'Ambiguous positions (prop)'\n",
    "                for locus_name in self.records_by_locus.keys():\n",
    "                    stat_dict[locus_name] = []\n",
    "                    for i in self.records_by_locus[locus_name]:\n",
    "                        for record in self.records:\n",
    "                            for feature in record.features:\n",
    "                                if feature.qualifiers['feature_id'][0] == i.id:\n",
    "                                    if stat in feature.qualifiers.keys():\n",
    "                                        stat_dict[locus_name].append(float(feature.qualifiers[stat][0]))\n",
    "                \n",
    "                draw_boxplot(stat_dict, ylabel, 'inline')\n",
    "                \n",
    "    ##################################               \n",
    "    # Project methods to fetch objects\n",
    "    ##################################\n",
    "    \n",
    "    def ft(self, token):\n",
    "        \n",
    "        \"\"\"\n",
    "        Will fetch the tree object which has the token in \n",
    "        its key, as long as there is only one\n",
    "        \"\"\"\n",
    "        \n",
    "        # check how many tree keys match the token\n",
    "        keys = [key for key in self.trees.keys() if token in key]\n",
    "        if len(keys) > 1:\n",
    "            raise IOError(\"The token %s was found in more then one tree key: %s\"\n",
    "                           %(token, str(keys)))\n",
    "        elif len(keys) == 0:\n",
    "            raise IOError(\"The token %s was not found in any tree key\"\n",
    "                           %token)\n",
    "        elif len(keys) == 1:\n",
    "            print \"returning tree object %s\"%keys[0]\n",
    "            return Tree(self.trees[keys[0]][1])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fa(self, token):\n",
    "        \n",
    "        \"\"\"\n",
    "        Will fetch the alignment object which has the token in \n",
    "        its key, as long as there is only one\n",
    "        \"\"\"\n",
    "        \n",
    "        # check how many aln keys match the token\n",
    "        keys = [key for key in self.alignments.keys() if token in key]\n",
    "        if len(keys) > 1:\n",
    "            raise IOError(\"The token %s was found in more then one alignment key: %s\"\n",
    "                          %(token, str(keys)))\n",
    "        elif len(keys) == 0:\n",
    "            raise IOError(\"The token %s was not found in any alignment key\"\n",
    "                          %token)\n",
    "        elif len(keys) == 1:\n",
    "            print \"returning alignment object %s\"%keys[0]\n",
    "            return self.alignments[keys[0]]        \n",
    "        \n",
    "        \n",
    "    def fta(self, token):\n",
    "        \n",
    "        \"\"\"\n",
    "        Will fetch the trimmed alignment object which has the token in \n",
    "        its key, as long as there is only one\n",
    "        \"\"\"\n",
    "        \n",
    "        # check how many trimmed aln keys match the token\n",
    "        keys = [key for key in self.trimmed_alignments.keys() if token in key]\n",
    "        if len(keys) > 1:\n",
    "            raise IOError(\"The token %s was found in more then one trimmed alignment key: %s\"\n",
    "                          %(token, str(keys)))\n",
    "        elif len(keys) == 0:\n",
    "            raise IOError(\"The token %s was not found in any trimmed alignment key\"\n",
    "                          %token)\n",
    "        elif len(keys) == 1:\n",
    "            print \"returning trimmed alignment object %s\"%keys[0]\n",
    "            return self.trimmed_alignments[keys[0]]            \n",
    "        \n",
    "        \n",
    "    def fr(self, locus_name, filter=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Will fetch the record objects of the specified locus, \n",
    "        as long as there is at least one.\n",
    "        filter should be a list of lists. Every (sub)list is\n",
    "        a pair of qualifier and value. If filter is specified,\n",
    "        only records that have all the specified values in the\n",
    "        specified qualifiers will be kept.\n",
    "        \"\"\"\n",
    "        \n",
    "        # check how many record keys match the token\n",
    "        keys = [key for key in self.records_by_locus.keys() if locus_name in key]\n",
    "        if len(keys) > 1:\n",
    "            raise IOError(\"The locus name %s fits more then one locus: %s\"\n",
    "                          %(locus_name, str(keys)))\n",
    "        elif len(keys) == 0:\n",
    "            raise  IOError(\"The locus %s was not found\"\n",
    "                           %locus_name)\n",
    "        elif len(keys) == 1:\n",
    "            records = []\n",
    "            if filter:\n",
    "                for r in self.records_by_locus[keys[0]]:\n",
    "                    qualifiers = get_qualifiers_dictionary(self, r.id)\n",
    "                    get = True\n",
    "                    for f in filter:\n",
    "                        if not (f[0] in qualifiers.keys() and qualifiers[f[0]] == f[1]):\n",
    "                            get = False\n",
    "                    if get:\n",
    "                        records.append(r) \n",
    "            else:\n",
    "                for r in self.records_by_locus[keys[0]]:\n",
    "                    records.append(r)\n",
    "            print \"returning records list of locus %s and filter %s\"%(keys[0], str(filter))\n",
    "            return records\n",
    "        \n",
    "   \n",
    "    def propagate_metadata(self):\n",
    "        for t in self.trees.keys():\n",
    "            for l in self.trees[t][0]:\n",
    "                feature_id = l.feature_id\n",
    "                record_id = feature_id.rpartition('_')[0]\n",
    "                record = [r for r in self.records if r.id == record_id][0]\n",
    "                annotations = record.annotations\n",
    "                source_qualifiers = [f for f in record.features if f.type == 'source'][0].qualifiers\n",
    "                feature_qualifiers = [f for f in record.features if f.qualifiers['feature_id'][0] == feature_id][0].qualifiers\n",
    "                for i in annotations:\n",
    "                    l.add_feature(\"annotations_%s\"%i,type_to_single_line_str(annotations[i]))\n",
    "                for i in source_qualifiers:\n",
    "                    l.add_feature(\"source_%s\"%i,type_to_single_line_str(source_qualifiers[i]))\n",
    "                for i in feature_qualifiers:\n",
    "                    l.add_feature(i,type_to_single_line_str(feature_qualifiers[i]))\n",
    "            self.trees[t][1] = self.trees[t][0].write(features=[])\n",
    "\n",
    "            \n",
    "##############################################################################################\n",
    "if False:\n",
    "    \"\"\"Tools for loci explorations in a GenBank File\"\"\"\n",
    "##############################################################################################\n",
    "\n",
    "programspath = \"\"\n",
    "\n",
    "def list_loci_in_genbank(genbank_filename, control_filename, loci_report = None):\n",
    "    \"\"\"\n",
    "    Takes a genbank file, returns a loci csv file and a list of loci and their counts. The latter goes\n",
    "    either to stdout or to a file.\n",
    "    \n",
    "    >>> list_loci_in_genbank(\"test-data/test.gb\", \"test-data/temp_loci.csv\", loci_report = \"test-data/temp_loci.txt\")\n",
    "    >>> assert open(\"test-data/temp_loci.csv\",'r').read() == open(\"test-data/test_loci.csv\",'r').read() \n",
    "    >>> import os\n",
    "    >>> os.remove(\"test-data/temp_loci.csv\")\n",
    "    \"\"\"\n",
    "    \n",
    "    stdout = sys.stdout\n",
    "    if  loci_report: \n",
    "        sys.stdout = open(loci_report, 'w')\n",
    "    \n",
    "    genbank_synonyms = gb_syn.gb_syn()\n",
    "    \n",
    "    # Open GenBank file\n",
    "    MelPCgenes = open(genbank_filename, 'rU')\n",
    "   \n",
    "    gene_dict = {} #set up a gene_dict dictionary\n",
    "   \n",
    "    # For each record\n",
    "    \n",
    "    for record in SeqIO.parse(MelPCgenes, 'genbank') :\n",
    "        # Look at all features for this record\n",
    "        for feature in record.features:\n",
    "         \n",
    "            # If it's a CDS or rRNA...\n",
    "            if feature.type == 'CDS' or feature.type == 'rRNA':\n",
    "                # If it contains some attribute called 'gene' save that\n",
    "                if 'gene' in feature.qualifiers:\n",
    "                    geneName = feature.qualifiers['gene'][0]\n",
    "                    geneName = geneName.replace(',','_')\n",
    "                    geneName = geneName.replace('/','_')\n",
    "\n",
    "                    if feature.type+','+geneName in gene_dict:\n",
    "                        gene_dict[feature.type+','+geneName]+=1\n",
    "                    else:    \n",
    "                        gene_dict[feature.type+','+geneName]=1\n",
    "                    #print(geneName)\n",
    "\n",
    "                # Else if it contains a 'product' qualifier\n",
    "                elif 'product' in feature.qualifiers:\n",
    "                    geneName = feature.qualifiers['product'][0]\n",
    "                    geneName = geneName.replace(',','_')\n",
    "                    geneName = geneName.replace('/','_') \n",
    "\n",
    "                    if feature.type+','+geneName in gene_dict:\n",
    "                        gene_dict[feature.type+','+geneName]+=1\n",
    "                    else:    \n",
    "                        gene_dict[feature.type+','+geneName]=1\n",
    "                    #print(geneName)\n",
    "\n",
    "                else:\n",
    "                    print 'Could not find either gene or product in '+record.id\n",
    "                    #print feature.qualifiers\n",
    "\n",
    "       \n",
    "    #sorting happens via a list\n",
    "   \n",
    "    sorted_gene_names = gene_dict.items()\n",
    "    sorted_gene_names.sort(key = lambda i: i[0].lower())\n",
    "    control_file_lines = {}\n",
    "   \n",
    "   \n",
    "    print('\\n' + \"There are \" + str(len(sorted_gene_names)) + \" gene names (or gene product names) detected\")\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Gene and count sorted by gene name\")\n",
    "    print(\"----------------------------------\")\n",
    "   \n",
    "    for key, value in sorted_gene_names:\n",
    "        #print key, value\n",
    "        print(str(value) +\" instances of \" + key)\n",
    "        feature_type = key.split(',')[0]\n",
    "        alias = key.split(',')[1]\n",
    "        gen_group = None\n",
    "        for group in genbank_synonyms:\n",
    "            if alias in group:\n",
    "                gen_group = group\n",
    "        if gen_group:\n",
    "            if gen_group[0].replace(' ','_') in control_file_lines.keys():\n",
    "                control_file_lines[gen_group[0].replace(' ','_')].append(alias)\n",
    "            else:\n",
    "                control_file_lines[gen_group[0].replace(' ','_')] = [feature_type, alias]\n",
    "        else:\n",
    "            name = alias.replace(' ','_').replace('/','_')\n",
    "            control_file_lines[name] = [feature_type, alias]\n",
    "                    \n",
    "    control_file_handle = open(control_filename, 'wt')\n",
    "    for line in sort(control_file_lines.keys()):  \n",
    "        control_file_handle.write('dna,%s,%s'%(control_file_lines[line][0],line))\n",
    "        for a in control_file_lines[line][1:]: \n",
    "            control_file_handle.write(',%s'%a)\n",
    "        control_file_handle.write('\\n')\n",
    "                            \n",
    "    control_file_handle.close()                 \n",
    "   \n",
    "    print(\"-------------------------------\")\n",
    "    print(\"Gene and count sorted by counts\")\n",
    "    print(\"-------------------------------\")\n",
    "    sorted_gene_names.sort(key = lambda i: int(i[1]), reverse=True)\n",
    "    for key, value in sorted_gene_names:\n",
    "        #print key, value\n",
    "        print(str(value) +\" instances of \" + key)\n",
    "    sys.stdout = stdout\n",
    "    return    \n",
    "\n",
    "##############################################################################################\n",
    "class AlnConf:\n",
    "##############################################################################################\n",
    "    \n",
    "    \"\"\"\n",
    "    >>> coi = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
    "    >>> lsu = Locus('dna', 'rRNA', '28S', ['28s','28S','LSU rRNA','28S ribosomal RNA','28S large subunit ribosomal RNA'])\n",
    "    >>> pj = Project([coi, lsu])\n",
    "    \n",
    "    # cline_str = muscle = AlnConf(pj, method_name='MuscleDefaults',\n",
    "    #                                      cmd='muscle', program_name='muscle',\n",
    "    #                                     cline_args=dict())\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pj, method_name='mafftDefault', CDSAlign=True, codontable=1, program_name='mafft',\n",
    "                 cmd='mafft', loci='all',\n",
    "                 cline_args=dict()):\n",
    "        self.id = str(random.randint(10000,99999))+str(time.time())\n",
    "        self.method_name=method_name\n",
    "        self.CDSAlign=CDSAlign\n",
    "        self.program_name=program_name\n",
    "        self.loci = pj.loci\n",
    "        if not loci == 'all':\n",
    "            self.loci = []\n",
    "            for locus_name in loci:\n",
    "                for locus in pj.loci:\n",
    "                    if locus_name == locus.name:\n",
    "                        self.loci.append(locus)\n",
    "        mutable_loci_list = []\n",
    "        removed_loci = []\n",
    "        for locus in self.loci: \n",
    "            if len(pj.records_by_locus[locus.name]) < 4:\n",
    "                removed_loci.append(locus.name)\n",
    "                #print \"%s have less than 4 sequences and will be dropped from this conf object. Don't use it in a concatenation\"%locus.name\n",
    "            else:\n",
    "                mutable_loci_list.append(locus)\n",
    "        if len(removed_loci) > 0:\n",
    "            print \"These loci have less than 4 sequences and will be dropped from this conf object. Don't use them in a concatenation:\\n%s\\n\\n\"%removed_loci\n",
    "        self.loci = mutable_loci_list\n",
    "        self.CDS_proteins = {}\n",
    "        self.CDS_in_frame = {}\n",
    "        self.codontable = codontable \n",
    "        self.aln_input_strings = {}\n",
    "        self.command_lines = {}\n",
    "        self.timeit = [time.asctime()]\n",
    "        self.platform = []\n",
    "        self.cline_args = cline_args\n",
    "        self.cmd = cmd\n",
    "        if not program_name == 'mafft' and cmd == 'mafft':\n",
    "            self.cmd = pj.defaults[program_name]\n",
    "        elif program_name == 'mafft' and cmd != 'mafft' and not 'mafft' in cmd:\n",
    "            self.cmd = pj.defaults['mafft']\n",
    "            \n",
    "        # make defalut input files\n",
    "        if pj.records_by_locus == {}:\n",
    "            pj.extract_by_locus()\n",
    "        for key in pj.records_by_locus.keys():\n",
    "            if key in [l.name for l in self.loci]:\n",
    "                SeqIO.write(pj.records_by_locus[key], self.id+'_'+key+'.fasta', 'fasta')    \n",
    "        for locus in self.loci:\n",
    "            # put default input file filename and string in the AlnConf object\n",
    "            input_filename=self.id+'_'+locus.name+'.fasta'\n",
    "            self.aln_input_strings[locus.name] = [open(input_filename,'r').read()]\n",
    "            # If CDS prepare reference protein input file and in frame CDS input file\n",
    "            if locus.feature_type == 'CDS' and locus.char_type == 'dna' and self.CDSAlign: \n",
    "                self.CDS_proteins[locus.name] = []\n",
    "                self.CDS_in_frame[locus.name] = []\n",
    "                for record in pj.records:\n",
    "                    for feature in record.features:\n",
    "                        if (feature.type == 'CDS' and 'gene' in feature.qualifiers.keys() and\n",
    "                            feature.qualifiers['gene'][0] in locus.aliases and \n",
    "                            feature.qualifiers['feature_id'][0] in [f.id for f in pj.records_by_locus[locus.name]]):\n",
    "                            S = feature.extract(record.seq)\n",
    "                            # Make in-frame CDS input file seq start in frame\n",
    "                            if 'codon_start' in feature.qualifiers.keys():\n",
    "                                i = feature.qualifiers['codon_start'][0]\n",
    "                                if i > 1:\n",
    "                                    S = S[(int(i)-1):]\n",
    "                            # Make in-frame CDS input file seq end in frame\n",
    "                            if len(S)%3 == 1:\n",
    "                                S = S[:-1]\n",
    "                            elif len(S)%3 == 2:\n",
    "                                S = S[:-2]  \n",
    "                            # make protein input file seq\n",
    "                            if not 'translation' in feature.qualifiers.keys():\n",
    "                                raise IOError(\"Feature %s has no 'translation' qualifier\"%\n",
    "                                              feature.qualifiers['feature_id'][0])\n",
    "                            P = Seq(feature.qualifiers['translation'][0], IUPAC.protein)\n",
    "                            # Remove 3' positions that are based on partial codons\n",
    "                            while len(P)*3 > len(S):\n",
    "                                P = P[:-1]\n",
    "                            # remove complete termination codon\n",
    "                            if (len(S)/3)-1 == len(P):\n",
    "                                S = S[:-3]\n",
    "                            # make in frame cds record\n",
    "                            feature_record = SeqRecord(seq = S, id = feature.qualifiers['feature_id'][0],\n",
    "                                                               description = '')\n",
    "                            # put it in the object\n",
    "                            self.CDS_in_frame[locus.name].append(feature_record)\n",
    "                            # make protein record\n",
    "                            feature_record = SeqRecord(seq = P, id = feature.qualifiers['feature_id'][0],\n",
    "                                                       description = '')\n",
    "                            # Put the protein records in the AlnConf object\n",
    "                            self.CDS_proteins[locus.name].append(feature_record)\n",
    "                           \n",
    "                                    \n",
    "                # check same number of prot and cds objects                    \n",
    "                if len(pj.records_by_locus[locus.name]) > len(self.CDS_proteins[locus.name]):\n",
    "                    raise RuntimeError('For the CDS locus '+locus.name+': more nuc seqs than prot seqs.'+\n",
    "                                       ' You may miss a \\'translate\\' or \\'gene\\' qualifier in some of '+\n",
    "                                       'the features.')\n",
    "                elif len(pj.records_by_locus[locus.name]) < len(self.CDS_proteins[locus.name]):\n",
    "                    raise RuntimeError('For the CDS locus '+locus.name+': less nuc seqs than prot seqs.'+\n",
    "                                       ' You may miss a \\'translate\\' or \\'gene\\' qualifier in some of '+\n",
    "                                       'the features.')\n",
    "                unmatched = []\n",
    "                for record in self.CDS_in_frame[locus.name]:\n",
    "                    for prot in self.CDS_proteins[locus.name]:\n",
    "                        if prot.id == record.id:\n",
    "                            if not len(prot.seq)*3 == len(record.seq):\n",
    "                                unmatched.append(record.id)\n",
    "                unmatched_string = ''\n",
    "                if len(unmatched) > 0:\n",
    "                    for u in unmatched:\n",
    "                        unmatched_string += u+' '\n",
    "                    raise RuntimeError('The following CDS/protein pairs are unmatched: '+unmatched_string)\n",
    "                    \n",
    "                SeqIO.write(self.CDS_in_frame[locus.name],\n",
    "                            self.id+'_CDS_in_frame_'+locus.name+'.fasta', 'fasta')\n",
    "                input_filename2=self.id+'_CDS_in_frame_'+locus.name+'.fasta'\n",
    "                SeqIO.write(self.CDS_proteins[locus.name],\n",
    "                            self.id+'_CDS_proteins_'+locus.name+'.fasta', 'fasta')\n",
    "                input_filename=self.id+'_CDS_proteins_'+locus.name+'.fasta'\n",
    "                self.aln_input_strings[locus.name][0] = [open(input_filename,'r').read(),\n",
    "                                                         open(input_filename2,'r').read()]\n",
    "            cline = dict(dict(input=input_filename), **cline_args)\n",
    "            if self.program_name == 'mafft':\n",
    "                self.command_lines[locus.name] = MafftCommandline(cmd=self.cmd)\n",
    "            elif self.program_name == 'muscle':\n",
    "                self.command_lines[locus.name] = MuscleCommandline(cmd=self.cmd)\n",
    "            for c in cline.keys():\n",
    "                self.command_lines[locus.name].__setattr__(c,cline[c])\n",
    "            print str(self.command_lines[locus.name])\n",
    "            \n",
    "            \n",
    "            \n",
    "    def __str__(self):\n",
    "        loci_string = ''\n",
    "        for n in [i.name for i in self.loci]:\n",
    "            loci_string += n+','\n",
    "        loci_string = loci_string[:-1]\n",
    "        command_lines = ''\n",
    "        for i in self.command_lines.keys():\n",
    "            command_lines += i+': '+str(self.command_lines[i])+'\\n'\n",
    "        date = str(self.timeit[0])\n",
    "        execution = str(self.timeit[3])\n",
    "        plat = str(self.platform).replace(\",\",'\\n').replace(']','').replace(\"'\",'').replace('[','')\n",
    "        return str(\"AlnConf named %s with ID %s\\n\"+         \n",
    "                \"Loci: %s \\n\"+       \n",
    "                \"Executed on: %s\\n\"+\n",
    "                \"Commands:\\n\"+\n",
    "                \"%s\\n\"+\n",
    "                \"Environment:\\n\"+    \n",
    "                \"%s\\n\"+\n",
    "                \"execution time:\\n\"+\n",
    "                \"%s\")%(self.method_name, str(self.id), loci_string, date, command_lines, plat, execution) \n",
    "\n",
    "##############################################################################################\n",
    "class TrimalConf:\n",
    "##############################################################################################\n",
    "    def __init__(self, pj, method_name='gappyout', program_name='trimal',\n",
    "                 cmd='default', alns='all', trimal_commands=dict(gappyout=True)):\n",
    "        \n",
    "        if len(pj.alignments) == 0:\n",
    "            raise RuntimeError(\"No sequence alignments found\")\n",
    "        self.id = str(random.randint(10000,99999))+str(time.time())\n",
    "        self.method_name=method_name\n",
    "        self.program_name=program_name\n",
    "        self.alignments = pj.alignments\n",
    "        if not alns == 'all':\n",
    "            self.alignments = {}\n",
    "            for aln_name in alns:\n",
    "                if aln_name in pj.alignments.keys():\n",
    "                    self.alignments[aln_name] = pj.alignments[aln_name]\n",
    "        self.command_lines = {}\n",
    "        self.timeit = [time.asctime()]\n",
    "        self.platform = []\n",
    "        self.cline_args = trimal_commands\n",
    "        self.cmd = cmd\n",
    "        if cmd == 'default':\n",
    "            self.cmd = pj.defaults['trimal']\n",
    "        irelevant = ['out', 'clustal', 'fasta', 'nbrf', 'nexus', 'mega',\n",
    "                     'phylip3.2', 'phylip', 'sgt', 'scc', 'sct', 'sfc',\n",
    "                     'sft','sident']\n",
    "        for aln in self.alignments:\n",
    "            input_filename = self.id+'_'+aln+'.fasta'\n",
    "            AlignIO.write(self.alignments[aln], input_filename,'fasta')\n",
    "            self.command_lines[aln] = \"%s -in %s \"%(self.cmd, input_filename)\n",
    "            for kwd in trimal_commands:\n",
    "                if kwd in irelevant:\n",
    "                    warnings.simplefilter('always')\n",
    "                    warnings.warn(\"%s is irelevant in this context and will be ignored\"%kwd)\n",
    "                else:\n",
    "                    if not trimal_commands[kwd] == None and not trimal_commands[kwd] == False:\n",
    "                        if trimal_commands[kwd] == True:\n",
    "                            self.command_lines[aln] += \"-%s \"%(kwd)\n",
    "                        else:\n",
    "                            self.command_lines[aln] += \"-%s %s \"%(kwd, trimal_commands[kwd])\n",
    "            \n",
    "            self.command_lines[aln+'@'+self.method_name] = self.command_lines[aln][:-1]\n",
    "            print self.command_lines[aln+'@'+self.method_name]\n",
    "            self.command_lines.pop(aln, None)\n",
    "        \n",
    "    def __str__(self):\n",
    "        aln_string = ''\n",
    "        for n in self.alignments.keys():\n",
    "            aln_string += n+','\n",
    "        aln_string = aln_string[:-1]\n",
    "        command_lines = ''\n",
    "        for i in self.command_lines.keys():\n",
    "            command_lines += i+': '+str(self.command_lines[i])+'\\n'\n",
    "        date = str(self.timeit[0])\n",
    "        execution = str(self.timeit[3])\n",
    "        plat = str(self.platform).replace(\",\",'\\n').replace(']','').replace(\"'\",'').replace('[','')\n",
    "        return str(\"TrimalConf named %s with ID %s\\n\"+         \n",
    "                \"Alignments: %s \\n\"+       \n",
    "                \"Executed on: %s\\n\"+\n",
    "                \"Commands:\\n\"+\n",
    "                \"%s\\n\"+\n",
    "                \"Environment:\"+    \n",
    "                \"%s\\n\"+\n",
    "                \"execution time:\\n\"+\n",
    "                \"%s\")%(self.method_name, str(self.id), aln_string, date, command_lines, plat, execution)  \n",
    "\n",
    "\n",
    "def use_sh_support_as_branch_support(tree_filename):\n",
    "    string = open(tree_filename,'r').read()\n",
    "    string = re.sub(r'\\[',r'[&&NHX:support=',string)\n",
    "    t = Tree(string)\n",
    "    t.dist=0\n",
    "    return t.write(features=[])\n",
    "    \n",
    "def transfer_support_same_topo(tree_file_with_support,\n",
    "                               tree_file_without_support):\n",
    "    supported = Tree(tree_file_with_support) \n",
    "    unsupported = Tree(tree_file_without_support)\n",
    "    supported_leaf_names = sorted(supported.get_leaf_names())\n",
    "    unsupported_leaf_names = sorted(unsupported.get_leaf_names())\n",
    "    if not len(unsupported_leaf_names) == len(supported_leaf_names):\n",
    "        raise IOError(tree_file_with_support + ' and ' + tree_file_without_support +\n",
    "                      ' are not the same length')\n",
    "    for i in range(len(supported_leaf_names)):\n",
    "        if not supported_leaf_names[i] == unsupported_leaf_names[i]:\n",
    "            raise IOError('The trees do not share all leaves or leaf names')\n",
    "    same_root = supported.get_leaf_names()[0]\n",
    "    unsupported.set_outgroup(same_root)\n",
    "    supported.set_outgroup(same_root)\n",
    "    for ns in supported.traverse():\n",
    "        ns_leaves = ns.get_leaf_names()\n",
    "        if not unsupported.check_monophyly(values=ns_leaves, target_attr=\"name\"):\n",
    "            raise RuntimeError('trees do not share topology and/or all the leaf names')\n",
    "        else:\n",
    "            unsupported_ancestor = unsupported.get_common_ancestor(ns_leaves)\n",
    "            unsupported_ancestor.support = ns.support\n",
    "    unsupported.write(outfile = tree_file_without_support)    \n",
    "    \n",
    "    \n",
    "def make_raxml_partfile(tree_method, pj, trimmed_alignment_name):\n",
    "\n",
    "    concatenation = None\n",
    "    for c in pj.concatenations:\n",
    "        if c.name == trimmed_alignment_name:\n",
    "            concatenation = c\n",
    "    \n",
    "    #concatenation = filter(lambda concatenation: concatenation.name == trimmed_alignment_name, pj.concatenations)[0]\n",
    "    \n",
    "    model = []\n",
    "    for locus in concatenation.loci:\n",
    "        part_name = None\n",
    "        for trm_aln in concatenation.used_trimmed_alns.keys():\n",
    "            if locus.name == trm_aln.partition('@')[0]:\n",
    "                part_name = trm_aln\n",
    "        if not part_name:\n",
    "            warnings.warn('There is no trimmed alignment for locus '+locus.name+' in concatenation '+concatenation.name)\n",
    "        else:\n",
    "            part_length = concatenation.used_trimmed_alns[part_name]\n",
    "            if locus.char_type == 'prot':\n",
    "                m = None\n",
    "                if isinstance(tree_method.matrix,dict):\n",
    "                    m = tree_method.matrix[locus.name]\n",
    "                elif isinstance(tree_method.matrix,str):\n",
    "                    m = tree_method.matrix\n",
    "                else:\n",
    "                    #todo write error\n",
    "                    pass\n",
    "                model.append([m,part_name,part_length])\n",
    "            elif locus.char_type == 'dna':\n",
    "                model.append(['DNA',part_name,part_length])\n",
    "                    \n",
    "    # make partition file\n",
    "                    \n",
    "    partfile = open(tree_method.id+'_'+concatenation.name+'_partfile','wt')\n",
    "    i = 1\n",
    "    for m in model:\n",
    "        partfile.write(m[0]+', '+m[1]+'='+str(i)+'-'+str(m[2]+i-1)+'\\n')\n",
    "        i += m[2]\n",
    "    partfile.close()\n",
    "    return tree_method.id+'_'+concatenation.name+'_partfile'\n",
    "\n",
    "def make_raxml_input_matrix_file(tree_method, trimmed_alignment_name):\n",
    "    SeqIO.write(tree_method.trimmed_alignments[trimmed_alignment_name],\n",
    "                tree_method.id+'_'+trimmed_alignment_name+'.fasta','fasta')\n",
    "    return tree_method.id+'_'+trimmed_alignment_name+'.fasta'\n",
    "\n",
    "def write_raxml_clines(tree_method, pj, trimmed_alignment_name):\n",
    "            \n",
    "    cline_que = 0\n",
    "\n",
    "    support_replicates = 100\n",
    "    ML_replicates = 1\n",
    "    if '-N' in tree_method.cline_args.keys():\n",
    "        ML_replicates = tree_method.cline_args['-N']\n",
    "    if '-#' in tree_method.cline_args.keys():\n",
    "        support_replicates = tree_method.cline_args['-#']\n",
    "    \n",
    "    partfile = None\n",
    "    \n",
    "    # Check if it is a concatenation and make partfile\n",
    "\n",
    "    for c in pj.concatenations:\n",
    "        if c.name == trimmed_alignment_name.partition('@')[0]:\n",
    "            partfile = make_raxml_partfile(tree_method, pj, trimmed_alignment_name)\n",
    "    \n",
    "    input_filename = make_raxml_input_matrix_file(tree_method, trimmed_alignment_name)\n",
    "    model = tree_method.model\n",
    "    try:\n",
    "        locus_char_type = filter(lambda locus: locus.name == trimmed_alignment_name.partition('@')[0], pj.loci)[0].char_type\n",
    "    except:\n",
    "        locus_char_type = 'prot'\n",
    "    \n",
    "    if partfile:\n",
    "        model='PROT'+model+'JTT'\n",
    "    else:\n",
    "        if locus_char_type == 'dna':\n",
    "            model = 'GTR'+tree_method.model\n",
    "        elif  locus_char_type == 'prot':\n",
    "            if isinstance(tree_method.matrix,str):\n",
    "                model = 'PROT'+tree_method.model+tree_method.matrix\n",
    "            elif isinstance(tree_method.matrix,dict):\n",
    "                model = 'PROT'+tree_method.model+tree_method.matrix[trimmed_alignment_name]\n",
    "        \n",
    "    presets = {'fa': [{'-f': 'a',\n",
    "                           '-p': random.randint(99,999),\n",
    "                           '-x':  random.randint(99,999),\n",
    "                           '-s': input_filename,\n",
    "                           '-N': support_replicates,\n",
    "                           '-n': tree_method.id+'_'+trimmed_alignment_name+'0',\n",
    "                           '-m': model}\n",
    "                      ],\n",
    "                'fD_fb':[{'-f': 'D',\n",
    "                          '-p': random.randint(99,999),\n",
    "                          '-s': input_filename,\n",
    "                          '-N': ML_replicates,\n",
    "                           '-n': tree_method.id+'_'+trimmed_alignment_name+'0',\n",
    "                           '-m': model},{'-f': 'b',\n",
    "                                                       '-p': random.randint(99,999),\n",
    "                                                       '-s': input_filename,\n",
    "                                                       '-n': tree_method.id+'_'+trimmed_alignment_name+'1',\n",
    "                                                       '-m': model,\n",
    "                                                       '-t': 'RAxML_bestTree.'+tree_method.id+'_'+trimmed_alignment_name+'0',\n",
    "                                                       '-z': 'RAxML_rellBootstrap.'+tree_method.id+'_'+trimmed_alignment_name+'0'}\n",
    "                         ],\n",
    "                'fd_b_fb':[{'-f': 'd',\n",
    "                          '-p': random.randint(99,999),\n",
    "                          '-s': input_filename,\n",
    "                          '-N': ML_replicates,\n",
    "                           '-n': tree_method.id+'_'+trimmed_alignment_name+'0',\n",
    "                           '-m': model},{\n",
    "                                                       '-p': random.randint(99,999),\n",
    "                                                       '-b': random.randint(99,999),\n",
    "                                                       '-s': input_filename,\n",
    "                                                       '-#': support_replicates,\n",
    "                                                       '-n': tree_method.id+'_'+trimmed_alignment_name+'1',\n",
    "                                                       '-m': model,\n",
    "                                                       '-T': tree_method.threads},\n",
    "                                                       {'-f': 'b',\n",
    "                                                       '-p': random.randint(99,999),\n",
    "                                                       '-s': input_filename,\n",
    "                                                       '-n': tree_method.id+'_'+trimmed_alignment_name+'2',\n",
    "                                                       '-m': model,\n",
    "                                                       '-t': 'RAxML_bestTree.'+tree_method.id+'_'+trimmed_alignment_name+'0',\n",
    "                                                       '-z': 'RAxML_bootstrap.'+tree_method.id+'_'+trimmed_alignment_name+'1'}\n",
    "                         ],\n",
    "                'fF_fJ': [{'-f': 'F',\n",
    "                           '-p': random.randint(99,999),\n",
    "                           '-s': input_filename,\n",
    "                           '-n': tree_method.id+'_'+trimmed_alignment_name+'0',\n",
    "                           '-m': model},{'-f': 'J',\n",
    "                                         '-t': 'RAxML_fastTree.'+tree_method.id+'_'+trimmed_alignment_name+'0',\n",
    "                                         '-p': random.randint(99,999),\n",
    "                                         '-s': input_filename,\n",
    "                                         '-n': tree_method.id+'_'+trimmed_alignment_name+'1',\n",
    "                                         '-m': model}],\n",
    "                \n",
    "                'fd_fJ': [{'-f': 'd',\n",
    "                          '-p': random.randint(99,999),\n",
    "                          '-s': input_filename,\n",
    "                          '-N': ML_replicates,\n",
    "                          '-n': tree_method.id+'_'+trimmed_alignment_name+'0',\n",
    "                          '-m': model},{'-f': 'J',\n",
    "                                         '-t': 'RAxML_bestTree.'+tree_method.id+'_'+trimmed_alignment_name+'0',\n",
    "                                         '-p': random.randint(99,999),\n",
    "                                         '-s': input_filename,\n",
    "                                         '-n': tree_method.id+'_'+trimmed_alignment_name+'1',\n",
    "                                         '-m': model}]\n",
    "                \n",
    "                }\n",
    "\n",
    "    \n",
    "    if 'PTHREADS' in tree_method.cmd:\n",
    "        if tree_method.threads < 2:\n",
    "            tree_method.threads = 2\n",
    "            warnings.warn('raxmlHPC-PTHREADS requires at least 2 threads. Setting threads to 2')\n",
    "        for preset in presets:\n",
    "            for c in presets[preset]:\n",
    "                c['-T'] = tree_method.threads\n",
    "    else:\n",
    "        if tree_method.threads > 1:\n",
    "            raise RuntimeWarning('This is a serial raxmlHPC. Setting threads to 1.'+ \n",
    "                                 'PTHREADS executables have to have explicit filename, eg, raxmlHPC-PTHREADS-SSE3')\n",
    "    if partfile:\n",
    "        for preset in presets.keys():\n",
    "            for cline in range(len(presets[preset])):\n",
    "                presets[preset][cline] = dict({'-q': partfile}, **presets[preset][cline])               \n",
    "    return presets[tree_method.preset] \n",
    "\n",
    "##############################################################################################\n",
    "class RaxmlConf:\n",
    "##############################################################################################\n",
    "    \n",
    "    def __init__(self, pj, method_name='fa', program_name='raxmlHPC-PTHREADS-SSE3', keepfiles=False,\n",
    "                 cmd='default', preset = 'fa', alns='all', model='GAMMA', matrix='JTT', threads=4,\n",
    "                 cline_args={}):\n",
    "        \n",
    "        \n",
    "        if len(pj.trimmed_alignments) == 0:\n",
    "            raise RuntimeError(\"No trimmed sequence alignments found\")\n",
    "            \n",
    "        self.id = str(random.randint(10000,99999))+str(time.time())\n",
    "        self.method_name=method_name\n",
    "        self.program_name=program_name\n",
    "        self.preset = preset\n",
    "        self.cline_args = cline_args\n",
    "        self.model = model\n",
    "        self.matrix = matrix\n",
    "        self.threads = threads\n",
    "        self.trimmed_alignments = pj.trimmed_alignments\n",
    "        if not alns == 'all':\n",
    "            self.trimmed_alignments = {}\n",
    "            for aln_name in alns:\n",
    "                if aln_name in pj.trimmed_alignments.keys():\n",
    "                    self.trimmed_alignments[aln_name] = pj.trimmed_alignments[aln_name]\n",
    "        self.aln_input_strings = {}\n",
    "        self.command_lines = {}\n",
    "        self.timeit = [time.asctime()]\n",
    "        self.platform = []\n",
    "        self.cmd = cmd\n",
    "        self.keepfiles = keepfiles\n",
    "        if cmd == 'default':\n",
    "            self.cmd = pj.defaults['raxmlHPC']\n",
    "        \n",
    "        for trimmed_alignment in self.trimmed_alignments.keys():\n",
    "            self.command_lines[trimmed_alignment] = []\n",
    "            command_lines = write_raxml_clines(self, pj, trimmed_alignment)\n",
    "            for command_line in command_lines:\n",
    "                cline_object = RaxmlCommandline(cmd=self.cmd)\n",
    "                for c in command_line.keys():\n",
    "                    cline_object.__setattr__(c,command_line[c])\n",
    "                self.command_lines[trimmed_alignment].append(cline_object)\n",
    "                print str(cline_object)\n",
    "\n",
    "    def __str__(self):\n",
    "        aln_string = ''\n",
    "        for n in self.trimmed_alignments.keys():\n",
    "            aln_string += n+','\n",
    "        aln_string = aln_string[:-1]\n",
    "        command_lines = ''\n",
    "        for i in self.command_lines.keys():\n",
    "            command_lines += i+': '+str(self.command_lines[i])+'\\n'\n",
    "        date = str(self.timeit[0])\n",
    "        execution = str(self.timeit[3])\n",
    "        plat = str(self.platform).replace(\",\",'\\n').replace(']','').replace(\"'\",'').replace('[','')\n",
    "        return str(\"RaxmlConf named %s with ID %s\\n\"+         \n",
    "                \"Alignments: %s \\n\"+       \n",
    "                \"Executed on: %s\\n\"+\n",
    "                \"Commands:\\n\"+\n",
    "                \"%s\\n\"+\n",
    "                \"Environment:\\n\"+    \n",
    "                \"%s\\n\"+\n",
    "                \"execution time:\\n\"+\n",
    "                \"%s\")%(self.method_name, str(self.id), aln_string, date, command_lines, plat, execution)  \n",
    "\n",
    "def make_pb_input_matrix_file(conf_obj, trimmed_alignment_name):\n",
    "    SeqIO.write(conf_obj.trimmed_alignments[trimmed_alignment_name],\n",
    "                conf_obj.id+'_'+trimmed_alignment_name+'.phylip','phylip-relaxed')\n",
    "    return conf_obj.id+'_'+trimmed_alignment_name+'.phylip'\n",
    "\n",
    "\n",
    "def write_pb_cline(conf_obj, pj, trimmed_alignment):\n",
    "    cline = \"%s -d %s\"%(conf_obj.cmd, make_pb_input_matrix_file(conf_obj, trimmed_alignment))\n",
    "    for key in conf_obj.cline_args:\n",
    "        kw = key\n",
    "        if key[0] == '-':\n",
    "            kw = key[1:]\n",
    "        cline += \" -%s\"%str(kw)\n",
    "        if not str(conf_obj.cline_args[key]) == str(True):\n",
    "            cline += \" %s\"%str(conf_obj.cline_args[key]) \n",
    "    cline += \" %s_%s\"%(conf_obj.id, trimmed_alignment)\n",
    "    return cline\n",
    "            \n",
    "###################################################################################################\n",
    "class PbConf:\n",
    "###################################################################################################\n",
    "    \n",
    "    def __init__(self, pj, method_name = 'dna_cat_gtr', program_name='phylobayes', keepfiles=True,\n",
    "                 cmd='default', alns='all', cline_args=dict(nchain=\"2 100 0.1 100\",\n",
    "                                                            gtr=True,\n",
    "                                                            cat=True)):\n",
    "        self.id = str(random.randint(10000,99999))+str(time.time())\n",
    "        self.method_name=method_name\n",
    "        self.program_name=program_name\n",
    "        self.cline_args = cline_args\n",
    "        self.trimmed_alignments = pj.trimmed_alignments\n",
    "        if not alns == 'all':\n",
    "            self.trimmed_alignments = {}\n",
    "            for aln_name in alns:\n",
    "                if aln_name in pj.trimmed_alignments.keys():\n",
    "                    self.trimmed_alignments[aln_name] = pj.trimmed_alignments[aln_name]\n",
    "        self.aln_input_strings = {}\n",
    "        self.command_lines = {}\n",
    "        self.timeit = [time.asctime()]\n",
    "        self.platform = []\n",
    "        self.cmd = cmd\n",
    "        self.keepfiles = keepfiles\n",
    "        if cmd == 'default':\n",
    "            self.cmd = pj.defaults['pb']\n",
    "        \n",
    "        for trimmed_alignment in self.trimmed_alignments.keys():\n",
    "            self.command_lines[trimmed_alignment] = [write_pb_cline(self, pj, trimmed_alignment)]\n",
    "            print self.command_lines[trimmed_alignment][0]\n",
    "            \n",
    "#class FastTreeConf:\n",
    "    \n",
    "\n",
    "            \n",
    "from pylab import *\n",
    "import random\n",
    "\n",
    "def draw_boxplot(dictionary, y_axis_label, figs_folder): #'locus':[values]\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    items = dictionary.items()\n",
    "    items.sort()\n",
    "    data = [locus[1] for locus in items]\n",
    "        \n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(0.3*len(data),10)\n",
    "    plt.subplots_adjust(top=0.99, bottom=0.3)\n",
    "\n",
    "    #bp = plt.boxplot(data, widths=0.75, patch_artist=True)\n",
    "    bp = plt.boxplot(data, patch_artist=True)\n",
    "    \n",
    "    for box in bp['boxes']:\n",
    "    # change outline color\n",
    "        box.set( color='black', linewidth=1)\n",
    "        \n",
    "    # change fill color\n",
    "        box.set( facecolor = 'red', alpha=0.85 )\n",
    "        \n",
    "    # change color, linestyle and linewidth of the whiskers\n",
    "    for whisker in bp['whiskers']:\n",
    "        whisker.set(color='gray', linestyle='solid', linewidth=2.0)\n",
    "\n",
    "    # change color and linewidth of the caps\n",
    "    for cap in bp['caps']:\n",
    "        cap.set(color='gray', linewidth=2.0)\n",
    "\n",
    "    # change color and linewidth of the medians\n",
    "    for median in bp['medians']:\n",
    "        #median.set(color='#b2df8a', linewidth=2)\n",
    "        median.set(color='white', linewidth=2)\n",
    "\n",
    "    # change the style of fliers and their fill\n",
    "    for flier in bp['fliers']:\n",
    "        flier.set(marker='o', color='#e7298a', alpha=0.5)\n",
    "    \n",
    "    # Add a light horizontal grid to the plot\n",
    "    ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',\n",
    "              alpha=0.7)\n",
    "    \n",
    "    # Hide these grid behind plot objects\n",
    "    ax1.set_axisbelow(True)\n",
    "    \n",
    "    #set title and axis labels\n",
    "    #ax1.set_title('Sequence length distribution per locus\\n', size=18)\n",
    "    \n",
    "    xlabels = [locus[0] for locus in items]\n",
    "    \n",
    "    xticks(range(1,len(data)+1), xlabels, size=14, rotation='vertical')\n",
    "    #subplots_adjust(left=0.3, bottom=0.8)\n",
    "    \n",
    "    ax1.set_ylabel(y_axis_label, size=18)\n",
    "    \n",
    "    name = str(random.randint(1000,2000))\n",
    "    if figs_folder=='inline':\n",
    "        fig.show()\n",
    "    else:\n",
    "        fig.savefig(figs_folder + '/' + name +'.png')\n",
    "        close('all')\n",
    "    return figs_folder + '/' + name+'.png'\n",
    "    \n",
    "#################################################################################\n",
    "def report_methods(pj, figs_folder, output_directory, size='small'):\n",
    "#################################################################################\n",
    "        \"\"\"\n",
    "        Main HTML reporting function. This function iterates over the \n",
    "        Project's attributes and generate appropriate html formated report lines.\n",
    "        \n",
    "        pj -               The Project object\n",
    "        \n",
    "        figs_folder -      The directory to which tree figures were saved. This is\n",
    "                           specified in the annotate Project method\n",
    "                           \n",
    "        output_directory - The directory to which this report will be written.It \n",
    "                           can be inherited from the publish function which uses \n",
    "                           this function.\n",
    "        \"\"\"\n",
    "        \n",
    "        #========================================================================\n",
    "        #                                   HEAD\n",
    "        #========================================================================\n",
    "        \n",
    "        # Checking if 'report_methods' was called by 'publish' in order to infer\n",
    "        # which folders to create and what errors to raise for existing \n",
    "        # directories. If we were called by 'publish' we should allow \n",
    "        # output_directory to exist because 'publish' has just created it.\n",
    "        # 'publish' would have also raised an error if it existed before.\n",
    "        curframe = inspect.currentframe()\n",
    "        calframe = inspect.getouterframes(curframe, 2)\n",
    "        callername = calframe[1][3]\n",
    "        print \"reporter was called by \"+str(callername)\n",
    "        # Here we manage mkdir and error raising for preexisting \n",
    "        # output_directory, based of the caller function\n",
    "        if os.path.isdir(output_directory) and str(callername) != 'publish':\n",
    "            raise RuntimeError('%s already exists'%output_directory) \n",
    "        else:\n",
    "            # This will make output_directory if it doesnt exist and\n",
    "            # will add a subdirectory 'files' which will store the\n",
    "            # stylesheet file and the figure files\n",
    "            os.makedirs('%s/files'%output_directory)\n",
    "        \n",
    "        # This will contain the text that points to the stylesheet file\n",
    "        # I have named it rp.css. We need to add something that writes\n",
    "        # it here. Or that get it from the source directory and places it\n",
    "        # in /files\n",
    "        \n",
    "        css_line = '<link rel=\"stylesheet\" type=\"text/css\" href=\"files/rp.css\">'\n",
    "\n",
    "        # This list will contain the report lines/ tables as values. We will append\n",
    "        # each new report line to it\n",
    "        report_lines = ['<html>','<head>',css_line,'<h1>']\n",
    "    \n",
    "        # Main report title, will print the time in which the 'Project' object\n",
    "        # was initiated (not the time when the report was made as in previous \n",
    "        # versions.\n",
    "        head = 'reprophylo analysis from '+pj.starttime\n",
    "        \n",
    "\n",
    "        report_lines.append(head)\n",
    "        report_lines += ['</h1>','</head>','<body>','']\n",
    "                \n",
    "        #========================================================================\n",
    "        #                                   BODY\n",
    "        #========================================================================\n",
    "\n",
    "        \n",
    "        #############################  section 1:   DATA  #######################\n",
    "        \n",
    "        if pj.user:\n",
    "            report_lines += ['<h2>','User Info','</h2>', '']\n",
    "            for item in pj.user:\n",
    "                report_lines += ['<strong>%s: </strong>'%item[0], str(item[1])]\n",
    "            report_lines += ['']\n",
    "        \n",
    "        report_lines += ['<h2>','Data','</h2>', '']\n",
    "        \n",
    "        print \"now printing species table\"\n",
    "        # Species over loci table\n",
    "        #------------------------------------------------------------------------\n",
    "        title = 'species representation in sequence data'.title()\n",
    "        report_lines += ('<h3>', title, '</h3>', '')\n",
    "        \n",
    "        \n",
    "        # This will write a CSV file of the times each locus occures for each \n",
    "        # species. The species are sorted in alphabetical order. The CSV will \n",
    "        # be writen to 'outfile_name'. After 'outfile_name' is processed it will\n",
    "        # be deleted.The CSV will be read with the csv module and the resulting\n",
    "        # list of lists will be made into a table using HTML and than added to \n",
    "        # 'report_lines'. The species an gene counts are made by 'species_vs_loci'\n",
    "        # based on the sequences record objects (biopython) found in pj.records.\n",
    "        # pj.records is a list of SeqRecord objects\n",
    "        outfile_name= str(random.randint(1000,2000))\n",
    "        pj.species_vs_loci(outfile_name)\n",
    "        with open(outfile_name, 'rb') as csvfile:\n",
    "            sp_vs_lc = list(csv.reader(csvfile, delimiter='\\t', quotechar='|'))\n",
    "            report_lines += ['',\n",
    "                             HTML.table(sp_vs_lc[1:], header_row=sp_vs_lc[0]),\n",
    "                             '']\n",
    "            # The following writes a pre text of the same thing\n",
    "            #field_sizes = []\n",
    "            #for i in range(len(sp_vs_lc[0])):\n",
    "            #    lengths = []\n",
    "            #    for row in sp_vs_lc:\n",
    "            #        lengths.append(len(row[i]))\n",
    "            #    field_sizes.append(max(lengths))\n",
    "            #for row in sp_vs_lc:\n",
    "            #    string = ''\n",
    "            #    for i in range(len(row)):\n",
    "            #        string += row[i].ljust(field_sizes[i]+3)\n",
    "            #    report_lines.append(string)\n",
    "        \n",
    "        os.remove(outfile_name)\n",
    "        \n",
    "        print \"now making sequence statistics plots\"        \n",
    "        # Sequence statistic plots\n",
    "        #------------------------------------------------------------------------\n",
    "        title = 'Sequence statistic plots'.title()\n",
    "        report_lines += ('<h3>', title, '</h3>', '')\n",
    "        \n",
    "        # This will plot 4 box plot figures representing the distribution of seq\n",
    "        # length, GC content, %ambiguity in nuc and prot seqs for each locus.\n",
    "        if len(pj.records_by_locus.keys())>0:\n",
    "            \n",
    "            # This will determine the with of the figure, 0.5' per locus\n",
    "            scale = str(len(pj.records_by_locus.keys())*0.5)\n",
    "            \n",
    "            # This will make a list of seq length for each locus. Seq length are calced\n",
    "            # using the record.seq in 'pj.records_by_locus'. 'pj.records_by_locus is a\n",
    "            # dict with loci names as keys, and lists of SeqReocrd objects as values\n",
    "            lengths_dict = {}\n",
    "            for locus_name in pj.records_by_locus.keys():\n",
    "                lengths_dict[locus_name] = []\n",
    "                for record in pj.records_by_locus[locus_name]:\n",
    "                    lengths_dict[locus_name].append(len(record.seq))\n",
    "            \n",
    "            # This draws a box plot of sequence length distributions and puts a png in \n",
    "            # the 'files' directory.\n",
    "            fig_filename = draw_boxplot(lengths_dict, 'Seq length (bp)', '%s/files'%output_directory)\n",
    "            \n",
    "            \n",
    "            # Distribution of sequence lengths\n",
    "            #---------------------------------------------------------------------\n",
    "            title = 'Distribution of sequence lengths'\n",
    "            report_lines += ( '<h4>', title, '</h4>',  '')\n",
    "            \n",
    "            # This will write the img tag for the seq length boxplot in the report html\n",
    "            # The src attribute is the png file path. The commented lines are an alternative\n",
    "            # making an embeded figure.\n",
    "            if os.path.isfile(fig_filename):\n",
    "                #data_uri = open(fig_filename, 'rb').read().encode('base64').replace('\\n', '')\n",
    "                #img_tag = '<img height=400 width='+scale+' src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
    "                img_tag = '<img src=\"%s\">'%(fig_filename.partition('/')[-1])\n",
    "                report_lines.append(img_tag)\n",
    "                #os.remove(fig_filename)\n",
    "            \n",
    "            \n",
    "            # This will make GC content, nuc_degen_prop and prot_degen_prop png file,\n",
    "            # will put them in the /files subdirectory and will write the html sections\n",
    "            # for them, including img tags. All three params are feature qualifiers of\n",
    "            # SeqRecord objects found in pj.records, which is a list. \n",
    "            for stat in ('GC_content', 'nuc_degen_prop', 'prot_degen_prop'):\n",
    "                # This will make a dict with loci as keys and a list of stat values as\n",
    "                # dict values.\n",
    "                stat_dict = {}\n",
    "                ylabel = 'GC ontent (%)'\n",
    "                if not stat == 'GC_content':\n",
    "                    ylabel = 'Ambiguous positions (prop)'\n",
    "                for locus_name in pj.records_by_locus.keys():\n",
    "                    stat_dict[locus_name] = []\n",
    "                    for i in pj.records_by_locus[locus_name]:\n",
    "                        for record in pj.records:\n",
    "                            for feature in record.features:\n",
    "                                if feature.qualifiers['feature_id'][0] == i.id:\n",
    "                                    if stat in feature.qualifiers.keys():\n",
    "                                        stat_dict[locus_name].append(float(feature.qualifiers[stat][0]))\n",
    "                \n",
    "                # This will make the boxplot png and will put in in the /files subdirectory\n",
    "                fig_filename = draw_boxplot(stat_dict, ylabel, '%s/files'%output_directory)\n",
    "                \n",
    "                # Distribution of stat\n",
    "                #---------------------------------------------------------------------\n",
    "                title = 'Distribution of sequence statistic \\\"'+stat+'\\\"'\n",
    "                report_lines += ( '<h4>', title, '</h4>', '')\n",
    "                \n",
    "                # This will make the img tag using the png path as src. The commented lines are an alternative\n",
    "                # making an embeded image\n",
    "                if os.path.isfile(fig_filename):\n",
    "                    #data_uri = open(fig_filename, 'rb').read().encode('base64').replace('\\n', '')\n",
    "                    img_tag = '<img src=\"%s\">'%(fig_filename.partition('/')[-1])\n",
    "                    #img_tag = '<img height=400 width='+scale+' src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
    "                    report_lines.append(img_tag)\n",
    "                    #os.remove(fig_filename)\n",
    "                \n",
    "        print \"now reporting concatenations\"\n",
    "        # Description of data concatenations\n",
    "        #------------------------------------------------------------------------\n",
    "        title = 'Description of data concatenations'.title()\n",
    "        report_lines += ('<h3>', title, '</h3>', '')\n",
    "        \n",
    "        # filter out concatenation objects that were not used to build a concatenation\n",
    "        # by taking only concat names that are in the keys of pj.trimmed_alignments.\n",
    "        # pj.trimmed_alignments is a dict with alignment names as keys and list \n",
    "        # lists containing an alignment object (biopython) and an alignment string as\n",
    "        # values.\n",
    "        composed_concatenations = []\n",
    "        for c in pj.concatenations:\n",
    "            if c.name in pj.trimmed_alignments.keys():\n",
    "                composed_concatenations.append(c)\n",
    "\n",
    "        for c in composed_concatenations:\n",
    "            \n",
    "            title = ('content of concatenation \\\"' + c.name + '\\\"').title()\n",
    "            report_lines += ('<h4>', title, '</h4>', '')\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "            # This will write the concatenation attribute\n",
    "            report_lines.append('Rules for  \\\"' + c.name + '\\\":')\n",
    "            rule_1 = 'OTUs must have the loci: '\n",
    "            for locus in c.otu_must_have_all_of:\n",
    "                rule_1 += locus + ', '\n",
    "            report_lines.append(rule_1)\n",
    "            rule_2 = 'OTUs must have at least one of each group: '\n",
    "            for group in c.otu_must_have_one_of:\n",
    "                rule_2 += str(group) +', '\n",
    "            report_lines += (rule_2, '')\n",
    "            \n",
    "            # This are the otus and loci names in the concatenation. c.feature_id_dict\n",
    "            # is a dict with the otius as keys and dicts as values. Each of these dicts\n",
    "            # have the loci as keys and the feature id as value\n",
    "            otus = c.feature_id_dict.keys()\n",
    "            loci = [locus.name for locus in c.loci]\n",
    "            \n",
    "            # This is the table's header\n",
    "            table_lines = [['','']+[locus.name for locus in c.loci]]\n",
    "            \n",
    "            \n",
    "            # Commented lines are an alternative way to write the table as \n",
    "            # pre text\n",
    "            #otus_max_length = max([len(i) for i in otus])+33\n",
    "            #loci_columns_max_length = []\n",
    "            \n",
    "            #for locus in loci:\n",
    "            #    lengths = [len(locus)]\n",
    "            #    for otu in otus:\n",
    "            #        if locus in c.feature_id_dict[otu].keys():\n",
    "            #            lengths.append(len(c.feature_id_dict[otu][locus]))\n",
    "            #        else:\n",
    "            #           lengths.append(0)\n",
    "            #    loci_columns_max_length.append(max(lengths)+3)\n",
    "                \n",
    "            #concat_header = ''.ljust(otus_max_length)\n",
    "            #for i in range(len(loci)):\n",
    "            #    concat_header += loci[i].ljust(loci_columns_max_length[i])\n",
    "            #report_lines += (concat_header, '~'*len(concat_header))\n",
    "                \n",
    "            \n",
    "            # This will write the table\n",
    "            for otu in otus:\n",
    "                \n",
    "                otu_species = ''\n",
    "                for locus in loci:\n",
    "                    if locus in c.feature_id_dict[otu].keys():\n",
    "                        feature_qualifiers = get_qualifiers_dictionary(pj, c.feature_id_dict[otu][locus])\n",
    "                        if 'source_organism' in feature_qualifiers.keys():\n",
    "                            otu_species = feature_qualifiers['source_organism']\n",
    "                otu_line = [otu, otu_species]    \n",
    "                #concat_line = (otu+' '+otu_species).ljust(otus_max_length)\n",
    "                for i in range(len(loci)):\n",
    "                    if loci[i] in c.feature_id_dict[otu].keys():\n",
    "                        #concat_line += c.feature_id_dict[otu][loci[i]].ljust(loci_columns_max_length[i])\n",
    "                        otu_line.append(c.feature_id_dict[otu][loci[i]])\n",
    "                    else:\n",
    "                        #concat_line += ''.ljust(loci_columns_max_length[i])\n",
    "                        otu_line.append('')\n",
    "                table_lines.append(otu_line)\n",
    "            \n",
    "                #report_lines.append(concat_line)\n",
    "        \n",
    "            report_lines.append(HTML.table(table_lines[1:], header_row=table_lines[0]))  \n",
    "        \n",
    "        #############################  section 2:   METHODS  #######################\n",
    "        \n",
    "        # This section prints some attributes of each of the 'Conf' objects used.\n",
    "        # The 'Conf' objects are found in a list called pj.used_methods. \n",
    "        # In an unpickled 'Project' object, the 'Conf' objects are replaced by lists of \n",
    "        # strings describing the attributes because the objects themselves do not\n",
    "        # pickle well. The formating of the list representations when they are printed\n",
    "        # still needs some beautification. Also, I plan a 'revive_methods' func to turn\n",
    "        # them back to 'Conf' objects that can be rerun.\n",
    "        report_lines += ['', '<h2>','Methods','</h2>', '']\n",
    "        \n",
    "        print \"now reporting methods\"\n",
    "        for method in pj.used_methods:\n",
    "            # This will print list representations of the 'Conf' objects\n",
    "            if isinstance(method,str):\n",
    "                title = method.split('\\n')[0]\n",
    "                report_lines += ('', '<h4>', title, '</h4>','')\n",
    "                report_lines += ('<pre>',method,'</pre>')\n",
    "            \n",
    "            # These will print attributes in actual 'Conf' objects \n",
    "            elif isinstance(method, AlnConf):\n",
    "                title = 'Seuqence Alignment Method \\\"'+method.method_name+'\\\", method ID: '+method.id\n",
    "                report_lines += ('', '<h4>', title, '</h4>', '')\n",
    "                #--------------------------------------------------------\n",
    "                align_line = 'Included loci :'\n",
    "                for locus in [locus.name for locus in method.loci]:\n",
    "                    align_line += locus + ', '\n",
    "                report_lines.append(align_line)\n",
    "                report_lines.append('Total execution time: '+str(method.timeit[3])+' sec\\'')\n",
    "                report_lines.append('Performed on: '+str(method.timeit[0]))\n",
    "                report_lines += method.platform\n",
    "                report_lines.append('')\n",
    "\n",
    "                report_lines.append('Command lines:')\n",
    "                for cline in method.command_lines.keys():\n",
    "                    report_lines.append('Alignment \\\"'+cline+'\\\":')\n",
    "                    report_lines.append('<pre style=\"white-space:normal;\">')\n",
    "                    report_lines.append(str(method.command_lines[cline]))\n",
    "                    report_lines.append('</pre>')\n",
    "                    \n",
    "            elif isinstance(method, RaxmlConf):\n",
    "                title = 'Raxml Tree Reconstruction Method \\\"'+method.method_name+'\\\", method ID: '+method.id\n",
    "                report_lines += ('', '<h4>', title, '</h4>', '')\n",
    "                #--------------------------------------------------------\n",
    "                tree_line = 'Included alignments :'\n",
    "                for aln in method.trimmed_alignments.keys():\n",
    "                    tree_line += aln + ', '\n",
    "                report_lines.append(tree_line)\n",
    "                report_lines.append('Total execution time: '+str(method.timeit[3])+' sec\\'')\n",
    "                report_lines.append('Performed on: '+str(method.timeit[0]))\n",
    "                report_lines += method.platform\n",
    "                report_lines.append('')\n",
    "\n",
    "                report_lines.append('Command lines:')\n",
    "                for aln in method.command_lines.keys():\n",
    "                    report_lines.append('Alignment \\\"'+aln+'\\\":')\n",
    "                    report_lines.append('<pre style=\"white-space:normal;\">')\n",
    "                    for cline in method.command_lines[aln]:\n",
    "                        report_lines.append(str(cline))\n",
    "                    report_lines.append('</pre>')\n",
    "                    report_lines.append('')\n",
    "                    \n",
    "            elif isinstance(method, TrimalConf):\n",
    "                title = 'Trimal alignment trimming Method \\\"'+method.method_name+'\\\", method ID: '+method.id\n",
    "                report_lines += ('<h4>', title, '</h4>', '')\n",
    "                #--------------------------------------------------------\n",
    "                aln_line = 'Included alignments :'\n",
    "                for aln in method.alignments.keys():\n",
    "                    aln_line += aln + ', '\n",
    "                report_lines.append(aln_line)\n",
    "                report_lines.append('Total execution time: '+str(method.timeit[3])+' sec\\'')\n",
    "                report_lines.append('Performed on: '+str(method.timeit[0]))\n",
    "                report_lines += method.platform\n",
    "                report_lines.append('')\n",
    "\n",
    "                report_lines.append('Command lines:')\n",
    "                for aln in method.command_lines.keys():\n",
    "                    report_lines.append('Alignment \\\"'+aln+'\\\":')\n",
    "                    report_lines.append('<pre style=\"white-space:normal;\">')\n",
    "                    report_lines.append(str(method.command_lines[aln]))\n",
    "                    report_lines.append('</pre>')\n",
    "                    report_lines.append('')\n",
    "                \n",
    "                \n",
    "        report_lines += ['',''] \n",
    "        \n",
    "        #############################  section 3:   RESULTS  #######################\n",
    "        \n",
    "        report_lines += ['', '<h2>','Results','</h2>', '']\n",
    "        print \"now reporting alignment statistics\"\n",
    "        # Global alignmnet statistics\n",
    "        #------------------------------------------------------------------------\n",
    "        title = 'Global alignmnet statistics'.title()\n",
    "        report_lines += ('<h3>', title, '</h3>', '')\n",
    "        \n",
    "        \n",
    "        # This prints things like num of unique seqs and num of parsimony informative\n",
    "        # cloumns. Takes the info from 'pj.aln_summaries' which is a list of strings.\n",
    "        # Alignment length: 1566\n",
    "        #Number of rows: 94\n",
    "        #Unique sequences: 87\n",
    "        #Average gap prop.: 0.488445\n",
    "        #Variable columns: 1045\n",
    "        #Parsimony informative: 402\n",
    "        #Undetermined sequences: 0\n",
    "        \n",
    "        \n",
    "        if len(pj.aln_summaries)>0:\n",
    "            report_lines += [('<pre>Name=Alignment name\\n'+\n",
    "                              'NumPos=Alignment length\\n'+\n",
    "                              'NumSeq=Number of sequences\\n'+\n",
    "                              'Unique=Number of unique sequences\\n'+\n",
    "                              'GapProp=Average gap proportion\\n'+\n",
    "                              'VarCols=Total variable positions\\n'+\n",
    "                              'ParsInf=Parsimony informative positions\\n'+\n",
    "                              'UnSeqs=Undetermined sequences (mostly/only gaps)\\n'+\n",
    "                              'UnSeqsCutoff=Length cutoff which defines undetermined\\n</pre>')]\n",
    "            T = [['Name','NumPos','NumSeq','Unique','GapProp','VarCols','ParsInf','UnSeqs','UnSeqsCutoff']]\n",
    "            comments = []\n",
    "            for summary in pj.aln_summaries:\n",
    "                line = []\n",
    "                for i in summary.splitlines():\n",
    "                    try:\n",
    "                        line.append(i.split(': ')[1])\n",
    "                    except:\n",
    "                        comments.append(i)\n",
    "                T.append(line)\n",
    "            report_lines += ['',\n",
    "                             HTML.table(T[1:], header_row=T[0]),\n",
    "                             '','<pre>']+comments+['</pre>']\n",
    "        else:\n",
    "            report_lines += ['','No sequence alignments in this Project','']\n",
    "                \n",
    "        \n",
    "        # Per position alignmnet statistics\n",
    "        #------------------------------------------------------------------------\n",
    "        title = 'per position alignmnet statistics'.title()\n",
    "        report_lines += ('<h3>', title, '</h3>', '')\n",
    "        if len(pj.alignments.keys())>0 and not size == 'small':                    \n",
    "            title = 'Alignment statistics before trimming'\n",
    "            report_lines += ('', '<h4>', title, '</h4>', '')\n",
    "            report_lines += ['<h4>','Trimal\\'s Residue Similarity Score (-scc)','</h4>', '']\n",
    "            \n",
    "            # draw_trimal_scc(project, num_plots_in_raw, output_dir...\n",
    "            # 'trimmed' determines if it will analyse trimmed or raw alignments\n",
    "            # alignments are taken from pj.alignments or pj.trimmed_alignments which are dictionaries\n",
    "            # with alignment names as keys and lists containing an aln object and al string as values\n",
    "            \n",
    "            # scc on raw alignments\n",
    "            fig_file = draw_trimal_scc(pj, 2, '%s/files'%output_directory, trimmed=False)\n",
    "            if os.path.isfile(fig_file):\n",
    "                    #data_uri = open(fig_file, 'rb').read().encode('base64').replace('\\n', '')\n",
    "                    #img_tag = '<img src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
    "                    img_tag = '<img src=\"%s\">'%(fig_file.partition('/')[-1])\n",
    "                    report_lines.append(img_tag)\n",
    "                    #os.remove(fig_file)\n",
    "            report_lines += [ '<h4>','Trimal\\'s column gap gcore (-sgc)','</h4>', '']\n",
    "            \n",
    "            # sgc on raw alignments\n",
    "            fig_file = draw_trimal_scc(pj, 2, '%s/files'%output_directory, trimmed=False, alg='-sgc')\n",
    "            if os.path.isfile(fig_file):\n",
    "                    #data_uri = open(fig_file, 'rb').read().encode('base64').replace('\\n', '')\n",
    "                    #img_tag = '<img src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
    "                    img_tag = '<img src=\"%s\">'%(fig_file.partition('/')[-1])\n",
    "                    report_lines.append(img_tag)\n",
    "                    #os.remove(fig_file)\n",
    "        else:\n",
    "            report_lines += ['No alignments or too many alignments in this project','']\n",
    "            \n",
    "        if len(pj.trimmed_alignments.keys())>0 and not size=='small':          \n",
    "            title = 'Alignment statistics after trimming'\n",
    "            report_lines += ('', '<h4>', title, '</h4>', '')\n",
    "            report_lines += ['<h4>','\"Trimal\\'s Residue Similarity Score (-scc)','</h4>', '']\n",
    "            \n",
    "            # scc on trimmed alignments\n",
    "            fig_file = draw_trimal_scc(pj, 2, '%s/files'%output_directory, trimmed=True)\n",
    "            if os.path.isfile(fig_file):\n",
    "                    #data_uri = open(fig_file, 'rb').read().encode('base64').replace('\\n', '')\n",
    "                    #img_tag = '<img src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
    "                    img_tag = '<img src=\"%s\">'%(fig_file.partition('/')[-1])\n",
    "                    report_lines.append(img_tag)\n",
    "                    #os.remove(fig_file)\n",
    "            report_lines += ['<h4>','Trimal\\'s column gap gcore (-sgc)','</h4>',  '']\n",
    "            \n",
    "            # sgc on trimmed alignments\n",
    "            fig_file = draw_trimal_scc(pj, 2, '%s/files'%output_directory, trimmed=True, alg='-sgc')\n",
    "            if os.path.isfile(fig_file):\n",
    "                    #data_uri = open(fig_file, 'rb').read().encode('base64').replace('\\n', '')\n",
    "                    #img_tag = '<img src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
    "                    img_tag = '<img src=\"%s\">'%(fig_file.partition('/')[-1])\n",
    "                    report_lines.append(img_tag)\n",
    "                    #os.remove(fig_file)\n",
    "        else:\n",
    "            report_lines += ['No trimmed alignments, or too many, in this project','']\n",
    "        \n",
    "        print \"making RF matrix\"\n",
    "        title = 'Robinson-Foulds distances'.title()\n",
    "        report_lines += ('<h3>', title, '</h3>', '')\n",
    "        \n",
    "        if len(pj.trees.keys())>1:\n",
    "            try:\n",
    "                RF_filename, legend = calc_rf(pj, '%s/files'%output_directory)\n",
    "                scale = str(len(legend)*60)\n",
    "                if os.path.isfile(RF_filename):\n",
    "                        #data_uri = open(RF_filename, 'rb').read().encode('base64').replace('\\n', '')\n",
    "                        #img_tag = '<img height='+scale+' width='+scale+' src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
    "                        img_tag = '<img src=\"%s\">'%(RF_filename.partition('/')[-1])\n",
    "                        report_lines.append(img_tag)\n",
    "                        #os.remove(RF_filename)\n",
    "                \n",
    "                report_lines.append('<h3>Legend<h3><pre>')\n",
    "                report_lines += legend\n",
    "                report_lines.append('</pre>')\n",
    "            except:\n",
    "                report_lines += ['Found unrooted tree(s), skipping RF distance calculation']\n",
    "\n",
    "        else:\n",
    "            report_lines += ['Less than two trees in this Project','']\n",
    "\n",
    "                \n",
    "        #############################  section 4:   TREES  #######################\n",
    "        print \"reporting trees\"\n",
    "        report_lines += ['', '<h2>','Trees','</h2>', '']\n",
    "        \n",
    "        for tree in pj.trees.keys():\n",
    "            report_lines += ('<h2>'+tree.split('@')[0]+'</h2>',\n",
    "                             '<h3>Alignment method: '+tree.split('@')[1]+'</h3>',\n",
    "                             '<h3>Trimming method: '+tree.split('@')[2]+'</h3>',\n",
    "                             '<h3>Tree method: '+tree.split('@')[3]+'</h3>',\n",
    "                             '<pre style=\"white-space:normal;\">',\n",
    "                             'Tree Method ID: '+pj.trees[tree][0].get_leaves()[0].tree_method_id,'</pre>')\n",
    "            \n",
    "            report_lines += ('<h3>newick format</h3>','','<pre style=\"white-space:normal;\">',pj.trees[tree][0].write(),'</pre>','')\n",
    "            report_lines += ('<h3>nhx format</h3>','','<pre>',pj.trees[tree][1],'</pre>','','','','')\n",
    "            \n",
    "            \n",
    "            \n",
    "            if os.path.isfile(figs_folder+'/'+pj.trees[tree][0].get_leaves()[0].tree_method_id+'.png'):\n",
    "                origin = figs_folder+'/'+pj.trees[tree][0].get_leaves()[0].tree_method_id+'.png'\n",
    "                dest = '%s/files/%s'%(output_directory, pj.trees[tree][0].get_leaves()[0].tree_method_id+'.png')\n",
    "                shutil.copyfile(origin, dest)\n",
    "                #data_handle = open(figs_folder+'/'+pj.trees[tree][0].get_leaves()[0].tree_method_id+'.png','rb')\n",
    "                #data_uri = data_handle.read().encode('base64').replace('\\n', '')\n",
    "                #data_handle.close()\n",
    "                #img_tag = '<img width=500 src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
    "                img_tag = '<img width=500 src=\"%s\">'%(dest.partition('/')[-1])\n",
    "                report_lines.append(img_tag)\n",
    "\n",
    "                \n",
    "        report_lines.append('</body>')\n",
    "        report_lines.append('</html>')\n",
    "        \n",
    "        lines = []\n",
    "        \n",
    "        for line in report_lines:\n",
    "            if '<' in line:\n",
    "                lines.append(line)\n",
    "            else:\n",
    "                lines.append(line.replace('\\n','<br>')+'<br>')\n",
    "        \n",
    "        \n",
    "        return lines\n",
    "    \n",
    "        \n",
    "def pickle_pj(pj, pickle_file_name):\n",
    "    import os\n",
    "    if os.path.exists(pickle_file_name):\n",
    "        os.remove(pickle_file_name)\n",
    "    import cloud.serialization.cloudpickle as pickle\n",
    "    output = open(pickle_file_name,'wb')\n",
    "    pickle.dump(pj, output)\n",
    "    output.close()\n",
    "    if __builtin__.git:\n",
    "        import rpgit\n",
    "        rpgit.gitAdd(pickle_file_name)\n",
    "        comment = \"A pickled Project from %s\" % time.asctime()\n",
    "        rpgit.gitCommit(comment) \n",
    "        \n",
    "    return pickle_file_name\n",
    "    \n",
    "def unpickle_pj(pickle_file_name):\n",
    "    import cloud.serialization.cloudpickle as pickle\n",
    "    pickle_handle = open(pickle_file_name, 'rb')\n",
    "    pkl_pj = pickle.pickle.load(pickle_handle)\n",
    "    new_pj = Project(pkl_pj.loci)\n",
    "    attr_names = ['aln_summaries',\n",
    "                  'alignments',\n",
    "                  'concatenations',\n",
    "                  'records',\n",
    "                  'records_by_locus',\n",
    "                  'trees',\n",
    "                  'trimmed_alignments',\n",
    "                  ]\n",
    "    \n",
    "    for attr_name in attr_names:\n",
    "       setattr(new_pj,attr_name,getattr(pkl_pj,attr_name))\n",
    "        \n",
    "    for i in pkl_pj.used_methods:\n",
    "        if isinstance(i, str):\n",
    "            new_pj.used_methods.append(i)\n",
    "        else:\n",
    "            new_pj.used_methods.append(str(i))\n",
    "    return new_pj\n",
    "\n",
    "def publish(pj, folder_name, figures_folder, size='small'):\n",
    "    \n",
    "    import os, time\n",
    "    folder = None\n",
    "    zip_file = None\n",
    "    if folder_name.endswith('.zip'):\n",
    "        zip_file = folder_name\n",
    "        folder = folder_name[:-4]\n",
    "    else:\n",
    "        folder = folder_name\n",
    "        zip_file = folder_name + '.zip'\n",
    "    print \"checking if file exists\"\n",
    "    if os.path.exists(folder) or os.path.exists(zip_file):\n",
    "        raise IOError(folder_name + ' already exists')\n",
    "    \n",
    "    os.makedirs(folder)\n",
    "    pj.write(folder+'/tree_and_alns.nexml','nexml')\n",
    "    pj.write(folder+'/sequences_and_metadata.gb','genbank')\n",
    "    report = open(folder+'/report.html','wt')\n",
    "    lines = report_methods(pj, figures_folder, folder_name, size)\n",
    "    for line in lines:\n",
    "        report.write(line + '\\n')\n",
    "    report.close()\n",
    "\n",
    "    #'report_lines' is now taking care of puting the figures in the zip folder, within /files\n",
    "    #for tree in pj.trees.keys():\n",
    "    #    if os.path.isfile(figures_folder+'/'+pj.trees[tree][0].get_leaves()[0].tree_method_id+'.png'):\n",
    "    #        from shutil import copyfile\n",
    "    #        copyfile(figures_folder+'/'+pj.trees[tree][0].get_leaves()[0].tree_method_id+'.png',\n",
    "    #                 folder+'/'+pj.trees[tree][0].get_leaves()[0].tree_method_id+'.png')\n",
    "            \n",
    "         \n",
    "    print \"pickling\"\n",
    "    pickle_name = time.strftime(\"%a_%d_%b_%Y_%X\", time.gmtime())+'.pkl'\n",
    "    pickle_pj(pj, folder + '/' + pickle_name)\n",
    "\n",
    "    \n",
    "    import zipfile, shutil\n",
    "    print \"archiving\"\n",
    "    zf = zipfile.ZipFile(zip_file, \"w\")\n",
    "    for dirname, subdirs, files in os.walk(folder):\n",
    "        zf.write(dirname)\n",
    "        for filename in files:\n",
    "            zf.write(os.path.join(dirname, filename))\n",
    "    zf.close()\n",
    "    shutil.rmtree(folder)\n",
    "    print \"report ready\"\n",
    "    \n",
    "def calc_rf(pj, figs_folder):\n",
    "    meta = 'feature_id'\n",
    "    if len(pj.concatenations) > 0:\n",
    "        meta = pj.concatenations[0].otu_meta\n",
    "\n",
    "    trees = pj.trees.keys()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for t1 in trees:\n",
    "        line = []\n",
    "        dupT1 = Tree(pj.trees[t1][0].write())\n",
    "        for l in dupT1:\n",
    "            for record in pj.records:\n",
    "                for feature in record.features:\n",
    "                    if feature.qualifiers['feature_id'][0] == l.name and meta in feature.qualifiers.keys():\n",
    "                        l.name = feature.qualifiers[meta][0]\n",
    "        for t2 in trees:\n",
    "            dupT2 = Tree(pj.trees[t2][0].write())\n",
    "            for l in dupT2:\n",
    "                for record in pj.records:\n",
    "                    for feature in record.features:\n",
    "                        if feature.qualifiers['feature_id'][0] == l.name and meta in feature.qualifiers.keys():\n",
    "                            l.name = feature.qualifiers[meta][0]\n",
    "            rf, max_rf, common_leaves, parts_t1, parts_t2 = dupT1.robinson_foulds(dupT2)        \n",
    "            line.append(rf/float(max_rf))        \n",
    "        data.append(line)   \n",
    "    \n",
    "    row_labels = [str(i) for i in range(len(trees))]\n",
    "    column_labels = row_labels\n",
    "    legend = ['#'.ljust(10,' ')+'LOCUS'.ljust(20,' ')+'ALIGNMENT METHOD'.ljust(20,' ')+\n",
    "              'TRIMMING METHOD'.ljust(20,' ')+'TREE METHOD'.ljust(20,' ')]\n",
    "    for i in trees:\n",
    "        line = str(trees.index(i)).ljust(10,' ')\n",
    "        for val in i.split('@'):\n",
    "            line += val.ljust(20,' ')\n",
    "        legend.append(line)\n",
    "    fig, ax = plt.subplots()\n",
    "    data = np.array(data)\n",
    "    heatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(data.shape[0])+0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)\n",
    "\n",
    "    # want a more natural, table-like display\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    "\n",
    "    ax.set_xticklabels(row_labels, minor=False, size=14, rotation='vertical')\n",
    "    ax.set_yticklabels(column_labels, minor=False, size=14)\n",
    "    #fig.set_size_inches(12.5,12.5)\n",
    "    fig.colorbar(heatmap, cmap=plt.cm.Blues)\n",
    "    name = str(random.randint(1000,2000))\n",
    "    fig.savefig(figs_folder + '/' + name +'.png')\n",
    "    close('all')\n",
    "    return figs_folder + '/' + name+'.png', legend \n",
    "\n",
    "def draw_trimal_scc(pj, num_col, figs_folder, trimmed=False, alg = '-scc'):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random, os\n",
    "    from Bio import AlignIO\n",
    "    \n",
    "    # get the alignment objects\n",
    "    #-------------------------#\n",
    "    alignments = pj.alignments.items()\n",
    "    if trimmed:\n",
    "        alignments = pj.trimmed_alignments.items()\n",
    "    num_alns = len(alignments)\n",
    "\n",
    "    subplots_arrangement = []\n",
    "    #-----------------------#\n",
    "    num_rows = round(float(num_alns)/num_col)\n",
    "    if num_rows < float(num_alns)/num_col:\n",
    "        num_rows += 1\n",
    "        \n",
    "    fig = plt.figure(figsize=(10*num_col,2.3*num_rows), dpi=80, frameon = False)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    subplots_arrangement += [num_rows, num_col]\n",
    "\n",
    "    \n",
    "    #Calc with trimal and plot\n",
    "    #------------------------#\n",
    "    for i in range(1,num_alns+1):\n",
    "        import subprocess as sub\n",
    "        subplot_position = subplots_arrangement +[i]\n",
    "        aln_name = alignments[i-1][0]\n",
    "        aln_obj = alignments[i-1][1]\n",
    "        name = str(random.randint(1000,2000))+'_'+aln_name+'_for_trimal_graph.fasta'\n",
    "        AlignIO.write(aln_obj, name, 'fasta')\n",
    "        stderr = open('stderr','wt')\n",
    "        #stdout = os.popen(programpath+'trimal '+alg+' -in '+name)#.read()\n",
    "        stdout = sub.Popen(programspath+\"trimal \"+alg+\" -in \" + name,\n",
    "                       shell=True, stdout=sub.PIPE, stderr=stderr).stdout\n",
    "        stderr.close()\n",
    "        var = pd.read_table(stdout, sep='\\t+', skiprows=3, engine='python')\n",
    "        os.remove('stderr')\n",
    "        if alg == '-scc':\n",
    "            var.columns = ['position', 'variability']\n",
    "        elif alg == '-sgc':\n",
    "            var.columns = ['position', 'pct_gaps', 'gap_score']\n",
    "        \n",
    "        #Plot residue similarity figure, for nucleotides this is identity value \n",
    "        fig.add_subplot(subplot_position[0], subplot_position[1], subplot_position[2])\n",
    "        if alg == '-scc':\n",
    "            var.variability.plot(color='g',lw=2)\n",
    "        elif alg == '-sgc':\n",
    "            var.pct_gaps.plot(color='g',lw=2)\n",
    "        plt.title(aln_name.replace('@',' '), fontsize=14)\n",
    "        plt.axis([1,len(aln_obj[0].seq), 0, 1.1]) #0 to 1 scale for y axis\n",
    "        xlab = \"alignment position\"\n",
    "        ylab = \"similarity score\"\n",
    "        if alg == '-sgc':\n",
    "            ylab = \"percent gaps\"\n",
    "            plt.axis([1, len(aln_obj[0].seq), 0, 110]) #0 to 100 scale for y axis, ie percent\n",
    "        plt.xlabel(xlab, fontsize=10)\n",
    "        plt.ylabel(ylab, fontsize=10)\n",
    "        plt.grid(True)\n",
    "        os.remove(name)\n",
    "    figname = str(random.randint(1000,2000))\n",
    "    fig.savefig(figs_folder + '/' + figname +'.png')\n",
    "    plt.close('all')\n",
    "    return figs_folder + '/' + figname+'.png'\n",
    "\n",
    "def view_csv_as_table(csv_filename, delimiter, quotechar='|'):\n",
    "    with open(csv_filename, 'rb') as csvfile:\n",
    "        sp_vs_lc = list(csv.reader(csvfile, delimiter=delimiter, quotechar=quotechar))\n",
    "        field_sizes = []\n",
    "        for i in range(len(sp_vs_lc[0])):\n",
    "            lengths = []\n",
    "            for row in sp_vs_lc:\n",
    "                lengths.append(len(row[i]))\n",
    "            field_sizes.append(max(lengths))\n",
    "        for row in sp_vs_lc:\n",
    "            string = ''\n",
    "            for i in range(len(row)):\n",
    "                string += row[i].ljust(field_sizes[i]+3)\n",
    "            print string\n",
    "\n",
    "def rfmt_tree_for_and_char_matirx_bayestraits(pj, qual_list, rootmeta, rootvalue, treefile=None, \n",
    "                              treetoken=None, treeburnin=0, treestep=1, treeformat=5):\n",
    "    if treefile and treetoken:\n",
    "        raise IOError(\"Only specify treefile ot treetoken, not both\")\n",
    "    T = None\n",
    "    if treefile:\n",
    "        T = open(treefile,'r').readlines()\n",
    "        T = T[:-1]\n",
    "    elif treetoken:\n",
    "        T = [pj.ft(treetoken).write()]\n",
    "    else:\n",
    "        raise IOError(\"Specify treefile or treetoken\")\n",
    "    \n",
    "    leaf_names = Tree(T[0].rstrip(), format=treeformat).get_leaf_names()\n",
    "    \n",
    "    translate = []\n",
    "    i = 1\n",
    "    for n in leaf_names:\n",
    "        translate.append([n,str(i)])\n",
    "        i +=1 \n",
    "    \n",
    "    char_matrix = \"\"\n",
    "    for n in leaf_names:\n",
    "        line = \"%s \"%n\n",
    "        quals = get_qualifiers_dictionary(pj, n)\n",
    "        for qual in qual_list:\n",
    "            if qual in quals.keys():\n",
    "                line += \"%s \"%str(quals[qual])\n",
    "            else:\n",
    "                line += \"- \"\n",
    "        line = line[:-1]+'\\n'\n",
    "        char_matrix += line\n",
    "            \n",
    "    reformatted_tree = \"\"\"#NEXUS\n",
    "    Begin Trees;\n",
    "        TRANSLATE\n",
    "    \"\"\"\n",
    "    for t in translate[:-1]:\n",
    "        reformatted_tree += '\\t\\t'+t[1]+'\\t'+t[0]+',\\n'\n",
    "    reformatted_tree += '\\t\\t'+translate[-1][1]+'\\t'+translate[-1][0]+';\\n'\n",
    "    \n",
    "    for i in range(int(len(T)*treeburnin),len(T),treestep):\n",
    "        j = T[i]\n",
    "        newick = Tree(j.rstrip(), format=treeformat)\n",
    "        brlns = []\n",
    "        for n in newick.traverse():\n",
    "            brlns.append(n.dist)\n",
    "        if sorted(brlns)[2] == 0.0: # three 0 length branches - too much\n",
    "            pass\n",
    "        else:\n",
    "            R = None\n",
    "            count = 0\n",
    "            for l in newick:\n",
    "                if get_qualifiers_dictionary(pj, l.name)[rootmeta] == rootvalue:\n",
    "                    R = l.name\n",
    "                    count += 1\n",
    "            if not count == 1:\n",
    "                raise RuntimeError(\"%s does not exist or not unique in qualifier %s\"%(rootvalue, rootmeta))\n",
    "            newick.set_outgroup(newick&R)\n",
    "            newick_str = newick.write(format=5)\n",
    "            for t in translate:\n",
    "                newick_str = re.sub(t[0],t[1],newick_str)\n",
    "            reformatted_tree += 'Tree tree'+str(i)+'= '+newick_str+'\\n'\n",
    "    reformatted_tree += 'End;\\n'\n",
    "    \n",
    "    return reformatted_tree, char_matrix\n",
    "    \n",
    "# Exonerate\n",
    "\n",
    "def bayestraits(pj, qual_list, rootmeta, rootvalue,\n",
    "                    treefile=None, treetoken=None,\n",
    "                    treeburnin=0, treestep=1,\n",
    "                    treeformat=5,\n",
    "                    bayestraits = 'BayesTraits',\n",
    "                    commands = [4,1,'kappa','delta','lambda','run']):\n",
    "    # make command file\n",
    "    import random\n",
    "    rand = random.randint(1000000,9999999)\n",
    "    cfile = open(str(rand),'wt')\n",
    "    for i in commands:\n",
    "        cfile.write(str(i)+'\\n')\n",
    "    cfile.close()\n",
    "    reformatted_tree, char_matrix = rfmt_tree_for_and_char_matirx_bayestraits(pj, qual_list, rootmeta, rootvalue, treefile=treefile, \n",
    "                                                                              treetoken=treetoken, treeburnin=treeburnin, treestep=treestep,\n",
    "                                                                              treeformat=treeformat)\n",
    "    \n",
    "    tfile = open(str(rand)+'.nex','wt')\n",
    "    tfile.write(reformatted_tree)\n",
    "    tfile.close()\n",
    "    \n",
    "    mfile = open(str(rand)+'.txt','wt')\n",
    "    mfile.write(char_matrix)\n",
    "    mfile.close()\n",
    "    \n",
    "    cline = \"%s %s %s < %s\" %(bayestraits, str(rand)+'.nex', str(rand)+'.txt', str(rand))\n",
    "    import os\n",
    "    stdout = os.popen(cline).read()\n",
    "    os.remove(str(rand))\n",
    "    os.remove(str(rand)+'.nex')\n",
    "    os.remove(str(rand)+'.txt')\n",
    "    return stdout\n",
    "\n",
    "\n",
    "# Exonerate\n",
    "\n",
    "\n",
    "\n",
    "class ExonerateCommandLine:\n",
    "    \n",
    "    \"\"\"cline object with execute methods\"\"\"\n",
    "        \n",
    "    def __init__(self,\n",
    "                 q, #query filename\n",
    "                 t, #target filename\n",
    "                 path='exonerate',\n",
    "                 Q=\"unknown\",# query alphabet\n",
    "                 T=\"unknown\", # target alphabet\n",
    "                 querychunkid=0, #query job number\n",
    "                 targetchunkid=0, #target job number\n",
    "                 querychunktotal=0, #Num of queries\n",
    "                 targetchunktotal=0, #Num of targets\n",
    "                 E=\"FALSE\", #exhaustive search\n",
    "                 B=\"FALSE\", #rapid comparison between long seqs\n",
    "                 forcescan=\"none\", #Force FSM scan on query or target sequences q or t\n",
    "                 saturatethreshold=0, #word saturation threshold\n",
    "                 customserver=\"NULL\", # Custom command to send non-standard server\n",
    "                 fastasuffix=\".fa\", #Fasta file suffix filter (in subdirectories)\n",
    "                 m=\"ungapped\",\n",
    "                 s=100, #Score threshold for gapped alignment\n",
    "                 percent=0.0, #Percent self-score threshold\n",
    "                 showalignment=\"TRUE\",\n",
    "                 showsugar=\"FALSE\",\n",
    "                 showcigar=\"FALSE\",\n",
    "                 showvulgar=\"FALSE\",\n",
    "                 showquerygff=\"FALSE\", # Include GFF output on query in results\n",
    "                 showtargetgff=\"FALSE\", #Include GFF output on target in results\n",
    "                 ryo=\"NULL\", #Roll-your-own printf-esque output format\n",
    "                 n=0, #Report best N results per query\n",
    "                 S=\"TRUE\", #Search for suboptimal alignments\n",
    "                 g=\"TRUE\", #Use gapped extension\n",
    "                 refine=\"none\", #none|full|region\n",
    "                 refineboundary=32, #Refinement region boundary\n",
    "                 D=32, #Maximum memory to use for DP tracebacks (Mb)\n",
    "                 C=\"TRUE\", #Use compiled viterbi implementations\n",
    "                 terminalrangeint=12, #Internal terminal range\n",
    "                 terminalrangeext=12, #External terminal range\n",
    "                 joinrangeint=12, #Internal join range\n",
    "                 joinrangeext=12, #External join range\n",
    "                 x=50, #Gapped extension threshold\n",
    "                 singlepass=\"TRUE\", #Generate suboptimal alignment in a single pass\n",
    "                 joinfilter=0, #BSDP join filter threshold\n",
    "                 A=\"none\", #Path to sequence annotation file\n",
    "                 softmaskquery=\"FALSE\", #Allow softmasking on the query sequence\n",
    "                 softmasktarget=\"FALSE\", #Allow softmasking on the target sequence\n",
    "                 d=\"nucleic\", #DNA substitution matrix\n",
    "                 p=\"blosum62\", #Protein substitution matrix\n",
    "                 M=64, #Memory limit for FSM scanning <Mb>\n",
    "                 forcefsm=\"none\", #Force FSM type ( normal | compact )\n",
    "                 wordjump=1, #Jump between query words\n",
    "                 o=-12, #Affine gap open penalty\n",
    "                 e=-4, #Affine gap extend penalty\n",
    "                 codongapopen=-18, #Codon affine gap open penalty\n",
    "                 codongapextend=-8, #Codon affine gap extend penalty\n",
    "                 minner=10, #Minimum NER length\n",
    "                 maxner=50000, #Maximum NER length\n",
    "                 neropen=-20, #NER open penalty\n",
    "                 minintron=30, #Minimum intron length\n",
    "                 maxintron=20000, #Maximum intron length\n",
    "                 i=-30, #Intron Opening penalty\n",
    "                 f=-28, #Frameshift creation penalty\n",
    "                 useaatla=\"TRUE\", #useaatla\n",
    "                 geneticcode=1, #Use built-in or custom genetic code\n",
    "                 hspfilter=0, #Aggressive HSP filtering level\n",
    "                 useworddropoff=\"TRUE\", #Use word neighbourhood dropoff\n",
    "                 seedrepeat=1, #Seeds per diagonal required for HSP seeding\n",
    "                 dnawordlen=12, #Wordlength for DNA words\n",
    "                 proteinwordlen=6, #Wordlength for protein words\n",
    "                 codonwordlen=12, #Wordlength for codon words\n",
    "                 dnahspdropoff=30, #DNA HSP dropoff score\n",
    "                 proteinhspdropoff=20, #Protein HSP dropoff score\n",
    "                 codonhspdropoff=40, #Codon HSP dropoff score\n",
    "                 dnahspthreshold=75, #DNA HSP threshold score\n",
    "                 proteinhspthreshold=30, #Protein HSP threshold score\n",
    "                 codonhspthreshold=50, #Codon HSP threshold score\n",
    "                 dnawordlimit=0, #Score limit for dna word neighbourhood\n",
    "                 proteinwordlimit=4, #Score limit for protein word neighbourhood\n",
    "                 codonwordlimit=4, #Score limit for codon word neighbourhood\n",
    "                 geneseed=0, #Geneseed Threshold\n",
    "                 geneseedrepeat=3, #Seeds per diagonal required for geneseed HSP seeding\n",
    "                 alignmentwidth=80, #Alignment display width\n",
    "                 forwardcoordinates=\"TRUE\", #Report all coordinates on the forward strand\n",
    "                 quality=0, #HSP quality threshold\n",
    "                 splice3=\"primate\", #Supply frequency matrix for 3' splice sites\n",
    "                 splice5=\"primate\", #Supply frequency matrix for 5' splice sites\n",
    "                 forcegtag=\"FALSE\"): #Force use of gt...ag splice sites\n",
    "        \n",
    "        import inspect\n",
    "\n",
    "        frame = inspect.currentframe()\n",
    "        args, _, _, values = inspect.getargvalues(frame)\n",
    "\n",
    "        cline = values['path']+' '\n",
    "        \n",
    "        for k in ('path','frame','inspect', 'self'):\n",
    "            del values[k] \n",
    "        \n",
    "        for keyward in values:\n",
    "            \n",
    "            dash = '-'\n",
    "            if len(keyward) > 1:\n",
    "                 dash = '--'\n",
    "            cline += dash+keyward+' '+str(values[keyward])+' '\n",
    "        self.cline_string = cline\n",
    "        self.stdout = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.cline_string\n",
    "    \n",
    "    def execute(self):\n",
    "        import subprocess as sub\n",
    "        p = sub.Popen(self.cline_string, shell=True, stdout=sub.PIPE, stderr=sub.PIPE)\n",
    "        self.stdout = p.communicate()[0]\n",
    "        return self.stdout\n",
    "\n",
    "# an all inclusive exonerate ryo format \n",
    "\n",
    "roll = (\"STARTRYOqfull = %qas @!!@!!@ qcds = %qcs @!!@!!@ qid = %qi @!!@!!@ qdescription = %qd @!!@!!@ \"+\n",
    "       \"qlen = %qal @!!@!!@ qstrand = %qS @!!@!!@ qtype = %qt @!!@!!@ qbegin = %qab @!!@!!@ qend = %qae @!!@!!@ \"+\n",
    "       \"tfull = %tas @!!@!!@ tcds = %tcs @!!@!!@ tid = %ti @!!@!!@ tdescription = %td @!!@!!@ \"+\n",
    "       \"tlen = %tal @!!@!!@ tstrand = %tS @!!@!!@ ttype = %tt @!!@!!@ tbegin = %tab @!!@!!@ tend = %tae @!!@!!@ \"+\n",
    "       \"Etotal = %et @!!@!!@ Eident = %ei @!!@!!@ Esim = %es @!!@!!@ Emis = %em @!!@!!@ Pident = %pi @!!@!!@ \"+\n",
    "       \"Psim = %ps @!!@!!@ score = %s @!!@!!@ model = %m @!!@!!@ vulgar = %VENDRYO\")\n",
    "\n",
    "def parse_ryo(exo_results):\n",
    "    \n",
    "    \"\"\"parses exonerate results that include the ryo line above\"\"\"\n",
    "    \n",
    "    stats = []\n",
    "    ryos = [i.split(\"ENDRYO\")[0] for i in exo_results.split('STARTRYO')[1:]][1:]\n",
    "    if len(ryos) > 0:\n",
    "        for i in ryos:\n",
    "            a = {}\n",
    "            for line in i.split('@!!@!!@'):\n",
    "                k, v = [line.partition('=')[0], line.partition('=')[2]]\n",
    "                a[k.strip().rstrip()] = v.strip().rstrip()\n",
    "            stats.append(a)\n",
    "            a['qfull'] = a['qfull'].replace('\\n','').replace(\"\\s\",'')\n",
    "            a['qcds'] = a['qcds'].replace('\\n','').replace(\"\\s\",'')\n",
    "            a['tfull'] = a['tfull'].replace('\\n','').replace(\"\\s\",'')\n",
    "            a['tcds'] = a['tcds'].replace('\\n','').replace(\"\\s\",'')\n",
    "            \n",
    "    return stats\n",
    "\n",
    "def exonerate(q, d, **kwargs):\n",
    "    \n",
    "    \"\"\"Will run exonerate with the ryo format above.\n",
    "    Returns a dictionary with the ryo line content and the raw output as string\"\"\"\n",
    "    \n",
    "    if 'ryo' in kwargs.keys():\n",
    "        kwargs.pop('ryo', None)\n",
    "    results = ''\n",
    "    exoCline = ExonerateCommandLine(q, d, ryo=\"\\\"%s\\\"\"%roll, **kwargs)\n",
    "    results = exoCline.execute()\n",
    "    #print results\n",
    "    stats = parse_ryo(results)\n",
    "    #print stats\n",
    "    return stats, results\n",
    "\n",
    "\n",
    "def exonerate_ryo_to_gb(q, d, stats, results, get_query=False):\n",
    "    \n",
    "    \"\"\"takes the parsed ryo (stats) and the raw output (results) and\n",
    "    builds a gb file for reprophylo\"\"\"\n",
    "    \n",
    "    if get_query:\n",
    "        raise RuntimeError('get_query=True: currently only parses target. maybe Bio.SearchIO can help?')\n",
    "    \n",
    "    gencode = int(results.split('geneticcode ')[1].split()[0])\n",
    "    model = stats[0]['model']\n",
    "    if not model == 'protein2genome:local':\n",
    "        raise RuntimeError(\"only tested with the protein2genome model\")\n",
    "    #gencode = '1'\n",
    "    tfile = d\n",
    "    if '/' in d:\n",
    "        tfile = d.split('/')[-1]\n",
    "    matches = stats\n",
    "    b = 0\n",
    "    records = []\n",
    "    # making a short enough, yet unique and informative seq id is a challenge\n",
    "    # The approach here is to take the 4 first and last chars of the input file name\n",
    "    # and to add a serial number for each seq.\n",
    "    \n",
    "    # We add a random three digit number at the start\n",
    "    # because the eight file name chars are not always unique accorss files.\n",
    "    from random import randint\n",
    "        \n",
    "    rnd = randint(111,999)\n",
    "    \n",
    "    for match in matches:\n",
    "        ID = (\"%i|%s|%s|%i\"%(rnd, tfile[:4],tfile[-4:],b)).replace('.','')\n",
    "        \n",
    "        r = SeqRecord(seq=Seq(match['tfull'],\n",
    "                              alphabet=IUPAC.unambiguous_dna),\n",
    "                      id = ID,\n",
    "                      description = \"query: %s %s, target: %s %s, HSPID: %i\"%(match['qid'],\n",
    "                                                                              match['qdescription'],\n",
    "                                                                              match['tid'],\n",
    "                                                                              match['tdescription'],\n",
    "                                                                              b))\n",
    "        b += 1\n",
    "        source = SeqFeature(FeatureLocation(0, len(r.seq)), type='source')\n",
    "        source.qualifiers['file'] = [tfile]\n",
    "        for key in match:\n",
    "            source.qualifiers[key] = [match[key]]\n",
    "        r.features.append(source)\n",
    "        vulgar = match['vulgar'].split()\n",
    "        features = [vulgar[i:i+3] for i in range(0, len(vulgar) - 2, 3)]\n",
    "        pos = 0\n",
    "        CDS = None\n",
    "        for feature in features:\n",
    "            ftype = feature[0]\n",
    "            flength = int(feature[2])\n",
    "            f = SeqFeature(FeatureLocation(pos, pos+flength), type = ftype)\n",
    "            f.qualifiers['gene'] = [match['qid']]\n",
    "            pos += flength\n",
    "            r.features.append(f)\n",
    "        coding_locations = []\n",
    "        for j in r.features:\n",
    "            if j.type == 'M' or j.type == 'S':\n",
    "                coding_locations.append(j.location)\n",
    "            elif j.type == 'G' and (int(j.location.start) < int(j.location.end)):\n",
    "                coding_locations.append(j.location)\n",
    "        coding_locations = sorted(coding_locations, key = lambda l: int(l.start))\n",
    "        if len(coding_locations) == 1:\n",
    "            CDS = SeqFeature(coding_locations[0], type='CDS')\n",
    "        else:\n",
    "            CDS = SeqFeature(CompoundLocation(coding_locations), type='CDS')\n",
    "        CDS.qualifiers['gene'] = [match['qid']]\n",
    "        get = False\n",
    "        try:\n",
    "            CDS.qualifiers['translation'] = [str(CDS.extract(r.seq).translate(table=gencode)).replace('*','X')]\n",
    "            assert CDS.qualifiers['translation'][0] == str(Seq(match['tcds'].replace(' ','').replace('\\n',''),\n",
    "                                                                alphabet=IUPAC.ambiguous_dna).translate(table=gencode)).replace('*','X')\n",
    "            get = True\n",
    "        except:\n",
    "            CDS.qualifiers['translation'] = ['something went wrong']\n",
    "            print \"DEBUG bad CDS\"\n",
    "            print match['tid']\n",
    "            #print len(CDS.extract(r.seq)), len(match['tcds'].replace(' ',''))\n",
    "            #print str(CDS.extract(r.seq))[:20], str(CDS.extract(r.seq))[-20:]\n",
    "            #print match['tcds'].replace(' ','')[:20], match['tcds'].replace(' ','')[-20:]\n",
    "            print (str(CDS.extract(r.seq)) == match['tcds'].replace(' ',''))\n",
    "            if (str(CDS.extract(r.seq)) == match['tcds'].replace(' ','')):\n",
    "                print 'CDS retrieved correctly buy biopython could not translate'\n",
    "            else:\n",
    "                print 'Error in retrieved CDS (CDS built form vulgar does not match the CDS from ryo \\%tcs'\n",
    "            #print str(CDS.extract(r.seq))\n",
    "        if get:\n",
    "            r.features.append(CDS)\n",
    "            records.append(r)\n",
    "    a = SeqIO.write(records,'%s.gb'%d,'genbank')\n",
    "    return \"%i in %s.gb\"%(a, d)\n",
    "\n",
    "def report_aln_col_stat(pj, loci_names, num_col, figs_folder, trimmed=False, alg = '-scc'):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random, os\n",
    "    from Bio import AlignIO\n",
    "    \n",
    "    # get the alignment objects\n",
    "    #-------------------------#\n",
    "    alignments = pj.alignments.items()\n",
    "    if trimmed:\n",
    "        alignments = pj.trimmed_alignments.items()\n",
    "    alignments = [i for i in alignments if i[0].split('@')[0] in loci_names]\n",
    "    num_alns = len(alignments)\n",
    "    subplots_arrangement = []\n",
    "    #-----------------------#\n",
    "    num_rows = round(float(num_alns)/num_col)\n",
    "    if num_rows < float(num_alns)/num_col:\n",
    "        num_rows += 1\n",
    "        \n",
    "    fig = plt.figure(figsize=(10*num_col,2.3*num_rows), dpi=80, frameon = False)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    subplots_arrangement += [num_rows, num_col]\n",
    "\n",
    "\n",
    "    #Calc with trimal and plot\n",
    "    #------------------------#\n",
    "    for i in range(1,num_alns+1):\n",
    "        import subprocess as sub\n",
    "        subplot_position = subplots_arrangement +[i]\n",
    "        aln_name = alignments[i-1][0]\n",
    "        aln_obj = alignments[i-1][1]\n",
    "        name = str(random.randint(1000,2000))+'_'+aln_name+'_for_trimal_graph.fasta'\n",
    "        AlignIO.write(aln_obj, name, 'fasta')\n",
    "        stderr = open('stderr','wt')\n",
    "        #stdout = os.popen('trimal '+alg+' -in '+name)#.read()\n",
    "        stdout = sub.Popen(\"trimal \"+alg+\" -in \" + name,\n",
    "                       shell=True, stdout=sub.PIPE, stderr=stderr).stdout\n",
    "        stderr.close()\n",
    "        var = pd.read_table(stdout, sep='\\t+', skiprows=3, engine='python')\n",
    "        os.remove('stderr')\n",
    "        if alg == '-scc':\n",
    "            var.columns = ['position', 'variability']\n",
    "        elif alg == '-sgc':\n",
    "            var.columns = ['position', 'pct_gaps', 'gap_score']\n",
    "        \n",
    "        #Plot residue similarity figure, for nucleotides this is identity value\n",
    "        fig.add_subplot(subplot_position[0], subplot_position[1], subplot_position[2])\n",
    "        if alg == '-scc':\n",
    "            var.variability.plot(color='g',lw=2)\n",
    "        elif alg == '-sgc':\n",
    "            var.pct_gaps.plot(color='g',lw=2)\n",
    "        plt.title(aln_name.replace('@',' '), fontsize=14)\n",
    "        plt.axis([1,len(aln_obj[0].seq), 0, 1.1]) #0 to 1 scale for y axis\n",
    "        xlab = \"alignment position\"\n",
    "        ylab = \"similarity score\"\n",
    "        if alg == '-sgc':\n",
    "            ylab = \"percent gaps\"\n",
    "            plt.axis([1, len(aln_obj[0].seq), 0, 110]) #0 to 100 scale for y axis, ie percent\n",
    "        plt.xlabel(xlab, fontsize=10)\n",
    "        plt.ylabel(ylab, fontsize=10)\n",
    "        plt.grid(True)\n",
    "        os.remove(name)\n",
    "    figname = str(random.randint(1000,2000))\n",
    "    fig.savefig(figs_folder + '/' + figname +'.png')\n",
    "    plt.close('all')\n",
    "    return figs_folder + '/' + figname+'.png'\n",
    "\n",
    "##############################################################################################################\n",
    "if False:\n",
    "    \"\"\"\"LociStat class preliminaries\"\"\"\n",
    "##############################################################################################################\n",
    "\n",
    "def entropy(s, char_type):\n",
    "    \"\"\"\n",
    "    Return the Shannon's entropy value for a column in the alignment provided as a string (s) \n",
    "    given the character type (dna or prot).\n",
    "    \n",
    "    gaps are ignored, ambiguity is ignored.\n",
    "    \n",
    "    \n",
    "    homogenous column\n",
    "    \n",
    "    >>> entropy('tttttttt', 'dna')\n",
    "    -0.0\n",
    "    \n",
    "    hetrogenous column, case insensitive\n",
    "    \n",
    "    >>> entropy('ttgGaacC', 'dna')\n",
    "    2.0\n",
    "    >>> entropy('ttggaacc', 'dna')\n",
    "    2.0\n",
    "    \n",
    "    ignore gaps\n",
    "    \n",
    "    >>> entropy('ttgg--Ss', 'dna')\n",
    "    1.0\n",
    "    >>> entropy('ttggSs', 'dna')\n",
    "    1.0\n",
    "    \n",
    "    recognize alphabet\n",
    "    \n",
    "    >>> entropy('ttggSs', 'prot')\n",
    "    1.584962500721156\n",
    "    \"\"\"\n",
    "    missing = None\n",
    "    if char_type == 'prot':\n",
    "        missing = 'Xx'\n",
    "    elif char_type == 'dna':\n",
    "        missing = 'ryswkmbdhvnRYSWKMBDHVN'\n",
    "    s = s.replace('-','').replace('.','').replace('?','')\n",
    "    for m in missing:\n",
    "        s = s.replace(m,'')\n",
    "    p, lns = Counter(s.lower()), float(len(s.lower()))\n",
    "    \n",
    "    return -sum( count/lns * math.log(count/lns, 2) for count in p.values())\n",
    "\n",
    "def gapscore(aln_obj):\n",
    "    \n",
    "    \"\"\"\n",
    "    Use  TrimAl to get a list of gapscores given a MultipleSeqAlignmnet\n",
    "    object\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    name = str(random.randint(1000,2000))+'_for_trimal_graph.fasta'\n",
    "    AlignIO.write(aln_obj, name, 'fasta')\n",
    "    stderr = open('stderr','wt')\n",
    "    stdout = sub.Popen(programspath+\"trimal -sgc -in \" + name,\n",
    "                       shell=True, stdout=sub.PIPE, stderr=stderr).stdout\n",
    "    stderr.close()\n",
    "    var = pd.read_table(stdout, sep='\\t+', skiprows=3, engine='python')\n",
    "    os.remove('stderr')\n",
    "    var.columns = ['position', 'pct_gaps', 'gap_score']\n",
    "    os.remove(name)  \n",
    "    return [i[1] for i in var.to_dict()['gap_score'].items()]\n",
    "\n",
    "def conservation(aln_obj):\n",
    "    \n",
    "    \"\"\"\n",
    "    Use  TrimAl to get a list of conservation values given a MultipleSeqAlignmnet\n",
    "    object\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    name = str(random.randint(1000,2000))+'_for_trimal_graph.fasta'\n",
    "    AlignIO.write(aln_obj, name, 'fasta')\n",
    "    stderr = open('stderr','wt')\n",
    "    stdout = sub.Popen(programspath+\"trimal -scc -in \" + name,\n",
    "                       shell=True, stdout=sub.PIPE, stderr=stderr).stdout\n",
    "    stderr.close()\n",
    "    var = pd.read_table(stdout, sep='\\t+', skiprows=3, engine='python')\n",
    "    os.remove('stderr')\n",
    "    var.columns = ['position', 'conservation']\n",
    "    os.remove(name)  \n",
    "    return [i[1] for i in var.to_dict()['conservation'].items()]\n",
    "\n",
    "def get_entropies(pj, trimmed = True, alignmnet_method=None, trimming_method=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return a dictionary with alignment names as keys and  entropy keys as values\n",
    "    given the alignments or trimmed alignmnets dictionary.\n",
    "    \n",
    "    \n",
    "    If a locus has more than one alignment in the dictionary, go for the specified method\n",
    "    or the first occurance\n",
    "    \"\"\"\n",
    "    entropies = {}\n",
    "    aln_dict = None\n",
    "    if trimmed:\n",
    "        aln_dict = pj.trimmed_alignments\n",
    "        if aln_dict == {}:\n",
    "            raise IOError(\"No trimed alignments in the Project\")\n",
    "                          \n",
    "    elif not trimmed:\n",
    "        aln_dict = pj.alignments\n",
    "        if aln_dict == {}:\n",
    "            raise IOError(\"No alignments in the Project\")\n",
    "        \n",
    "    for aln_name in aln_dict.keys():\n",
    "        char_type = None\n",
    "        char_type_list = [l.char_type for l in pj.loci if l.name == aln_name.split('@')[0]]\n",
    "        get = True\n",
    "        if len(char_type_list) == 0:\n",
    "            get = False\n",
    "            warnings.warn('Cannot find Locus for alignment %s. Is it a supermatrix? Skipping.'%aln_name)            \n",
    "        elif len(char_type_list) > 1:\n",
    "            if (alignmnet_method and \n",
    "                not aln_name.split('@')[1] == alignmnet_method):\n",
    "                get = False\n",
    "                warnings.warn('Skipping %s, taking only %s'%aln_name,alignmnet_method)\n",
    "            elif (trimmed and trimming_method and \n",
    "                not aln_name.split('@')[2] == trimming_method):\n",
    "                get = False\n",
    "                warnings.warn('Skipping %s, taking only %s'%aln_name,trimming_method)  \n",
    "            elif aln_name.split('@')[0] in [i.split('@')[0] for i in entropies.keys()]:\n",
    "                exists = [i for i in entropies.keys() if i.split('@')[0] == aln_name.split('@')[0]][0]\n",
    "                get = False\n",
    "                warning.warn('Skipping %s, already have %s'%aln_name, exists)\n",
    "        if get:    \n",
    "            char_type = char_type_list[0]\n",
    "            aln_obj = aln_dict[aln_name]\n",
    "            entropies[aln_name] =[]\n",
    "            for i in range(aln_obj.get_alignment_length()):\n",
    "                column = aln_obj[:,i]\n",
    "                entropies[aln_name].append(entropy(column, char_type))\n",
    "\n",
    "    return entropies\n",
    "\n",
    "def get_gapscores(pj, trimmed = True, alignmnet_method=None, trimming_method=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return a dictionary with alignment names as keys and  gap scores as values\n",
    "    given the alignments or trimmed alignmnets dictionary.\n",
    "    \n",
    "    \n",
    "    If a locus has more than one alignment in the dictionary, go for the specified method\n",
    "    or the first occurance\n",
    "    \"\"\"\n",
    "    gapscores = {}\n",
    "    aln_dict = None\n",
    "    if trimmed:\n",
    "        aln_dict = pj.trimmed_alignments\n",
    "        if aln_dict == {}:\n",
    "            raise IOError(\"No trimed alignments in the Project\")\n",
    "    elif not trimmed:\n",
    "        aln_dict = pj.alignments\n",
    "        if aln_dict == {}:\n",
    "            raise IOError(\"No alignments in the Project\")\n",
    "        \n",
    "    for aln_name in aln_dict.keys():\n",
    "        char_type = None\n",
    "        char_type_list = [l.char_type for l in pj.loci if l.name == aln_name.split('@')[0]]\n",
    "        get = True\n",
    "        if len(char_type_list) == 0:\n",
    "            get = False\n",
    "            warnings.warn('Cannot find Locus for alignment %s. Is it a supermatrix? Skipping.'%aln_name)            \n",
    "        elif len(char_type_list) > 1:\n",
    "            if (alignmnet_method and \n",
    "                not aln_name.split('@')[1] == alignmnet_method):\n",
    "                get = False\n",
    "                warnings.warn('Skipping %s, taking only %s'%aln_name,alignmnet_method)\n",
    "            elif (trimmed and trimming_method and \n",
    "                not aln_name.split('@')[2] == trimming_method):\n",
    "                get = False\n",
    "                warnings.warn('Skipping %s, taking only %s'%aln_name,trimming_method)  \n",
    "            elif aln_name.split('@')[0] in [i.split('@')[0] for i in gapscores.keys()]:\n",
    "                exists = [i for i in gapscores.keys() if i.split('@')[0] == aln_name.split('@')[0]][0]\n",
    "                get = False\n",
    "                warning.warn('Skipping %s, already have %s'%aln_name, exists)   \n",
    "        if get:    \n",
    "            char_type = char_type_list[0]\n",
    "            aln_obj = aln_dict[aln_name]\n",
    "            gapscores[aln_name] = gapscore(aln_obj)  \n",
    "    return gapscores\n",
    "\n",
    "def get_conservations(pj, trimmed = True, alignmnet_method=None, trimming_method=None):\n",
    "    \"\"\"\n",
    "    Return a dictionary with alignment names as keys and  conservation scores as values\n",
    "    given the alignments or trimmed alignmnets dictionary.\n",
    "    \n",
    "    \n",
    "    If a locus has more thn one alignment in the dictionary, go for the specified method\n",
    "    or the first occurance\n",
    "    \"\"\"\n",
    "    conservations = {}\n",
    "    aln_dict = None\n",
    "    if trimmed:\n",
    "        aln_dict = pj.trimmed_alignments\n",
    "        if aln_dict == {}:\n",
    "            raise IOError(\"No trimed alignments in the Project\")\n",
    "            \n",
    "    elif not trimmed:\n",
    "        aln_dict = pj.alignments\n",
    "        if aln_dict == {}:\n",
    "            raise IOError(\"No alignments in the Project\")\n",
    "        \n",
    "    for aln_name in aln_dict.keys():\n",
    "        char_type = None\n",
    "        char_type_list = [l.char_type for l in pj.loci if l.name == aln_name.split('@')[0]]\n",
    "        get = True\n",
    "        if len(char_type_list) == 0:\n",
    "            get = False\n",
    "            warnings.warn('Cannot find Locus for alignment %s. Is it a supermatrix? Skipping.'%aln_name)            \n",
    "        elif len(char_type_list) > 1:\n",
    "            if (alignmnet_method and \n",
    "                not aln_name.split('@')[1] == alignmnet_method):\n",
    "                get = False\n",
    "                warnings.warn('Skipping %s, taking only %s'%aln_name,alignmnet_method)\n",
    "            elif (trimmed and trimming_method and \n",
    "                not aln_name.split('@')[2] == trimming_method):\n",
    "                get = False\n",
    "                warnings.warn('Skipping %s, taking only %s'%aln_name,trimming_method) \n",
    "            elif aln_name.split('@')[0] in [i.split('@')[0] for i in conservations.keys()]:\n",
    "                exists = [i for i in conservations.keys() if i.split('@')[0] == aln_name.split('@')[0]][0]\n",
    "                get = False\n",
    "                warning.warn('Skipping %s, already have %s'%aln_name, exists)       \n",
    "        if get:    \n",
    "            char_type = char_type_list[0]\n",
    "            aln_obj = aln_dict[aln_name]\n",
    "            conservations[aln_name] = conservation(aln_obj)  \n",
    "    return conservations\n",
    "\n",
    "def get_sequence_lengths(pj):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return a dictionary with locus names as keys and lists of sequence length as values\n",
    "    given a Project instance.\n",
    "    \n",
    "    The length are calculated from the unaligned and untrimmed sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(pj.records) == 0:\n",
    "            raise IOError('No records in the Project')\n",
    "    \n",
    "    lengths = {}\n",
    "\n",
    "    \n",
    "    if len(pj.records_by_locus) == 0:\n",
    "        pj.extract_by_locus()\n",
    "    \n",
    "    for locus in pj.records_by_locus:\n",
    "        lengths[locus] = []\n",
    "        for r in pj.records_by_locus[locus]:\n",
    "            lengths[locus].append(len(r.seq))\n",
    "    return lengths\n",
    "\n",
    "def get_sequence_gcs(pj):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return a dictionary with locus names as keys and lists of sequence %GC as values\n",
    "    given a Project instance.\n",
    "    \n",
    "    The %GC are calculated from the unaligned and untrimmed sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(pj.records) == 0:\n",
    "            raise IOError('No records in the Project')\n",
    "    \n",
    "    gcs = {}\n",
    "    \n",
    "    if len(pj.records_by_locus) == 0:\n",
    "        pj.extract_by_locus()\n",
    "    \n",
    "    if any([l.char_type == 'prot' for l in pj.loci]):\n",
    "        warnings.warn('Protein loci GC content will be set to 0.0')\n",
    "    \n",
    "    for locus in pj.records_by_locus:\n",
    "        char_type = [l for l in pj.loci if l.name == locus][0].char_type\n",
    "        if char_type == 'dna':\n",
    "            gcs[locus] = []\n",
    "    \n",
    "            for r in pj.records_by_locus[locus]:\n",
    "                gcs[locus].append(GC(r.seq))\n",
    "        else:\n",
    "            gcs[locus] = [0.0, 0.0, 0.0, 0.0]\n",
    "        \n",
    "    return gcs\n",
    "        \n",
    "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
    "    \"\"\"\n",
    "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
    "    \n",
    "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
    "    \"\"\"\n",
    "    ax = axes or plt.gca()\n",
    "    ax.spines['top'].set_visible(top)\n",
    "    ax.spines['right'].set_visible(right)\n",
    "    ax.spines['left'].set_visible(left)\n",
    "    ax.spines['bottom'].set_visible(bottom)\n",
    "    \n",
    "    #turn off all ticks\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    \n",
    "    #now re-enable visibles\n",
    "    if top:\n",
    "        ax.xaxis.tick_top()\n",
    "    if bottom:\n",
    "        ax.xaxis.tick_bottom()\n",
    "    if left:\n",
    "        ax.yaxis.tick_left()\n",
    "    if right:\n",
    "        ax.yaxis.tick_right()\n",
    "\n",
    "#########################################################################################\n",
    "class LociStats:\n",
    "#########################################################################################\n",
    "    \n",
    "    def __init__(self, pj, trimmed=True, alignmnet_method=None, trimming_method=None):\n",
    "        \n",
    "        self.loci = pj.loci\n",
    "        \n",
    "        self.entropeis = get_entropies(pj,\n",
    "                                       trimmed = trimmed,\n",
    "                                       alignmnet_method=alignmnet_method,\n",
    "                                       trimming_method=trimming_method)\n",
    "        \n",
    "        self.gapscores = get_gapscores(pj,\n",
    "                                       trimmed = trimmed,\n",
    "                                       alignmnet_method=alignmnet_method,\n",
    "                                       trimming_method=trimming_method)\n",
    "        \n",
    "        self.conservations = get_conservations(pj,\n",
    "                                               trimmed = trimmed,\n",
    "                                               alignmnet_method=alignmnet_method,\n",
    "                                               trimming_method=trimming_method)\n",
    "        \n",
    "        self.sequence_lengths = get_sequence_lengths(pj)\n",
    "        \n",
    "        self.sequence_gcs = get_sequence_gcs(pj)\n",
    "        \n",
    "        combined = []\n",
    "        \n",
    "        for key in self.sequence_lengths:\n",
    "            locus_stats = [key]\n",
    "            \n",
    "            entropies = [self.entropeis[i] for i in self.entropeis.keys() if i.split('@')[0] == key][0]\n",
    "            gapscores = [self.gapscores[i] for i in self.gapscores.keys() if i.split('@')[0] == key][0]\n",
    "            conservations = [self.conservations[i] for i in self.conservations.keys() if i.split('@')[0] == key][0]\n",
    "            \n",
    "            combined.append([key,\n",
    "                             self.sequence_lengths[key],\n",
    "                             self.sequence_gcs[key],\n",
    "                             entropies,\n",
    "                             gapscores,\n",
    "                             conservations])\n",
    "            \n",
    "        self.loci_stats = combined\n",
    "        \n",
    "        self.loci_stats_sorted = None\n",
    "        \n",
    "    def sort(self, parameter = 'entropy', percentile=50, percentile_range=(25,75), reverse = True):\n",
    "        j = None\n",
    "        if parameter == 'entropy':\n",
    "            j = 3\n",
    "        elif parameter == 'gapscore':\n",
    "            j = 4\n",
    "        elif parameter == 'conservation':\n",
    "            j = 5\n",
    "        elif parameter == 'sequence_length':\n",
    "            j = 1\n",
    "        elif parameter == 'sequence_gc':\n",
    "            j = 2\n",
    "        \n",
    "        self.loci_stats_sorted = sorted(self.loci_stats,\n",
    "                                        key=lambda i: (np.percentile(i[j], percentile),\n",
    "                                                       abs(np.percentile(i[j], percentile_range[1])-\n",
    "                                                           np.percentile(i[j], percentile_range[0]))),\n",
    "                                        reverse=reverse)\n",
    "        self.loci_stats_sorted = [list(i) for i in self.loci_stats_sorted]\n",
    "        \n",
    "    def plot(self, filename, figsize=(30,10), params='all', lable_fsize=40, xtick_fsize=4, ytick_fsize=4):\n",
    "        parameter_indices = [3,4,5,1,2]\n",
    "        \n",
    "        ytitles=['',\n",
    "                 'Sequence Lengths',\n",
    "                 'Sequence %GC',\n",
    "                 'Entropy',\n",
    "                 'Gap Score', \n",
    "                 'Conservation Scores']\n",
    "        \n",
    "        if not  params=='all':\n",
    "            parameter_indices = []\n",
    "            for param in params:\n",
    "                if param == 'entropy':\n",
    "                    parameter_indices.append(3)\n",
    "                elif param == 'gapscore':\n",
    "                    parameter_indices.append(4)\n",
    "                elif param == 'conservation':\n",
    "                    parameter_indices.append(5)\n",
    "                elif param == 'sequence_length':\n",
    "                    parameter_indices.append(1)\n",
    "                elif param == 'sequence_gc':\n",
    "                    parameter_indices.append(2)\n",
    "        if len(parameter_indices) == 0:\n",
    "            raise IOError('Must specify at least one parameter to plot')\n",
    "                \n",
    "        #fig = plt.figure(figsize=figsize, dpi=80, frameon = False)\n",
    "        \n",
    "        fig, axes = plt.subplots(len(parameter_indices), sharex=True, figsize=figsize, dpi=80, frameon = False)\n",
    "        \n",
    "        if len(parameter_indices) == 1:\n",
    "               axes = [axes]\n",
    "        \n",
    "        labels = [k[0] for k in self.loci_stats_sorted]\n",
    "        \n",
    "        j = 0\n",
    "        \n",
    "        for ax in axes:\n",
    "            \n",
    "            values = [k[parameter_indices[j]] for k in self.loci_stats_sorted]\n",
    "            \n",
    "            bp = ax.boxplot(values,0,'', positions = range(4,(len(values)*4)+1, 4))\n",
    "            ax.set_ylabel(ytitles[parameter_indices[j]], fontsize=lable_fsize)\n",
    "            #plt.xlabel(\"Locus\", fontsize=lable_fsize)\n",
    "            plt.xticks(range(4,((len(values)+1)*4),4), labels, rotation=90, fontsize = xtick_fsize)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            j += 1\n",
    "            \n",
    "            remove_border()\n",
    "            for box in bp['boxes']:\n",
    "            # change outline color\n",
    "                box.set( color='salmon', linewidth=1)\n",
    "                    \n",
    "            # change color, linestyle and linewidth of the whiskers\n",
    "            for whisker in bp['whiskers']:\n",
    "                whisker.set(color='lightgray', linestyle='solid', linewidth=2.0)\n",
    "            \n",
    "            # change color and linewidth of the caps\n",
    "            for cap in bp['caps']:\n",
    "                cap.set(color='gray', linewidth=2.0)\n",
    "        \n",
    "            # change color and linewidth of the medians\n",
    "            for median in bp['medians']:\n",
    "                median.set(color='white', linewidth=2)\n",
    "            \n",
    "        fig.savefig(filename)\n",
    "        \n",
    "    def slice_loci(self, median_range, otu_meta, parameter='entropy',\n",
    "                   otu_must_have_all_of=[], otu_must_have_one_of='any'):\n",
    "        \n",
    "        j = None\n",
    "        if parameter == 'entropy':\n",
    "            j = 3\n",
    "        elif parameter == 'gapscore':\n",
    "            j = 4\n",
    "        elif parameter == 'conservation':\n",
    "            j = 5\n",
    "        elif parameter == 'sequence_length':\n",
    "            j = 1\n",
    "        elif parameter == 'sequence_gc':\n",
    "            j = 2\n",
    "        \n",
    "        loci_names = [i[0] for i in self.loci_stats_sorted if median_range[0] < np.median(i[j]) < median_range[1]]\n",
    "        loci = [l for l in self.loci if l.name in loci_names]\n",
    "        concat_name = \"%s_%.2f_%.2f_loci_%s_to_%s\"%(parameter, float(median_range[0]), float(median_range[1]),\n",
    "                                                    loci_names[0],loci_names[-1])\n",
    "        \n",
    "        return Concatenation(concat_name, loci,\n",
    "                             otu_meta,\n",
    "                             otu_must_have_all_of=otu_must_have_all_of,\n",
    "                             otu_must_have_one_of=otu_must_have_one_of)\n",
    "    \n",
    "\n",
    "    \n",
    "    def slide_loci(self, otu_meta, median_range='all', parameter='entropy', start=0, length=2, step=1,\n",
    "                   otu_must_have_all_of=[],\n",
    "                   otu_must_have_one_of='any'):\n",
    "        \n",
    "        j = None\n",
    "        if parameter == 'entropy':\n",
    "            j = 3\n",
    "        elif parameter == 'gapscore':\n",
    "            j = 4\n",
    "        elif parameter == 'conservation':\n",
    "            j = 5\n",
    "        elif parameter == 'sequence_length':\n",
    "            j = 1\n",
    "        elif parameter == 'sequence_gc':\n",
    "            j = 2\n",
    "        \n",
    "        if median_range == 'all':\n",
    "            medians = [np.median(i) for i in [k[j] for k in self.loci_stats_sorted]]\n",
    "            median_range = [min(medians), max(medians)]\n",
    "            \n",
    "        loci_in_range = [[i[0], np.median(i[j])] for i in self.loci_stats_sorted \n",
    "                          if median_range[0] <= np.median(i[j]) <= median_range[1]]\n",
    "        \n",
    "        concatenations = []\n",
    "        \n",
    "        stop = False\n",
    "        \n",
    "        while not stop:\n",
    "            \n",
    "            window_loci = loci_in_range[start: start+length]\n",
    "                \n",
    "            window_loci_names = [n[0] for n in window_loci]\n",
    "            loci = [l for l in self.loci if l.name in window_loci_names]\n",
    "            window_start_median = window_loci[0][1]\n",
    "            window_end_median = window_loci[-1][1]\n",
    "            concat_name = \"%s_%.2f_%.2f_loci_%i_to_%i\"%(parameter, float(window_start_median), float(window_end_median),\n",
    "                                                        start, start+length-1)\n",
    "            print concat_name\n",
    "            concatenations.append(Concatenation(concat_name, loci,\n",
    "                                                otu_meta,\n",
    "                                                otu_must_have_all_of=otu_must_have_all_of,\n",
    "                                                otu_must_have_one_of=otu_must_have_one_of))\n",
    "            start = start+step\n",
    "            \n",
    "            if len(loci_in_range[start:]) < length:\n",
    "                stop = True\n",
    "        \n",
    "        return concatenations\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python reprophylo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cp reprophylo.py /home/amir/work/galaxy-dist_old/tools/reprophylo/.\n",
    "!cp reprophylo.py /home/amir/Dropbox/python_modules/.\n",
    "!cp reprophylo.py /home/amir/Dropbox/Docker/ReproPhylo/.\n",
    "!cp reprophylo.py /home/amir/Dropbox/Docker/ReproPhyloGalaxy/galaxy-dist/tools/reprophylo/.\n",
    "!cp reprophylo.py /home/amir/galaxy-dist/tools/reprophylo/.\n",
    "!sed -i \"s/programspath \\= \\\"\\\"/programspath \\= \\\"\\%s\\/galaxy-dist\\/tools\\/reprophylo\\/programs\\/\\\"\\%os.environ\\[\\'HOME\\'\\]/\" /home/amir/Dropbox/Docker/ReproPhyloGalaxy/galaxy-dist/tools/reprophylo/reprophylo.py \n",
    "!sed -i \"s/programspath \\= \\\"\\\"/programspath \\= \\\"\\%s\\/galaxy-dist\\/tools\\/reprophylo\\/programs\\/\\\"\\%os.environ\\[\\'HOME\\'\\]/\" /home/amir/work/galaxy-dist_old/tools/reprophylo/reprophylo.py\n",
    "!sed -i \"s/programspath \\= \\\"\\\"/programspath \\= \\\"\\%s\\/galaxy-dist\\/tools\\/reprophylo\\/programs\\/\\\"\\%os.environ\\[\\'HOME\\'\\]/\" /home/amir/galaxy-dist/tools/reprophylo/reprophylo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file gb_syn.py\n",
    "\n",
    "def gb_syn():\n",
    "    return  [\n",
    "            ['MT-CO1','cox1','COX1','COI','coi','cytochome oxidase 1','cytochome oxidase I',\n",
    "             'cytochome oxidase subunit 1','cytochome oxidase subunit I',\n",
    "             'cox I','coxI', 'cytochrome c oxidase subunit I'],\n",
    "            ['MT-CO2','cox2','COX2','COII','coii','cytochome oxidase 2','cytochome oxidase II',\n",
    "             'cytochome oxidase subunit 2','cytochome oxidase subunit II','cox II',\n",
    "             'cytochrome c oxidase subunit II'],\n",
    "            ['MT-CO3','cox3','COX3','COIII','coiii','cytochome oxidase 3',\n",
    "             'cytochome oxidase III','cytochome oxidase subunit 3','cytochome oxidase subunit III'\n",
    "             ,'cox III','cytochrome c oxidase subunit III'],\n",
    "            ['18s','18S','SSU rRNA','18S ribosomal RNA','small subunit 18S ribosomal RNA', '18S rRNA'],\n",
    "            ['28s','28S','LSU rRNA','28S ribosomal RNA','28S large subunit ribosomal RNA'],\n",
    "            ['MT-ATP6','atp6','ATP6','atpase6','atpase 6','ATPase6','ATPASE6'],\n",
    "            ['MT-ATP8','atp8','ATP8','atpase8','atpase 8','ATPase8','ATPASE8'],\n",
    "            ['ATP9','atp9'],\n",
    "            ['MT-CYB','cytochrome-b','cytb','CYTB','Cytb', 'cytochrome B','cytochrome b'],\n",
    "            ['ef1a','Ef1a', 'elongation factor 1-alpha','elongation factor-1 alpha',\n",
    "             'ef1-alpha','elongation factor 1 alpha'],\n",
    "            ['MT-ND1','nd1','ND1','nadh1','NADH1','nadh 1','NADH 1',\n",
    "             'NADH dehydrogenase subunit 1','NADH dehydrogenase subunit I','nad1','nadh1'],\n",
    "            ['MT-ND2','nd2','ND2','nadh2','NADH2','nadh 2','NADH 2','NADH dehydrogenase subunit 2', \n",
    "             'NADH dehydrogenase subunit II','nad2','nadh2'],\n",
    "            ['MT-ND3','nd3','ND3','nadh3','NADH3','nadh 3','NADH 3','NADH dehydrogenase subunit 3',\n",
    "             'NADH dehydrogenase subunit III','nad3','nadh3'],\n",
    "            ['MT-ND4','nd4','ND4','nadh4','NADH4','nadh 4','NADH 4','NADH dehydrogenase subunit 4',\n",
    "             'NADH dehydrogenase subunit IV','nad4','nadh4'],\n",
    "            ['MT-ND4L','nd4l','ND4L','nad4l','nadh4l', 'nadh4L', 'NADH dehydrogenase subunit 4l',\n",
    "             'NADH dehydrogenase subunit 4L'],\n",
    "            ['MT-ND5','nd5','ND5','nad5', 'nadh5','NADH5','nadh 5','NADH 5','NADH dehydrogenase subunit 5',\n",
    "             'NADH dehydrogenase subunit V'],\n",
    "            ['MT-ND6','nd6','ND6','nad6','nadh6','NADH6','nadh 6','NADH 6','NADH dehydrogenase subunit 6',\n",
    "             'NADH dehydrogenase subunit VI'],\n",
    "            ['rrnS','12srRNA','12s rRNA','12S ribosomal RNA','12S rRNA'],\n",
    "            ['rrnL','16srRNA','16s rRNA','16S ribosomal RNA','16S rRNA'],\n",
    "            ['C_mos', 'C-mos','c-mos','C-MOS'],\n",
    "            ['GAPDH','gapdh'],\n",
    "            ['RNApol2','RNApolII','RNA polymerase II']\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing USER\n"
     ]
    }
   ],
   "source": [
    "%%file USER\n",
    "name=Amir Szitenberg\n",
    "experiment=debug reprophylo\n",
    "sugar=one\n",
    "one more thing=I forget, never mind\n",
    "email=A.Szitenberg@hull.ac.uk\n",
    "second email=szitenberg@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reprophylo.py:610: UserWarning: Version control off\r\n",
      "  warnings.warn('Version control off')\r\n"
     ]
    }
   ],
   "source": [
    "!python reprophylo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def random_rgb():\n",
    "    import random\n",
    "    r = lambda: random.randint(0,255)\n",
    "    return ('#%02X%02X%02X' % (r(),r(),r()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#39156D\n"
     ]
    }
   ],
   "source": [
    "print random_rgb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree reconstruction tools in reprophylo\n",
    "I think that a tool that will simplify the RAxML command line while providing all the possible options will be more appreciated as a starting point than a small range of programs for which you have to write the command line from scratch. In addition, I think that instead of TreeReconstructionMethod object, as I did for alignments, I should make program specific object, ie, RaxmlMethodObject. The reason for this is that the user would usually want to fit the best alignment approach for each gene, but will want to use a uniform tree reconstruction method ( or several of them, but on all the datasets for each method).  \n",
    "  \n",
    "I plan to simplify the raxml command line by excluding keywards that are not related to tree reconstruction because other operations do not make sense, coming after sequence alignment in the reprophylo workflow (everything will be accesible, but not everything will be bundled into predesigned analyses)  \n",
    "The bundles will be either 'tree reconstruction' bundles, 'node support' bundels and 'both' bundles. It will be possible to add optional keywords to the bundes. In the description below I don't show all the keywards that will be included. e.g., -m (model) is always required and I don't write it here.\n",
    "\n",
    "\n",
    "\n",
    "# RAxML keyword blocks\n",
    "  \n",
    "## ML search combined with branch support\n",
    "*This section include one liners or a two liners that would be meaningless when ran individually*  \n",
    "\n",
    "#### Single best tree search With rappid bootstrap\n",
    "-f a -p 12345 -x 12345 -N (number or AUTO_MRE)  \n",
    "#### Multipe best tree searches With REL bootstrap\n",
    "-f D -p 12345 -N (number)  \n",
    "**then**  \n",
    "-f b -t RAxML_bestTree.name -z RAxML_rellBootstrap.name -m (model)  \n",
    "#### Fast tree with sh-like support and branch length\n",
    "-f F -p 12345  \n",
    "**then**  \n",
    "-f J -p 12345 -t RAxML_fastTree  \n",
    "*comment: if not followed by bootstrap analysis, do  use_sh_support_as_branch_support*\n",
    "#### Very fast tree with sh-like support and branch length\n",
    "-f E -p 12345  \n",
    "**then**  \n",
    "-f J -p 12345 -t RAxML_fastTree  \n",
    "*comment: if not followed by bootstrap analysis, do  use_sh_support_as_branch_support*  \n",
    "\n",
    "## Best tree search approaches\n",
    "*In liners that focus on thorogh best tree search, possibly with more than on randomized parsimony/ completely random starting tree*  \n",
    "  \n",
    "#### Best tree default method approach\n",
    "-f d -p 12345 -N (number)  \n",
    "#### Slow best tree search with better likelihood\n",
    "-f o -p 12345 -N (number)  \n",
    "  \n",
    "## Bipartition tree calculation approaches (calc supports and put on best tree)\n",
    "*Methods which focus on branch support calculation and their superimposition on a precalculated best tree or on an nni optimized fast tree with branch-lengths*  \n",
    "  \n",
    "#### Rapid bootstrap\n",
    "-x 12345 -N (number/autoMRE)  \n",
    "**then**  \n",
    "-f b -t RAxML_bestTree/RAxML_fastTreeSH_Support -z RAxML_Bootstrap -m (model)  \n",
    "####  Thorough bootstrap\n",
    "-b 12345 -N (number/autoMRE)  \n",
    "**then**  \n",
    "-f b -t RAxML_bestTree/RAxML_fastTreeSH_Support -z RAxML_Bootstrap -m (model)  \n",
    "  \n",
    "####  IC/TC approach\n",
    "*This approach is based on the level of incongruence between the concatenated tree and the gene trees. I tries replace bootstrap support or posterior probabilities which are inefficient when a lot of data is involved. \n",
    "Salichos and Rokas 2013 http://www.ncbi.nlm.nih.gov/pubmed/23657258  \n",
    "and http://mbe.oxfordjournals.org/content/early/2014/02/07/molbev.msu061.abstract*  \n",
    "  \n",
    "*Technically, from the reprophylo point of view, it differs from other approaches by the fact that it integrates all the gene trees and a concatenated tree, while other approches are performed indipendantly on each alignment. So it needs to pull all the trees calculated at one point and placed in the reprophylo project and intgrate them.*\n",
    "\n",
    "*The analysis is done by calling **-f i** and providing the concat tree to **-t** and the gene trees to **-z**. This will produce the concat tree with the IC and ICA scores as follows **(A,B):0.1325[IC,ICA]**. The scores will then need to be reformated as node features as I have done for the sh-like supports. There are many modifiers to this approach which I have to check carefully in the RAxML manual*  \n",
    "  \n",
    "## Parameter optimization appraches\n",
    "#### optimize br-len and other model parameters\n",
    "-f e -t (RAxML_fastTreeSH_Support or RAxML_fastTree or RAxML_bestTree)  \n",
    "*Comment: This will requlire the use of transfer_support_same_topo on the input and output trees*  \n",
    "  \n",
    "# General analysis modifiers\n",
    "*I excluded keywards that don't make sense in the tree reconstruction phase of reprophylo*  \n",
    "  \n",
    "A-scondary structure model, together with S  \n",
    "B-specify threshold for autoMR  \n",
    "c-specifiy num of CAT categories  \n",
    "d-start at random instead of parsimony randomized tree  \n",
    "D-ML search convergence criterion to stop the analysis (ever needed in raxml?)  \n",
    "e-parameter percision criterion  \n",
    "fI-ML based descision on midpoint for a tree passed with -t    \n",
    "ft-another way to search for a tree, not so sure  \n",
    "fT-optimization via through SPR moves  \n",
    "F-Just CAT, no GAMMA at all  \n",
    "g-constraint tree  \n",
    "H-No so sure  \n",
    "m-models, will need to be read from a control file   \n",
    "M-per partition branch length  \n",
    "n-derived from method.id  \n",
    "O-disable check of undetermined alignments  \n",
    "p-random seed for parsimony. set automatically  \n",
    "P-Used provided AA substitution model  \n",
    "q-partition file, created automatically  \n",
    "r-constraint tree, binary  \n",
    "s-input alignment  \n",
    "S-secondary structue  \n",
    "t-tree file  \n",
    "T-num of cores  \n",
    "u-use median instead of mean for GAMMA based rates  \n",
    "z-pass trees file  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
