{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext watermark"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%watermark -g -h -m -v -p ete2,biopython,cogent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPython 2.7.5+\n",
        "IPython 1.2.1\n",
        "\n",
        "ete2 2.2rev1056\n",
        "biopython 1.64\n",
        "cogent 1.5.3\n",
        "\n",
        "compiler   : GCC 4.8.1\n",
        "system     : Linux\n",
        "release    : 3.11.0-24-generic\n",
        "machine    : x86_64\n",
        "processor  : x86_64\n",
        "CPU cores  : 8\n",
        "interpreter: 64bit\n",
        "host name  : amir-TECRA-W50-A\n",
        "Git hash   : 2b8145d8d7450b1eb3cb5c538b18727ec9e3f0aa\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tetillidae Data\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from reprophylo import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Create database with Tetillidae sequences from genbank and a novel tetillid sequence"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Declare the loci to use Locus(char_type, feature_type, name,[alias1, alias2,..,aliasn])\n",
      "\n",
      "ssu = Locus('dna','rRNA','18S',\n",
      "            ['18s','18S','SSU rRNA','18S ribosomal RNA'])\n",
      "lsu = Locus('dna','rRNA','28S',\n",
      "            ['28s','28S','LSU rRNA','28S ribosomal RNA','28S large subunit ribosomal RNA'])\n",
      "coi = Locus('dna','CDS','coi',\n",
      "            ['cox1','COX1','coi','COI','CoI'])\n",
      "\n",
      "\n",
      "# Put the loci in a list\n",
      "\n",
      "loci = [coi, lsu, ssu]\n",
      "\n",
      "# Start a reprophylo database instance using the loci list\n",
      "\n",
      "db = Database(loci)\n",
      "\n",
      "# Read Genbank formated input into the database\n",
      "\n",
      "data_folder = '/home/amir/Dropbox/ReproPhylo/data/'\n",
      "input_filenames_list = [data_folder + 'Tetillidae.gb']\n",
      "db.read_embl_genbank(input_filenames_list)\n",
      "\n",
      "# Read denovo sequences\n",
      "\n",
      "denovo_filnames_list = [data_folder+'Tetillidae_denovo_sequence.fasta']\n",
      "db.read_denovo(denovo_filnames_list, 'dna')\n",
      "\n",
      "# Add a cds feature to the new sequence, it will be translated.\n",
      "\n",
      "db.add_feature_to_record('denovo0', 'CDS', location=[[1,786,1],[1742,2092,1]], \n",
      "                         qualifiers={'gene': 'cox1', 'product': 'cytochrome c oxidase subunit I',\n",
      "                                     'codon_start': 1,'transl_table': 4, 'organism': 'Craniella microsigma'})\n",
      "\n",
      "# Export the database\n",
      "\n",
      "db.write('Tet_Repro.gb', format = 'genbank')\n",
      "\n",
      "# Veiw the output GenBank file to see that the genbank entries contain only the specified loci.\n",
      "# other loci have been deleted\n",
      "# Check your novel sequence and the new cds feature you specified. A translation was included automatically\n",
      "\n",
      "db.write('Tet_Repro.csv', format = 'csv')\n",
      "\n",
      "# Chek the csv file to see which qualifiers need to be ammended or added"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### example reduced entry from genbank\n",
      "\n",
      "<pre>\n",
      "LOCUS       NC_010198              18089 bp    DNA              INV 10-JUL-2009\n",
      "DEFINITION  Cinachyrella kuekenthali mitochondrion, <strong>complete genome</strong>.\n",
      "ACCESSION   NC_010198\n",
      "VERSION     NC_010198.1  GI:164421113\n",
      "DBLINK      Project:28221 BioProject:PRJNA28221\n",
      "KEYWORDS    RefSeq.\n",
      "SOURCE      mitochondrion Cinachyrella kuekenthali (sponge)\n",
      "  ORGANISM  Cinachyrella kuekenthali\n",
      "            Eukaryota; Metazoa; Porifera; Demospongiae; Tetractinomorpha;\n",
      "            Spirophorida; Tetillidae; Cinachyrella.\n",
      "[..Some annotation lines deleted for presentation...]\n",
      "FEATURES             Location/Qualifiers\n",
      "     source          1..18089\n",
      "                     <strong>/feature_id=\"NC_010198.1_source\"</strong>\n",
      "                     /mol_type=\"genomic DNA\"\n",
      "                     /organelle=\"mitochondrion\"\n",
      "                     /db_xref=\"taxon:458489\"\n",
      "                     /common=\"sponge\"\n",
      "                     /organism=\"Cinachyrella kuekenthali\"\n",
      "     CDS             10028..11596\n",
      "                     /product=\"cytochrome c oxidase subunit I\"\n",
      "                     /codon_start=1\n",
      "                     /transl_table=4\n",
      "                     <strong>/feature_id=\"NC_010198.1_f0\"</strong>\n",
      "                     /db_xref=\"GI:164421124\"\n",
      "                     /db_xref=\"GeneID:5846723\"\n",
      "                     /translation=\"MDRLYLTRWLYSTNHKDIGTLYLLFGVFSGMIGSGFSLLIRLELS\n",
      "                     APGSMLGDDQLYNVMVTAHGLIMVFFLVMPVMIGGFGNWLVPLYIGAPDMAFPRLNNIS\n",
      "                     FWVLPPSAILLLGSAFVEQGVGTGWTLYPPLSSIQAHSGGSVDAAIFSIHLAGISSILG\n",
      "                     SMNFITTIFNMRAPGITMDRLPLFVWSILVTTYLLLLALPVLAGAITMLLTDRNFNTTF\n",
      "                     FDPAGGGDPILFQHLFWFFGHPEVYVLILPGFGIISQIIPTFAAKKQIFGYLGMVYAMV\n",
      "                     SIGILGFIVWAHHMFTVGMDADSRAYFSAATMIIAVPTGIKIFSWIATIVGGSLRTDTP\n",
      "                     MLWAMGFVFLFTVGGLTGIVVACSSLDILLHDTYYVVAHFHYVLSMGAIFAIFGGVYYW\n",
      "                     FGKITGYCYNEVLGKIHFWLMFIGVNLTFFPQHFLGLAGLPRRYSDYHDSFAGWNQISS\n",
      "                     LGSFISIVSVMVFLYLVYDAYVWEIKFVGWTKDSGHYPSLEWSQTSPPAHHTYNELPFV\n",
      "                     YKGSS\"\n",
      "                     /gene=\"COX1\"\n",
      "                     /protein_id=\"YP_001648419.1\"\n",
      "ORIGIN\n",
      "        1 gatttattta ttatatttat agactagtat aaacgggtag tgaatggata actaggatgt\n",
      "       61 tatataagga cgttattaat acgaaaattg ccaatgagaa agatgaaggg caaagttcct\n",
      "      121 aaaaaagaaa ataataaacc ctaggaattg aaacatctca gtactaggag gaaaagaaat\n",
      "      181 caagtgagat tccgttagta gtggcgagcg aaagcggagg tagaattgca aaataatagt\n",
      "      241 ctctgatata gaatatggag tagaataatt tgaagatagg tgtaccacgc atctaagtct\n",
      "      301 aaatatatat aacgtaccga tagtggacta gtactgtgaa ggaaagttga aagagataaa\n",
      "      361 aatgaaatag aacttgaaat tcgctatcta taagcagacg aaatcgtcgt accttttgta\n",
      "      [..Some sequence lines deleted for presentation...]\n",
      "      \n",
      "</pre>\n",
      "\n",
      "### Example denovo entry \n",
      "<pre>\n",
      "LOCUS       NIWA2850                2092 bp    DNA              UNK 01-JAN-1980\n",
      "DEFINITION  NIWA2850 Craniella microsigma cox1\n",
      "ACCESSION   denovo0\n",
      "VERSION     <strong>denovo0</strong>\n",
      "KEYWORDS    .\n",
      "SOURCE      .\n",
      "  ORGANISM  .\n",
      "            .\n",
      "FEATURES             Location/Qualifiers\n",
      "     source          1..2092\n",
      "                     <strong>/feature_id=\"denovo0_source\"</strong>\n",
      "                     /original_id=\"NIWA2850\"\n",
      "                     /original_desc=\"Craniella microsigma cox1\"\n",
      "     CDS             <strong>join(1..786,1742..2092)</strong>\n",
      "                     /product=\"cytochrome c oxidase subunit I\"\n",
      "                     /codon_start=1\n",
      "                     /transl_table=4\n",
      "                     <strong>/feature_id=\"denovo0_f0\"\n",
      "                     /translation=\"MIGTGFSLLIRLELSAPGLMLGDDHLYNVMVTAHGLIMVFFLVMP\n",
      "                     VMIGGFGNWMVPLYIGAPDMAFPRLNNISFWVLPPSLILLLGSAFVEQGVGTGWTLYPP\n",
      "                     LSSIQAHSGGSVDAAIFSLHLAGISSILGAMNFITTIFNMRAPGITMDRLPLFVWSILI\n",
      "                     TTYLLLLALPVLAGAITMLLTDRNFNTTFFDPAGGGDPILFQHLFWFFGHPEVYVLVLP\n",
      "                     GFGIVSQIIPTFAAKKQIFGYLGMVYAMVSIGILGFIVWAHHMFTVGMDADSRAYFSAA\n",
      "                     TMIIAVPTGIKIFSWIATVVGGSLRIDTPMLWAMGFVFLFTVGGLTGIVVASNSLDVLL\n",
      "                     HDTYYVVAHFHYVLSMGAIFAIFGGVYYWFGKITGYCYN\"</strong>\n",
      "                     /gene=\"cox1\"\n",
      "                     /organism=\"Craniella microsigma\"\n",
      "ORIGIN\n",
      "        1 atgataggaa ctggatttag cttgcttatt agattagaac tatccgctcc cggattaatg\n",
      "       61 ttgggtgacg accatttata caatgttatg gtcacggccc acggtcttat aatggtcttt\n",
      "      121 ttcttagtta tgccggttat gataggtggg ttcggtaatt gaatggttcc cctttacatc\n",
      "      181 ggggcaccgg atatggcttt tccaagatta aacaatatta gtttttgagt tttacccccc\n",
      "      [..Some sequence lines deleted for presentation...]\n",
      "</pre>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Edit the qualifiers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Copy the 'organism' source qualifier into each of the features so that it can be manipulated\n",
      "# it will be used to name the tree leaves\n",
      "\n",
      "db.add_qualifier_from_source('organism')\n",
      "\n",
      "# Create a 'genus' qualifier for each of the feature and take the value from the 'organism' qualifier\n",
      "# it will be used to determine clades' background colours\n",
      "\n",
      "for genus in ['Cinachyrella','Cinachyra','Amphitethya','Paratetilla','Acanthotetilla',\n",
      "              'Fangophilina', 'Tetilla','Craniella']:\n",
      "    db.if_this_then_that(genus, 'organism', genus, 'genus', mode='part')\n",
      "\n",
      "for genus in ['Geodia','Pachymatisma','Calthropella','Thenea','Theonella']:\n",
      "    db.if_this_then_that(genus, 'organism', 'Astrophorid', 'genus', mode='part')    \n",
      "\n",
      "# Put a value in the 'gene' qualifier for features that \n",
      "# don't have a 'gene' qualifier based on the 'product' qualifier.\n",
      "# Make sure to include these values in the loci aliases\n",
      "\n",
      "fix_gene_qualifier = [['18S ribosomal RNA', 'product', 'SSU rRNA', 'gene'],\n",
      "                      ['28S ribosomal RNA', 'product', 'LSU rRNA', 'gene'],\n",
      "                      ['AM076987.1_f1','feature_id','LAGLIDADG','gene']]\n",
      "for fix in fix_gene_qualifier:\n",
      "    db.if_this_then_that(fix[0], fix[1], fix[2], fix[3])\n",
      "\n",
      "\n",
      "# Copy the 'specimen_voucher' qualifier from the source feature\n",
      "# to each of the features so that it can be manipulated\n",
      "# It will be used as OTU indicator (i.e., to indicate different loci that belong to the same\n",
      "# leaf of the tree and should be concatenated\n",
      "\n",
      "db.add_qualifier_from_source('specimen_voucher')\n",
      "\n",
      "# Add missing/ correct wrong specimen vouchers according to feature id\n",
      "\n",
      "add_specimen_voucher = [[['JX177968.1_f0'],'specimen_voucher','QMG_321405'],\n",
      "                        [['JX177913.1_f0',\n",
      "                          'JX177935.1_f0',\n",
      "                          'JX177965.1_f0'],'specimen_voucher','TAU_25617'],\n",
      "                        [['JX177903.1_f0',\n",
      "                          'JX177938.1_f0'],'specimen_voucher','TAU_25618'],\n",
      "                        [['HM032740.1_f0',\n",
      "                          'JX177964.1_f0'],'specimen_voucher','TAU_25621'],\n",
      "                        [['HM032739.1_f0',\n",
      "                          'JX177962.1_f0'],'specimen_voucher','TAU_25622'],\n",
      "                        [['JX177968.1_f0'],'specimen_voucher','QMG_321405'],\n",
      "                        [['JX177891.1_f0'],'specimen_voucher','RMNH_POR_3100'],\n",
      "                        [['JX177900.1_f0',\n",
      "                          'JX177926.1_f0'],'specimen_voucher','TAU_25620'],\n",
      "                        [['JX177901.1_f0',\n",
      "                          'JX177961.1_f0',\n",
      "                          'JX177956.1_f0'],'specimen_voucher','TAU_25619'],\n",
      "                        [['HM032742.1_f0',\n",
      "                          'JX177957.1_f0'],'specimen_voucher','MNRJ_576']]\n",
      "for add in add_specimen_voucher:\n",
      "    db.add_qualifier(add[0],add[1],add[2])\n",
      "\n",
      "# Reformat specimen voucher according to the specimen voucher\n",
      "\n",
      "correct_specimen_voucher = [['QMG321405','specimen_voucher','QMG_321405','specimen_voucher'],\n",
      "                            ['MHNM 16194','specimen_voucher','MHNM_16194','specimen_voucher'],\n",
      "                            ['TAU 25456','specimen_voucher','TAU_25456','specimen_voucher'],\n",
      "                            ['QMG320636','specimen_voucher','QMG_320636','specimen_voucher'],\n",
      "                            ['QMG320270','specimen_voucher','QMG_320270','specimen_voucher'],\n",
      "                            ['ZMBN:85239','specimen_voucher','ZMBN_85239','specimen_voucher'],\n",
      "                            ['QMG318785','specimen_voucher','QMG_318785','specimen_voucher'],\n",
      "                            ['QMG316342','specimen_voucher','QMG_316342','specimen_voucher'],\n",
      "                            ['QMG314224','specimen_voucher','QMG_314224','specimen_voucher'],\n",
      "                            ['VM14754','specimen_voucher','VM_14754','specimen_voucher'],\n",
      "                            ['ZMBN:85240','specimen_voucher','ZMBN_85240','specimen_voucher'],\n",
      "                            ['ZMBN:81789','specimen_voucher','ZMBN_81789','specimen_voucher'],\n",
      "                            ['ZMBN:81787','specimen_voucher','ZMBN_81787','specimen_voucher'],\n",
      "                            ['ZMBN:81785','specimen_voucher','ZMBN_81785','specimen_voucher']]\n",
      "for correction in correct_specimen_voucher:\n",
      "    db.if_this_then_that(correction[0],correction[1],correction[2],correction[3])\n",
      "\n",
      "# Make a qualifier that will be used to concatenate OTU sequences\n",
      "\n",
      "db.copy_paste_within_feature('specimen_voucher','OTU_dict')\n",
      "\n",
      "# Add missing values to the OTU dictionary\n",
      "\n",
      "add_to_concatenation_dict=[[['AY737635.1_f0',\n",
      "                             'AY320032.1_f0'],'OTU_dict','Geodia_neptuni'],\n",
      "                           [['EF564339.1_f0',\n",
      "                             'HM592832.1_f0'],'OTU_dict','Pachymatisma_johnstonia'],\n",
      "                           [['HM592717.1_f0',\n",
      "                             'HM592765.1_f0'],'OTU_dict','Thenea_levis'],\n",
      "                           [['HM592745.1_f0',\n",
      "                             'HM592820.1_f0'],'OTU_dict','Theonella_swinhoei'],\n",
      "                           [['KC762708.1_f0',\n",
      "                             'NC_010198.1_f0'],'OTU_dict','Cinachyrella_kuekenthali'],\n",
      "                           [['HM592705.1_f0',\n",
      "                             'HM592826.1_f0'],'OTU_dict','Calthropella_geodioides']]\n",
      "\n",
      "for add in add_to_concatenation_dict:\n",
      "    db.add_qualifier(add[0],add[1],add[2])\n",
      "\n",
      "# add morphological info\n",
      "\n",
      "have_porocalices = ['Cinachyrella','Cinachyra','Paratetilla','Fangophilina','Acanthotetilla','Amphitethya']\n",
      "no_porocalices = ['Tetilla','Craniella']\n",
      "have_cortex = ['Craniella','Cinachyra','Fangophilina']\n",
      "lack_cortex = ['Cinachyrella','Tetilla', 'Paratetilla','Acanthotetilla','Amphitethya']\n",
      "have_calthrops = ['Paratetilla']\n",
      "lack_calthrops = ['Cinachyrella','Cinachyra','Fangophilina','Acanthotetilla','Amphitethya','Tetilla','Craniella']\n",
      "\n",
      "for g in have_porocalices:\n",
      "    db.if_this_then_that(g,'genus',2,'porocalices')\n",
      "for g in no_porocalices:\n",
      "    db.if_this_then_that(g,'genus',1,'porocalices')\n",
      "    \n",
      "for g in have_cortex:\n",
      "    db.if_this_then_that(g,'genus',2,'cortex')\n",
      "for g in lack_cortex:\n",
      "    db.if_this_then_that(g,'genus',1,'cortex')\n",
      "    \n",
      "for g in have_calthrops:\n",
      "    db.if_this_then_that(g,'genus',2,'calthrops')\n",
      "for g in lack_calthrops:\n",
      "    db.if_this_then_that(g,'genus',1,'calthrops')\n",
      "\n",
      "#if not g in Tetillidae:\n",
      "#    db.if_this_then_that(g,'genus',0,'porocalices')\n",
      "#    db.if_this_then_that(g,'genus',0,'cortex')\n",
      "#    db.if_this_then_that(g,'genus',0,'calthrops')\n",
      "# Export the database\n",
      "\n",
      "db.write('Tet_Repro.csv', format = 'csv')\n",
      "db.write('Tet_Repro.gb', format = 'genbank')\n",
      "\n",
      "# Check that your changes have been recorded"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example of entry with edited qualifiers in bold\n",
      "\n",
      "<pre>\n",
      "LOCUS       JX177987                1108 bp    DNA              INV 05-SEP-2013\n",
      "DEFINITION  Craniella cranium voucher ZMBN_85239 18S ribosomal RNA gene, partial\n",
      "            sequence.\n",
      "ACCESSION   JX177987\n",
      "VERSION     JX177987.1  GI:392932935\n",
      "KEYWORDS    .\n",
      "SOURCE      Craniella cranium\n",
      "  ORGANISM  Craniella cranium\n",
      "            Eukaryota; Metazoa; Porifera; Demospongiae; Tetractinomorpha;\n",
      "            Spirophorida; Tetillidae; Craniella.\n",
      "[..Some annotation lines deleted for presentation...]\n",
      "\n",
      "FEATURES             Location/Qualifiers\n",
      "     source          1..1108\n",
      "                     /identified_by=\"Paco Cardenas & Hans Tore Rap\"\n",
      "                     /feature_id=\"JX177987.1_source\"\n",
      "                     /mol_type=\"genomic DNA\"\n",
      "                     /country=\"Norway\"\n",
      "                     /note=\"authority: Craniella cranium (Mueller, 1776)\"\n",
      "                     /db_xref=\"taxon:1006792\"\n",
      "                     /specimen_voucher=\"ZMBN_85239\"\n",
      "                     /organism=\"Craniella cranium\"\n",
      "     rRNA            <1..>1108\n",
      "                     /product=\"18S ribosomal RNA\"\n",
      "                     /gene=\"SSU rRNA\"\n",
      "                     <strong>/specimen_voucher=\"ZMBN_85239\"</strong>\n",
      "                     <strong>/OTU_dict=\"ZMBN_85239\"</strong>\n",
      "                     /feature_id=\"JX177987.1_f0\"\n",
      "                     <strong>/genus=\"Craniella\"</strong>\n",
      "                     <strong>/organism=\"Craniella cranium\"</strong>\n",
      "ORIGIN\n",
      "        1 tcatatgctt gtctcaaaga ctaagccatg catgtccaag tatgaacgct tcgtactgtg\n",
      "       61 aaactgcgaa tggctcatta aatcagttat agtttatttg atggtcgttt tgctacatgg\n",
      "      121 ataactgtgg taattctaga gctaatacat gcacaaggtc ccgactctcg gaagggacgt\n",
      "      181 atttattaga tccaaaacca gcgcgggtgg cctctcgggg tgcccggtac ctgggcgatt\n",
      "      [..Some sequence lines deleted for presentation...]\n",
      "//\n",
      "</pre>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Define how to concatenate the loci"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Concatenation objects are used to tell reprophylo how to concatenate the loci\n",
      "# They provide tools to set rules regarding which loci/OTUs to include\n",
      "\n",
      "combined = Concatenation(name='combined', loci=db.loci, otu_meta='OTU_dict',\n",
      "               concat_must_have_all_of=['coi'], concat_must_have_one_of =[['18S','28S']])\n",
      "\n",
      "# could be e.g. concat_must_have_one_of =[['18S','28S'],['alg11','nd5']] to except an out with 18S and nd5\n",
      "# but without 28S and alg11"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Brake down the records by locus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# form sequence records for each locus\n",
      "\n",
      "db.extract_by_locus()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Define how to align each locus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rRNA_set = ['18S','28S']\n",
      "\n",
      "# coi will be aligned with mafft L-ins-i, which is the default. By default, will do codon alignment to CDSs\n",
      "\n",
      "linsi = AlignmentMethod(db, loci=['coi'])\n",
      "\n",
      "# 18s and 28s, put together in the rRNA_set list, will be aligned with muscle with default parameters\n",
      "\n",
      "muscle = AlignmentMethod(db, method_name='MuscleDefaults', loci=rRNA_set,\n",
      "                         cmd='muscle', program_name='muscle',\n",
      "                         cline_args=dict())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "mafft --localpair --maxiterate 1000 464461405582493.52_CDS_proteins_coi.fasta\n",
        "muscle -in 748161405582493.52_18S.fasta\n",
        "muscle -in 748161405582493.52_28S.fasta\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Align the loci"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db.align(alignment_methods=[linsi, muscle])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db.write_alns()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Mask ambigiously alignmed positions - To Do"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db.trim()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "AlignIO.write(db.trimmed_alignments['28S'],'28S_trimmed_aln.fasta','fasta')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Concatenate the loci based on the definitions above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db.add_concatenation(combined)\n",
      "db.make_concatenation_alignments()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Run the trees"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raxml_method_loci = RaxmlTreeReconstructionMethod(db, method_name='fa', program_name='raxmlHPC-PTHREADS-SSE3',\n",
      "                 cmd='raxmlHPC-PTHREADS-SSE3', preset = 'fa', alns=['coi','18S','28S'], model='GAMMA', matrix='JTT', threads=5,\n",
      "                 cline_args={'-#': 3})\n",
      "\n",
      "raxml_method_concat = RaxmlTreeReconstructionMethod(db, method_name='fD_fb', program_name='raxmlHPC-PTHREADS-SSE3',\n",
      "                 cmd='raxmlHPC-PTHREADS-SSE3', preset = 'fD_fb', alns=['combined'], model='GAMMA', matrix='JTT', threads=5,\n",
      "                 cline_args={'-N': 1})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "raxmlHPC-PTHREADS-SSE3 -f a -m GTRGAMMA -n 105591405545836.72_coi0 -p 655 -s 105591405545836.72_coi.fasta -T 5 -x 487 -N 3\n",
        "raxmlHPC-PTHREADS-SSE3 -f a -m GTRGAMMA -n 105591405545836.72_18S0 -p 515 -s 105591405545836.72_18S.fasta -T 5 -x 871 -N 3\n",
        "raxmlHPC-PTHREADS-SSE3 -f a -m GTRGAMMA -n 105591405545836.72_28S0 -p 49 -s 105591405545836.72_28S.fasta -T 5 -x 960 -N 3\n",
        "raxmlHPC-PTHREADS-SSE3 -f D -m PROTGAMMAJTT -n 673601405545836.73_combined0 -q 673601405545836.73_combined_partfile -p 536 -s 673601405545836.73_combined.fasta -T 5 -N 1\n",
        "raxmlHPC-PTHREADS-SSE3 -f b -m PROTGAMMAJTT -n 673601405545836.73_combined1 -q 673601405545836.73_combined_partfile -p 318 -s 673601405545836.73_combined.fasta -t RAxML_bestTree.673601405545836.73_combined0 -T 5 -z RAxML_rellBootstrap.673601405545836.73_combined0\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db.tree([raxml_method_loci, raxml_method_concat])\n",
      "for t in db.trees.keys():\n",
      "    print db.trees[t][0].tree_method_id, db.trees[t][1][:30]+'...'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "105591405545836.72_coi (HM032747.2_f0:1.40467e-06[&&N...\n",
        "105591405545836.72_18S ((JX177971.1_f0:9.88551e-07[&&...\n",
        "105591405545836.72_28S ((JX177960.1_f0:0.0165211[&&NH...\n",
        "673601405545836.73_combined (((TAU_25618:1.06671e-06[&&NHX...\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Annotate the trees"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db.clear_tree_annotations()\n",
      "\n",
      "\n",
      "genera_colors = {'Astrophorid': 'black',\n",
      "                 'Cinachyrella': 'black',\n",
      "                 'Paratetilla': 'blue', # Made a change here\n",
      "                 'Amphitethya': 'black',\n",
      "                 'Tetilla': 'black',        \n",
      "                 'Cinachyra': 'black',\n",
      "                 'Acanthotetilla': 'black',\n",
      "                 'Fangophilina': 'black',\n",
      "                 'Craniella': 'black'\n",
      "                 }\n",
      "\n",
      "supports = {'black':[100,99],\n",
      "            'dimgray':[99,75],\n",
      "            'silver':[75,50]}\n",
      "\n",
      "db.annotate('/home/amir/Dropbox/ReproPhylo',root_meta = 'genus',\n",
      "            root_value = 'Astrophorid',\n",
      "            leaf_labels_txt_meta=['organism','feature_id'],\n",
      "            #leaf_node_color_meta='organism', \n",
      "            #leaf_label_colors=genera_colors,\n",
      "\n",
      "            node_support_dict = supports,\n",
      "            #multifruc=90,\n",
      "            \n",
      "            #heat_map_meta = ['porocalices','cortex','calthrops'],\n",
      "            #heat_map_colour_scheme = 0\n",
      "            )\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<html>\n",
        "<A href=file:///home/amir/Dropbox/ReproPhylo/105591405545836.72_coi.png>105591405545836.72_coi</A><BR>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<A href=file:///home/amir/Dropbox/ReproPhylo/105591405545836.72_18S.png>105591405545836.72_18S</A><BR>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<A href=file:///home/amir/Dropbox/ReproPhylo/105591405545836.72_28S.png>105591405545836.72_28S</A><BR>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<A href=file:///home/amir/Dropbox/ReproPhylo/673601405545836.73_combined.png>673601405545836.73_combined</A><BR>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "</html>\n",
        "/home/amir/Dropbox/ReproPhylo\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "report = open('report.html','wt')\n",
      "for line in report_methods(db, '.'):\n",
      "    report.write(line + '\\n')\n",
      "report.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "#import cloud.serialization.cloudpickle as pickle\n",
      "#output = open(\"reprophylo_database.pkl\",'wb')\n",
      "#pickle.dump(db, output)\n",
      "#output.close()\n",
      "\n",
      "\n",
      "\n",
      "def publish(db, folder_name, figures_folder):\n",
      "    import os\n",
      "    folder = None\n",
      "    zip_file = None\n",
      "    if folder_name.endswith('.zip'):\n",
      "        zip_file = folder_name\n",
      "        folder = folder_name[:-4]\n",
      "    else:\n",
      "        folder = folder_name\n",
      "        zip_file = folder_name + '.zip'\n",
      "    if os.path.exists(folder) or os.path.exists(zip_file):\n",
      "        raise IOError(folder_name + ' already exists')\n",
      "    \n",
      "    os.makedirs(folder)\n",
      "    db.write(folder+'/tree_and_alns.nexml','nexml')\n",
      "    db.write(folder+'/sequences_and_metadata.gb','genbank')\n",
      "    report = open(folder+'/report.html','wt')\n",
      "    for line in report_methods(db, figures_folder):\n",
      "        report.write(line + '\\n')\n",
      "    report.close()\n",
      "\n",
      "    for tree in db.trees.keys():\n",
      "        if os.path.isfile(figures_folder+'/'+db.trees[tree][0].get_leaves()[0].tree_method_id+'.png'):\n",
      "            from shutil import copyfile\n",
      "            copyfile(figures_folder+'/'+db.trees[tree][0].get_leaves()[0].tree_method_id+'.png',\n",
      "                     folder+'/'+db.trees[tree][0].get_leaves()[0].tree_method_id+'.png')\n",
      "            \n",
      "         \n",
      "    \n",
      "    \n",
      "           \n",
      "    \n",
      "    import zipfile, shutil\n",
      "\n",
      "    zf = zipfile.ZipFile(zip_file, \"w\")\n",
      "    for dirname, subdirs, files in os.walk(folder):\n",
      "        zf.write(dirname)\n",
      "        for filename in files:\n",
      "            zf.write(os.path.join(dirname, filename))\n",
      "    zf.close()\n",
      "    shutil.rmtree(folder)\n",
      "publish(db, \"db_output\", '/home/amir/Dropbox/ReproPhylo')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Classes\n",
      "\n",
      "# Locus, Concatenation, Database, AlignmentMethod"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%file reprophylo.py\n",
      "\n",
      "from Bio import SeqIO\n",
      "import os, csv, sys, dendropy, re, time, random, glob, platform\n",
      "from Bio.Seq import Seq\n",
      "from Bio.Alphabet import IUPAC\n",
      "from Bio.SeqRecord import SeqRecord\n",
      "from Bio.SeqFeature import SeqFeature, FeatureLocation, CompoundLocation\n",
      "from Bio.Align.Applications import MafftCommandline, MuscleCommandline\n",
      "from StringIO import StringIO \n",
      "from Bio import AlignIO \n",
      "from Bio.Phylo.Applications import RaxmlCommandline\n",
      "from Bio.Align import MultipleSeqAlignment\n",
      "from Bio.SeqUtils import GC\n",
      "#from cogent import LoadSeqs, LoadTree\n",
      "#from cogent.app.raxml_v730 import build_tree_from_alignment\n",
      "from ete2 import *\n",
      "\n",
      "class Locus:\n",
      "       \n",
      "    char_type = 'NotSet'\n",
      "    feature_type = 'NotSet'\n",
      "    name = 'NotSet'\n",
      "    aliases = []\n",
      "\n",
      "    def __init__(self, char_type=char_type, feature_type=feature_type,\n",
      "                 name=name, aliases=aliases):\n",
      "        \"\"\" Use to tell ReproPhylo which partition to build from a mixed (or not) data.\n",
      "        \n",
      "        >>> locus = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "        >>> print locus\n",
      "        Locus(char_type=dna, feature_type=CDS, name=coi, aliases=cox1; COX1; coi; COI; CoI)\"\"\"\n",
      "\n",
      "        \n",
      "        self.char_type = char_type\n",
      "        self.feature_type = feature_type\n",
      "        self.name = name\n",
      "        self.aliases = aliases\n",
      "        \n",
      "        valid = ['dna','prot']\n",
      "        if not self.char_type in valid:\n",
      "            raise ValueError('self.char_type should be \\'dna\\' or \\'prot\\'')\n",
      "        if not type(self.feature_type) is str:\n",
      "            raise ValueError('self.feature_type should be a string')\n",
      "        if not type(self.name) is str:\n",
      "            raise ValueError('self.name should be a string')\n",
      "        if not type(self.aliases) is list:\n",
      "            raise ValueError('self.aliases should be a list')\n",
      "        else:\n",
      "            for a in self.aliases:\n",
      "                if not type(a) is str:\n",
      "                    raise ValueError('aliases in self.aliases have to be strings')\n",
      "            \n",
      "\n",
      "\n",
      "    def __str__(self):\n",
      "        aliases_str = ('; ').join(self.aliases)\n",
      "        return ('Locus(char_type='+self.char_type+', feature_type='+self.feature_type+\n",
      "                ', name='+self.name+', aliases='+aliases_str+')')\n",
      "            \n",
      "                \n",
      "\n",
      "#if __name__ == \"__main__\":\n",
      "#    import doctest\n",
      "#    doctest.testmod() \n",
      "       \n",
      "\n",
      "class Concatenation:\n",
      "    \n",
      "    name = 'NotSet'\n",
      "    loci = []\n",
      "    otu_meta = 'NotSet'\n",
      "    concat_must_have_all_of = []\n",
      "    concat_must_have_one_of = []\n",
      "    feature_id_dict = {}\n",
      "    \n",
      "    def __init__(self,\n",
      "                 name = name,\n",
      "                 loci = loci,\n",
      "                 otu_meta = otu_meta,\n",
      "                 concat_must_have_all_of = concat_must_have_all_of,\n",
      "                 concat_must_have_one_of = concat_must_have_one_of):\n",
      "        self.name = name\n",
      "        self.loci = loci\n",
      "        self.otu_meta = otu_meta\n",
      "        self.concat_must_have_all_of = concat_must_have_all_of\n",
      "        self.concat_must_have_one_of = concat_must_have_one_of\n",
      "        self.feature_id_dict = {}\n",
      "        seen = []\n",
      "        for locus in loci:\n",
      "            if not isinstance(locus, Locus):\n",
      "                raise TypeError(\"Expecting Locus object in loci list\")\n",
      "            if locus.name in seen:\n",
      "                raise NameError('Locus ' + locus.name + ' apears more than once in self.loci')\n",
      "            else:\n",
      "                seen.append(locus.name)\n",
      "        \n",
      "# tools to prepare the db\n",
      "\n",
      "\n",
      "def platform_report():\n",
      "    import pkg_resources\n",
      "    modules = []\n",
      "    for i in ('ete2','biopython','dendropy'):\n",
      "        modules.append(i+' version: '+\n",
      "                            pkg_resources.get_distribution(i).version)\n",
      "    return(['Platform: '+platform.platform(aliased=0, terse=0),\n",
      "            'Processor: '+platform.processor(),\n",
      "            'Python build: '+platform.python_build()[0] + platform.python_build()[1],\n",
      "            'Python compiler: '+platform.python_compiler(),\n",
      "            'Python implementation: ' +platform.python_implementation(),\n",
      "            'Python version: ' + platform.python_version()]+\n",
      "             modules+\n",
      "            ['User: ' +platform.uname()[1]])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def keep_feature(feature, loci):\n",
      "    \n",
      "    \"\"\" Returns true if a feature's type is in one of the loci and if the gene\n",
      "    or product qualifiers is in the aliases of one of the loci\n",
      "    \n",
      "    >>> coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "    >>> location = FeatureLocation(1,100)\n",
      "    >>> feature = SeqFeature()\n",
      "    >>> feature.location = location\n",
      "    >>> feature.type = 'CDS'\n",
      "    >>> feature.qualifiers['gene'] = ['CoI']\n",
      "    >>> a = keep_feature(feature, [coi])\n",
      "    >>> print a\n",
      "    True\"\"\"\n",
      "    \n",
      "    keep = 0\n",
      "    for g in loci:\n",
      "        if not g.name in g.aliases:\n",
      "            g.aliases.append(g.name)\n",
      "        if feature.type == 'source':\n",
      "            keep = 1\n",
      "        elif feature.type == g.feature_type:\n",
      "            qaul = False\n",
      "            if 'gene' in feature.qualifiers.keys():\n",
      "                qual = 'gene'\n",
      "            elif 'product' in feature.qualifiers.keys():\n",
      "                qual = 'product'\n",
      "            if not qual == False and feature.qualifiers[qual][0] in g.aliases:\n",
      "                keep = 1\n",
      "    if keep == 1:\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "  \n",
      "def dwindle_record(record, loci):\n",
      "    \n",
      "    \"\"\" retains only features that are called by guides and records with features that are\n",
      "        called by guides\n",
      "        \n",
      "    >>> coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "    >>> location = FeatureLocation(1,100)\n",
      "    >>> kept_feature = SeqFeature()\n",
      "    >>> kept_feature.location = location\n",
      "    >>> kept_feature.type = 'CDS'\n",
      "    >>> kept_feature.qualifiers['gene'] = ['CoI']\n",
      "    >>> dwindled_feature = SeqFeature()\n",
      "    >>> dwindled_feature.location = location\n",
      "    >>> dwindled_feature.type = 'rRNA'\n",
      "    >>> dwindled_feature.qualifiers['gene'] = ['LSU']\n",
      "    >>> s = 'atgc'*1000\n",
      "    >>> kept_record = SeqRecord(seq=Seq(s, IUPAC.ambiguous_dna), id='1', description='spam')\n",
      "    >>> kept_record.features.append(kept_feature)\n",
      "    >>> kept_record.features.append(dwindled_feature)\n",
      "    >>> print len(kept_record.features)\n",
      "    2\n",
      "    >>> a = dwindle_record(kept_record, [coi])\n",
      "    >>> print len(kept_record.features)\n",
      "    1\"\"\"\n",
      "    \n",
      "    dwindled_features = []\n",
      "    feature_count = 0\n",
      "    for feature in record.features:\n",
      "        if keep_feature(feature, loci)== True:\n",
      "            if feature.type == 'source' and not 'feature_id' in feature.qualifiers.keys():\n",
      "                feature.qualifiers['feature_id'] = [record.id + '_source']\n",
      "            elif not 'feature_id' in feature.qualifiers.keys():\n",
      "                feature.qualifiers['feature_id'] = [record.id + '_f' + str(feature_count)]\n",
      "                feature_count += 1\n",
      "            if not feature.type == 'source':\n",
      "                feature_seq = feature.extract(record.seq)\n",
      "                degen = len(feature_seq)\n",
      "                for i in ['A','T','G','C','U','a','t','g','c','u']:\n",
      "                    degen -= feature_seq.count(i)\n",
      "                feature.qualifiers['GC_content'] = [str(GC(feature_seq))]\n",
      "                feature.qualifiers['nuc_degen_prop'] = [str(float(degen)/len(feature_seq))]\n",
      "                if 'translation' in feature.qualifiers.keys():\n",
      "                    transl = feature.qualifiers['translation'][0]\n",
      "                    degen = 0\n",
      "                    for i in ['B', 'X', 'Z', 'b', 'x', 'z']:\n",
      "                        degen += transl.count(i)\n",
      "                    feature.qualifiers['prot_degen_prop'] = [str(float(degen)/len(transl))]                    \n",
      "            dwindled_features.append(feature)\n",
      "    record.features = dwindled_features\n",
      "    return record\n",
      "            \n",
      "           \n",
      "#def guess_format(input_filename):\n",
      "#    \n",
      "#    \"\"\" Runs a perl script that returns the input's format\n",
      "#    \n",
      "#    >>> filename = 'data/22_rotifer_COI_seqs.fas'\n",
      "#    >>> print guess_format(filename)\n",
      "#    fasta\"\"\"\n",
      "#    \n",
      "#    if os.path.exists(input_filename):\n",
      "#        os.system('perl guesser.pl ' + input_filename)\n",
      "#        guess = open(input_filename + '.format','r').read();\n",
      "#        os.remove(input_filename + '.format')\n",
      "#        return guess\n",
      "#    else:\n",
      "#        sys.exit('Cannot guess ' + input_filename + '. Does it exist?')\n",
      "      \n",
      "#def is_embl_or_gb(input_filename):\n",
      "#    \n",
      "#    \"\"\" Using the guess_format() function to check if the input conforms with\n",
      "#        embl or genbank formats.\n",
      "#    \"\"\" \n",
      "#    \n",
      "#    input_format = guess_format(input_filename)\n",
      "#    if input_format == 'embl' or input_format == 'genbank':\n",
      "#        return True\n",
      "#    else:\n",
      "#        return False\n",
      "\n",
      "def is_embl_or_gb(input_filename):\n",
      "    suffices = ['.gb','.embl']\n",
      "    gb = False\n",
      "    for s in suffices:\n",
      "        if s in input_filename:\n",
      "            gb = True\n",
      "    return gb\n",
      "\n",
      "def parse_input(input_filename, fmt):\n",
      "    return SeqIO.parse(input_filename, fmt)\n",
      "\n",
      "def generate_pickle_filename():\n",
      "    t = time.ctime(None)+'_'+str(random.random())\n",
      "    t = re.sub(' ','_',t)\n",
      "    t = re.sub(':','_',t)\n",
      "    t = re.sub('\\.','_',t)\n",
      "    return t+'.pkl'\n",
      "\n",
      "def list_to_string(List):\n",
      "    \n",
      "    \"\"\" handles list printing as a nice string\n",
      "    \n",
      "    >>> L = ['a','b','b']\n",
      "    >>> print list_to_string(L)\n",
      "    a;b;b\"\"\"\n",
      "    \n",
      "    string = ''\n",
      "    for i in List:\n",
      "        if type(i) is str and '\\n' in i:\n",
      "            string += lines_to_line(i).rstrip()+';'\n",
      "        else:\n",
      "            string += str(i)+';'\n",
      "    return string[:-1]\n",
      "\n",
      "def lines_to_line(lines):\n",
      "    \n",
      "    \"\"\" Replaces newline with space\"\"\"\n",
      "    \n",
      "    lines = lines.split('\\n')\n",
      "    return (' ').join(lines)\n",
      "\n",
      "def type_to_single_line_str(var):\n",
      "    if type(var) is str and '\\n' in var:\n",
      "        return lines_to_line(var)\n",
      "    elif type(var) is str or type(var) is int or type(var) is float:\n",
      "        return str(var)\n",
      "    elif type(var) is list and len(var) == 1:\n",
      "        return str(var[0])\n",
      "    elif type(var) is list and len(var) > 0:\n",
      "        return list_to_string(var)\n",
      "    else:\n",
      "        return var\n",
      "\n",
      "\n",
      "def get_qualifiers_dictionary(database, feature_id):\n",
      "    if type(feature_id) is list:\n",
      "        feature_id = feature_id[0]\n",
      "    record_id = feature_id.split('_')[0]\n",
      "    qualifiers_dictionary={}\n",
      "    for record in database.records:\n",
      "        if record.id in feature_id:\n",
      "            for annotation in record.annotations.keys():\n",
      "                qualifiers_dictionary['annotation_'+annotation]=record.annotations[annotation]\n",
      "            for feature in record.features:\n",
      "                if feature.type == 'source':\n",
      "                    for qualifier in feature.qualifiers.keys():\n",
      "                        qualifiers_dictionary['source_'+qualifier]=feature.qualifiers[qualifier][0]\n",
      "                elif feature.qualifiers['feature_id'][0] == feature_id:\n",
      "                    for qualifier in feature.qualifiers.keys():\n",
      "                        qualifiers_dictionary[qualifier]=feature.qualifiers[qualifier][0]\n",
      "    return qualifiers_dictionary\n",
      "\n",
      "def seq_format_from_suffix(suffix):\n",
      "    suffices = {'fasta': ['fas','fasta','fa','fna'],\n",
      "                'genbank': ['gb','genbank'],\n",
      "                'embl': ['embl']}\n",
      "    found = False\n",
      "    for key in suffices.keys():\n",
      "        if suffix in suffices[key]:\n",
      "            found = True\n",
      "            return key\n",
      "    if not found:\n",
      "        raise RuntimeError(suffix+' is not a recognised suffix of an unaligned sequence file')\n",
      "        \n",
      "class Database:\n",
      "    \n",
      "\n",
      "    def __init__(self, loci):\n",
      "        self.records = []\n",
      "        self.loci = loci\n",
      "        self.records_by_locus = {}\n",
      "        self.concatenations = []\n",
      "        self.alignments = {}\n",
      "        self.trimmed_alignments = {}\n",
      "        self.trees = {}\n",
      "        self.used_methods = []\n",
      "        seen = []\n",
      "        for locus in loci:\n",
      "            if not isinstance(locus, Locus):\n",
      "                raise TypeError(\"Expecting Locus object in loci list. \"+locus+\n",
      "                                \" not a Locus object\")\n",
      "            if locus.name in seen:\n",
      "                raise NameError('Locus ' + locus.name + ' apears more than once in self.loci')\n",
      "            else:\n",
      "                seen.append(locus.name)\n",
      "\n",
      "    def read_embl_genbank(self, input_filenames_list):\n",
      "        generators = []\n",
      "        for input_filename in input_filenames_list:\n",
      "            generators.append(parse_input(input_filename, 'gb'))\n",
      "#            if is_embl_or_gb(input_filename):\n",
      "#                generators.append(parse_input(input_filename, 'gb'))\n",
      "#                print 'to do: correct parse_input'\n",
      "#                print 'Accepted:', input_filename\n",
      "#            else:\n",
      "#                print ('Rejected: ' + input_filename + ' should be genbank or embl and end with .gb or .embl')\n",
      "            for generator in generators:\n",
      "                for record in generator:\n",
      "                    dwindled_record = dwindle_record(record, self.loci)\n",
      "                    if len(record.features) > 1:\n",
      "                        self.records.append(dwindled_record)\n",
      "                    elif len(record.features) == 1 and not record.features[0].type == 'source':\n",
      "                        self.records.append(dwindled_record)\n",
      "\n",
      "#    def read_denovo(self, input_filename, feature_type, char_type, source_qualifiers = {}):\n",
      "#        count = 0\n",
      "#        # start the counter where it stoped the last time we read denovo things\n",
      "#        for record in self.record:\n",
      "#            if 'denovo' in record.id:\n",
      "#                serial = int(record.id[6:])\n",
      "#                if serial > count:\n",
      "#                    count = serial+1\n",
      "#        denovo = SeqIO.parse(input_filename, guess_format(input_filename))\n",
      "#        for record in denovo:\n",
      "#            feature = SeqFeature(FeatureLocation(0, len(record.seq)), type=feature_type, strand=1)\n",
      "#            source = SeqFeature(FeatureLocation(0, len(record.seq)), type='source', strand=1)\n",
      "#            if len(source_qualifiers.keys())>0:\n",
      "#                for key in source_qualifiers.keys():\n",
      "#                    source.qualifiers[key] = source_qualifiers[key]\n",
      "#            feature.qualifiers['original_id'] = [record.id]\n",
      "#            feature.qualifiers['original_desc'] = [(' ').join(record.description.split()[1:])]\n",
      "#            record.id = 'denovo'+str(count)\n",
      "#            count += 1\n",
      "#            feature.qualifiers['feature_id'] = [record.id+'_f0']\n",
      "#            source.qualifiers['feature_id'] = [record.id+'_source']\n",
      "#            record.features = [source, feature]\n",
      "#            if char_type == 'prot':\n",
      "#                record.seq.alphabet = IUPAC.protein\n",
      "#            elif char_type == 'dna':\n",
      "#                record.seq.alphabet = IUPAC.ambiguous_dna\n",
      "#            self.records.append(record)\n",
      "            \n",
      "    def read_denovo(self, input_filenames, char_type):\n",
      "        count = 0\n",
      "        # start the counter where it stoped the last time we read denovo things\n",
      "        for record in self.records:\n",
      "            if 'denovo' in record.id:\n",
      "                serial = int(record.id[6:])\n",
      "                if serial > count:\n",
      "                    count = serial+1\n",
      "        for input_filename in input_filenames:\n",
      "            # Check input file format\n",
      "            #suffix = re.search(r'\\.([^\\.]+)$',input_filename).groups()[1]\n",
      "            #input_format = seq_format_from_suffix(suffix)\n",
      "            #if input_format == 'embl' or input_format == 'genbank':\n",
      "            #    raise IOError('To read embl or genbank files use self.read_embl_genbank([filename0, filename1])')\n",
      "            #denovo = SeqIO.parse(input_filename, input_format)\n",
      "            denovo = SeqIO.parse(input_filename, 'fasta')\n",
      "            for record in denovo:\n",
      "                source = SeqFeature(FeatureLocation(0, len(record.seq)), type='source', strand=1)\n",
      "                source.qualifiers['original_id'] = [record.id]\n",
      "                source.qualifiers['original_desc'] = [(' ').join(record.description.split()[1:])]\n",
      "                record.id = 'denovo'+str(count)\n",
      "                source.qualifiers['feature_id'] = [record.id+'_source']\n",
      "                record.features = [source]\n",
      "                if char_type == 'prot':\n",
      "                    record.seq.alphabet = IUPAC.protein\n",
      "                elif char_type == 'dna':\n",
      "                    record.seq.alphabet = IUPAC.ambiguous_dna\n",
      "                count += 1\n",
      "                self.records.append(record)\n",
      "                \n",
      "    def add_feature_to_record(self, record_id, feature_type, location='full', qualifiers={}):\n",
      "        for record in self.records:\n",
      "            if record.id == record_id:\n",
      "                #determine new feature id\n",
      "                feature_id = None\n",
      "                serials = []\n",
      "                for feature in record.features:\n",
      "                    if 'feature_id' in feature.qualifiers.keys():\n",
      "                        if '_f' in feature.qualifiers['feature_id']:\n",
      "                            f = feature.qualifiers['feature_id']\n",
      "                            serials.append(int(f.split('_')[1][1:]))\n",
      "                serials.sort(reverse = True)\n",
      "                if len(serials) > 0:\n",
      "                    feature_id = record.id + '_f' + str(serials[0]+1)\n",
      "                else:\n",
      "                    feature_id = record.id + '_f0'\n",
      "                feature = None\n",
      "                if location == 'full':\n",
      "                    feature = SeqFeature(FeatureLocation(0, len(record.seq)),\n",
      "                                         type=feature_type,\n",
      "                                         strand=1)\n",
      "                elif isinstance(location, list):\n",
      "                    for i in location:\n",
      "                        if not isinstance(i, list):\n",
      "                            raise RuntimeError('\\'location\\' takes either \\'full\\' or a list of lists')\n",
      "                    if len(location) == 1:\n",
      "                        feature = SeqFeature(FeatureLocation(int(location[0][0])-1,int(location[0][1])),\n",
      "                                             type=feature_type, strand=int(location[0][2]))\n",
      "                    elif len(location) > 1:\n",
      "                        list_of_locations = []\n",
      "                        for i in location:\n",
      "                            start = int(i[0]-1)\n",
      "                            end = int(i[1])\n",
      "                            strand = int(i[2])\n",
      "                            list_of_locations.append(FeatureLocation(start,end,strand=strand))\n",
      "                        feature = SeqFeature(CompoundLocation(list_of_locations),type=feature_type)\n",
      "                feature.qualifiers['feature_id'] = [feature_id]\n",
      "                if len(qualifiers.keys()) > 0:\n",
      "                    for key in qualifiers.keys():\n",
      "                        feature.qualifiers[key] = [qualifiers[key]]\n",
      "                if (('codon_start' in qualifiers.keys()) and\n",
      "                    ('transl_table' in qualifiers.keys())):\n",
      "                    cds = feature.extract(record.seq)\n",
      "                    if str(qualifiers['codon_start']) == '2':\n",
      "                        cds = cds[1:]\n",
      "                    elif str(qualifiers['codon_start']) == '3':\n",
      "                        cds = cds[2:]\n",
      "                    translation = cds.translate(table=int(qualifiers['transl_table']))\n",
      "                    if len(translation)*3 < float(0.9)*len(cds):\n",
      "                        raise RuntimeWarning('The translation of feature '+feature_id+' uses less than 90%'+\n",
      "                                             ' of the coding sequence')\n",
      "                    feature.qualifiers['translation'] = [str(translation)]\n",
      "                record.features.append(feature)\n",
      "\n",
      "    \n",
      "    def add_concatenation(self, concatenation_object):\n",
      "        if isinstance(concatenation_object, Concatenation):\n",
      "            seen = []\n",
      "            for s in self.concatenations:\n",
      "                seen.append(s.name)\n",
      "            if concatenation_object.name in seen:\n",
      "                raise NameError('Concatenation ' + concatenation_object.name +\n",
      "                                ' apears more than once in self.concatenations')\n",
      "            else:\n",
      "                self.concatenations.append(concatenation_object)\n",
      "        else:\n",
      "            raise TypeError(\"Expecting Concatenation object\")\n",
      "\n",
      "    def make_concatenation_alignments(self):\n",
      "        for s in self.concatenations:\n",
      "            \n",
      "            # get a non-redundant list of 'accross partitions' ids stored in 'meta', such as voucher specimen\n",
      "            \n",
      "            meta = s.otu_meta\n",
      "            meta_list = []\n",
      "            for record in self.records:\n",
      "                for feature in record.features:\n",
      "                    if not feature.type == 'source':\n",
      "                        qualifiers_dictionary = get_qualifiers_dictionary(self,\n",
      "                                                                          feature.qualifiers['feature_id'])\n",
      "                        if (meta in qualifiers_dictionary.keys() and\n",
      "                            not qualifiers_dictionary[meta] in meta_list):\n",
      "                            meta_list.append(qualifiers_dictionary[meta])\n",
      "                            \n",
      "            # make lists of available feature_ids in each locus\n",
      "            available_features = {}\n",
      "            for locus in s.loci:\n",
      "                available_features[locus.name] = []\n",
      "                for record in self.records_by_locus[locus.name]:\n",
      "                    available_features[locus.name].append(record.id)\n",
      "                    \n",
      "            # make a dict of individuals that fulfil the set's first rule\n",
      "            \n",
      "            seen_locus_names = []\n",
      "            \n",
      "            included_individuals = {}\n",
      "            for individual in meta_list:\n",
      "                include = True\n",
      "                for locus_name in s.concat_must_have_all_of:\n",
      "                    if not locus_name in seen_locus_names:\n",
      "                        seen_locus_names.append(locus_name)\n",
      "                    locus_specific_features = []\n",
      "                    for feature_id in available_features[locus_name]:\n",
      "                        qualifiers_dictionary = get_qualifiers_dictionary(self,feature_id)\n",
      "                        if meta in qualifiers_dictionary.keys() and qualifiers_dictionary[meta] == individual:\n",
      "                            locus_specific_features.append(feature_id)\n",
      "                    if len(locus_specific_features) == 1:\n",
      "                        if not individual in included_individuals.keys():\n",
      "                            included_individuals[individual] = {}\n",
      "                        included_individuals[individual][locus_name] = locus_specific_features[0]\n",
      "                    elif len(locus_specific_features) > 1:\n",
      "                        raise RuntimeError(individual + ' is not unique for ' + locus_name)\n",
      "                    else:\n",
      "                        include = False\n",
      "                if individual in included_individuals.keys() and not include:\n",
      "                    included_individuals.pop(individual, None)\n",
      "                    \n",
      "                       \n",
      "            # check if the individual fullfil the second set rule\n",
      "            \n",
      "            for individual in included_individuals.keys():\n",
      "                include = True\n",
      "                for loci_group in s.concat_must_have_one_of:\n",
      "                    count = 0\n",
      "                    for locus_name in loci_group:\n",
      "                        if not locus_name in seen_locus_names:\n",
      "                            seen_locus_names.append(locus_name)\n",
      "                        locus_specific_features = []\n",
      "                        for feature_id in available_features[locus_name]:\n",
      "                            qualifiers_dictionary = get_qualifiers_dictionary(self,feature_id)\n",
      "                            if meta in qualifiers_dictionary.keys() and qualifiers_dictionary[meta] == individual:\n",
      "                                locus_specific_features.append(feature_id)\n",
      "                        if len(locus_specific_features) == 1:\n",
      "                            count += 1\n",
      "                            included_individuals[individual][locus_name] = locus_specific_features[0]\n",
      "                        elif len(locus_specific_features) > 1:\n",
      "                            raise RuntimeError(individual + ' is not unique for ' + locus_name)\n",
      "                    if count == 0:\n",
      "                        include = False\n",
      "                if not include:\n",
      "                    included_individuals.pop(individual, None)\n",
      "                    \n",
      "            \n",
      "            # add loci that are in the set but not addressed in rules\n",
      "            \n",
      "            for individual in included_individuals.keys():\n",
      "                for locus in s.loci:\n",
      "                    if not locus.name in seen_locus_names:\n",
      "                        for feature_id in available_features[locus.name]:\n",
      "                            qualifiers_dictionary = get_qualifiers_dictionary(self,feature_id)\n",
      "                            if meta in qualifiers_dictionary.keys() and qualifiers_dictionary[meta] == individual:\n",
      "                                locus_specific_features.append(feature_id)\n",
      "                        if len(locus_specific_features) == 1:\n",
      "                            included_individuals[locus.name] = locus_specific_features[0]\n",
      "                        elif len(locus_specific_features) > 1:\n",
      "                            raise RuntimeError(individual + ' is not unique for ' + locus.name)\n",
      "                        \n",
      "            \n",
      "            # build alignment\n",
      "            \n",
      "            concat_records = []\n",
      "            alignment = []\n",
      "            for individual in included_individuals.keys():\n",
      "                sequence = ''\n",
      "                for locus in s.loci:\n",
      "                    length = len(self.trimmed_alignments[locus.name][0])\n",
      "                    if locus.name in included_individuals[individual].keys():\n",
      "                        for record in self.trimmed_alignments[locus.name]:\n",
      "                            if record.id == included_individuals[individual][locus.name]:\n",
      "                                sequence += str(record.seq)\n",
      "                    else:\n",
      "                        sequence += 'N'*length\n",
      "                concat_sequence = SeqRecord(seq = Seq(sequence), id = individual, description = '')\n",
      "                alignment.append(concat_sequence)\n",
      "            self.trimmed_alignments[s.name] = MultipleSeqAlignment(alignment)                \n",
      "            s.feature_id_dict = included_individuals    \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "    def write(self, filename, format = 'genbank'):\n",
      "        if format == 'nexml':\n",
      "            self.write_nexml(filename)\n",
      "        elif format == 'genbank' or format == 'embl':\n",
      "            SeqIO.write(self.records, filename, format)\n",
      "        elif format == 'csv':\n",
      "            # get titles for source and othe features\n",
      "            source_qualifiers = []\n",
      "            feature_qualifiers = []\n",
      "            for record in self.records:\n",
      "                for feature in record.features:\n",
      "                    for key in feature.qualifiers.keys():\n",
      "                        if feature.type == 'source' and not key in source_qualifiers:\n",
      "                            source_qualifiers.append(key)\n",
      "                        elif not feature.type == 'source' and not key in feature_qualifiers:\n",
      "                            feature_qualifiers.append(key)\n",
      "            \n",
      "            with open(filename, 'wb') as csvfile:\n",
      "                linewriter = csv.writer(csvfile, delimiter='\\t',\n",
      "                                        quotechar='|',\n",
      "                                        quoting=csv.QUOTE_MINIMAL)\n",
      "                linewriter.writerow(['record_id','seq']+source_qualifiers+['taxonomy']+feature_qualifiers)\n",
      "                for record in self.records:\n",
      "                    seq = ''\n",
      "                    if len(record.seq) <= 10:\n",
      "                        seq = str(record.seq)[0:11]\n",
      "                    else:\n",
      "                        seq = str(record.seq)[0:6] + '...' + str(record.seq)[-5:]\n",
      "                    \n",
      "                    \n",
      "                    line_start = [record.id, seq]\n",
      "                    source = None\n",
      "                    for feature in record.features:\n",
      "                        if feature.type == 'source':\n",
      "                            source = feature\n",
      "                    if not source == None:\n",
      "                        for qual in source_qualifiers:\n",
      "                            if qual in source.qualifiers.keys():\n",
      "                                line_start.append(type_to_single_line_str(source.qualifiers[qual]))\n",
      "                            else:\n",
      "                                line_start.append('null')\n",
      "                    elif source == None:\n",
      "                        for qual in source_qualifiers:\n",
      "                            line_start.append('null')\n",
      "                    if 'taxonomy' in record.annotations.keys():\n",
      "                        line_start.append(type_to_single_line_str(record.annotations['taxonomy']))\n",
      "                    else:\n",
      "                        line_start.append(['null'])\n",
      "                    for feature in record.features:\n",
      "                        if not feature.type == 'source':\n",
      "                            line = list(line_start)\n",
      "                            for qual in feature_qualifiers:\n",
      "                                if qual in feature.qualifiers.keys() and qual == 'translation':\n",
      "                                    trans = feature.qualifiers[qual][0]\n",
      "                                    if len(trans)<=10:\n",
      "                                        line.append(trans)\n",
      "                                    else:\n",
      "                                        line.append(trans[:6] + '...' + trans[-5:])\n",
      "                                elif qual in feature.qualifiers.keys():\n",
      "                                    line.append(type_to_single_line_str(feature.qualifiers[qual]))\n",
      "                                else:\n",
      "                                    line.append('null')\n",
      "                            linewriter.writerow(line)\n",
      "\n",
      "                            \n",
      "\n",
      "                \n",
      "    def if_this_then_that(self, IF_THIS, IN_THIS, THEN_THAT, IN_THAT, mode = 'whole'):\n",
      "        for record in self.records:\n",
      "            for feature in record.features:\n",
      "                if not feature.type == 'source':\n",
      "                    if IN_THIS in feature.qualifiers.keys():\n",
      "                        if not type(feature.qualifiers[IN_THIS]) is list:\n",
      "                            feature.qualifiers[IN_THIS] = [feature.qualifiers[IN_THIS]]\n",
      "                        for i in feature.qualifiers[IN_THIS]:\n",
      "                            if mode == 'whole':\n",
      "                                if i == IF_THIS:\n",
      "                                    feature.qualifiers[IN_THAT] = [THEN_THAT]\n",
      "                            elif mode == 'part':\n",
      "                                if IF_THIS in i:\n",
      "                                    feature.qualifiers[IN_THAT] = [THEN_THAT]\n",
      "    \n",
      "\n",
      "    def add_qualifier(self, feature_ids, name, value):\n",
      "        if not type(value) is list:\n",
      "                    value = [value]\n",
      "        for record in self.records:\n",
      "            for feature in record.features:\n",
      "                if feature.qualifiers['feature_id'][0] in feature_ids:\n",
      "                    feature.qualifiers[name] = value\n",
      "\n",
      "    def add_qualifier_from_source(self, qualifier):\n",
      "        for record in self.records:\n",
      "            source = None\n",
      "            for feature in record.features:\n",
      "                if feature.type == 'source':\n",
      "                    source = feature\n",
      "            value = None\n",
      "            if not source == None:\n",
      "              if qualifier in source.qualifiers.keys():\n",
      "                    value = source.qualifiers[qualifier]       \n",
      "            if not value == None:\n",
      "                if not type(value) is list:\n",
      "                    value = [value]\n",
      "                for feature in record.features:\n",
      "                    if not feature.type == 'source':\n",
      "                        feature.qualifiers[qualifier] = value\n",
      "    \n",
      "\n",
      "    def copy_paste_within_feature(self, from_qualifier, to_qualifier):\n",
      "        for record in self.records:\n",
      "            for feature in record.features:\n",
      "                if not feature.type == 'source':\n",
      "                    if from_qualifier in feature.qualifiers.keys():\n",
      "                        feature.qualifiers[to_qualifier] = feature.qualifiers[from_qualifier]\n",
      "                   \n",
      "    def copy_paste_from_features_to_source(self, from_feature_qual, to_source_qual):\n",
      "        for record in self.records:\n",
      "            source = None\n",
      "            values_from_features = []\n",
      "            for feature in record.features:\n",
      "                if not feature.type == 'source':\n",
      "                    if from_feature_qual in feature.qualifiers.keys():\n",
      "                        if not feature.qualifiers[from_feature_qual] in values_from_features:\n",
      "                            values_from_features += feature.qualifiers[from_feature_qual]\n",
      "                else:\n",
      "                    source = feature\n",
      "            if source == None:\n",
      "                source = SeqFeature(FeatureLocation(0, len(record.seq)), type='source', strand=1)\n",
      "                source.qualifiers['feature_id'] = record.id + '_source'\n",
      "                record.features = [source] + record.features\n",
      "            if not to_source_qual in source.qualifiers.keys():\n",
      "                source.qualifiers[to_source_qual] = values_from_features\n",
      "           \n",
      "    def species_vs_loci(self, outfile_name):\n",
      "        species_vs_loci = {}\n",
      "        for record in self.records:\n",
      "            organism = 'undef'\n",
      "            for feature in record.features:\n",
      "                if feature.type == 'source':\n",
      "                    if 'organism' in feature.qualifiers.keys():\n",
      "                        organism = feature.qualifiers['organism'][0]\n",
      "            if not organism in species_vs_loci.keys():\n",
      "                species_vs_loci[organism] = {}    \n",
      "            for feature in record.features:\n",
      "                if not feature.type == 'source':\n",
      "                    for locus in self.loci:\n",
      "                        if not locus.name in locus.aliases:\n",
      "                            locus.aliases.append(locus.name)\n",
      "                        if 'gene' in feature.qualifiers.keys():\n",
      "                            if feature.qualifiers['gene'][0] in locus.aliases:\n",
      "                                if not locus.name in species_vs_loci[organism].keys():\n",
      "                                    species_vs_loci[organism][locus.name] = 1\n",
      "                                else:\n",
      "                                    species_vs_loci[organism][locus.name] += 1\n",
      "                        elif 'product' in feature.qualifiers.keys():\n",
      "                            if feature.qualifiers['product'][0] in locus.aliases:\n",
      "                                if not locus.name in species_vs_loci[organism].keys():\n",
      "                                    species_vs_loci[organism][locus.name] = 1\n",
      "                                else:\n",
      "                                    species_vs_loci[organism][locus.name] += 1\n",
      "        with open(outfile_name, 'wb') as csvfile:\n",
      "            linewriter = csv.writer(csvfile, delimiter='\\t',\n",
      "                                    quotechar='|',\n",
      "                                    quoting=csv.QUOTE_MINIMAL)\n",
      "            loci_names = []\n",
      "            for g in self.loci:\n",
      "                loci_names.append(g.name)\n",
      "            linewriter.writerow(['species']+loci_names)\n",
      "            for organism in species_vs_loci.keys():\n",
      "                line = [organism]\n",
      "                for name in loci_names:\n",
      "                    if name in species_vs_loci[organism].keys():\n",
      "                        line.append(str(species_vs_loci[organism][name]))\n",
      "                    else:\n",
      "                        line.append('0')\n",
      "                linewriter.writerow(line)\n",
      "                    \n",
      "            \n",
      "        \n",
      "    def extract_by_locus(self):\n",
      "        data_by_locus = {}\n",
      "        for locus in self.loci:\n",
      "            if not locus.name in locus.aliases:\n",
      "                locus.aliases.append(locus.name)\n",
      "            records = []\n",
      "            for record in self.records:\n",
      "                for feature in record.features:\n",
      "                    if (feature.type == locus.feature_type and\n",
      "                        \n",
      "                        (('gene' in feature.qualifiers.keys() and\n",
      "                          feature.qualifiers['gene'][0] in locus.aliases) \n",
      "                         or\n",
      "                         ('product' in feature.qualifiers.keys() and \n",
      "                          feature.qualifiers['product'][0] in locus.aliases))\n",
      "                        \n",
      "                        ):\n",
      "                        if locus.char_type == 'dna':\n",
      "                            S = feature.extract(record.seq)\n",
      "                        elif locus.char_type == 'prot':\n",
      "                            S = Seq(feature.qualifiers['translation'][0], IUPAC.protein)\n",
      "                        feature_record = SeqRecord(seq = S, id = feature.qualifiers['feature_id'][0],\n",
      "                                                   description = '')\n",
      "                        records.append(feature_record)\n",
      "            data_by_locus[locus.name] = records\n",
      "        self.records_by_locus = data_by_locus\n",
      "\n",
      "\n",
      "    def write_by_locus(self, format = 'fasta'):\n",
      "        if self.records_by_locus == {}:\n",
      "            self.extract_by_locus\n",
      "        for key in self.records_by_locus.keys():\n",
      "            SeqIO.write(self.records_by_locus[key], key+'.'+format, format)\n",
      "            \n",
      "    def align(self, alignment_methods=[], pal2nal='./pal2nal.pl'):\n",
      "            seen_loci = []\n",
      "            for method in alignment_methods:\n",
      "                method.timeit.append(time.time())\n",
      "                method.platform = platform_report()\n",
      "                if method.program_name == 'muscle':\n",
      "                    method.platform.append('Program and version: '+os.popen(method.cmd + ' -version').read())\n",
      "                elif method.program_name == 'mafft':\n",
      "                    method.platform.append('Program and version: Get mafft to spit version to stdout')\n",
      "                for locus in method.loci:\n",
      "                    if locus.name in seen_loci:\n",
      "                        raise RuntimeError('locus '+locus.name+' is in more than one AlignmentMethod objects')\n",
      "                    else:\n",
      "                        seen_loci.append(locus.name)\n",
      "                    stdout, stderr = method.command_lines[locus.name]()\n",
      "                    align = AlignIO.read(StringIO(stdout), \"fasta\",  alphabet=IUPAC.protein)\n",
      "                    if method.CDSAlign and locus.feature_type == 'CDS':\n",
      "                        for seq in align:\n",
      "                            found = 0\n",
      "                            for s in method.CDS_in_frame[locus.name]:\n",
      "                                if s.id == seq.id:\n",
      "                                    found = 1\n",
      "                            if found == 0:\n",
      "                                raise RuntimeError(seq.id + ' is not in the CDS sequences')\n",
      "                        for s in method.CDS_in_frame[locus.name]:\n",
      "                            found = 0\n",
      "                            for seq in align:\n",
      "                                if s.id == seq.id:\n",
      "                                    found = 1\n",
      "                            if found == 0:\n",
      "                                raise RuntimeError(seq.id + ' is not in the protein sequences')\n",
      "                        for seq in method.CDS_in_frame[locus.name]:    \n",
      "                            for prot in align:\n",
      "                                if prot.id == seq.id:\n",
      "                                    i = 0\n",
      "                                    for p in str(prot.seq):\n",
      "                                        if not p == '-':\n",
      "                                            i += 1\n",
      "                                    if not i*3 == len(seq.seq):\n",
      "                                        raise RuntimeError('nuc and prot seqs have unmatched lengths for '+seq.id)\n",
      "                        aln_filename = method.id+'_'+locus.name+'.aln'\n",
      "                        AlignIO.write(align, aln_filename, 'fasta')\n",
      "                        cds_filename = method.id+'_CDS_in_frame_'+locus.name+'.fasta'\n",
      "                        stdout = os.popen('perl '+pal2nal+' '+aln_filename+' '+cds_filename + ' -nostderr').read()\n",
      "                        align = AlignIO.read(StringIO(stdout), \"clustal\",  alphabet=IUPAC.ambiguous_dna)\n",
      "                        #from Bio import CodonAlign\n",
      "                        #codon_aln = CodonAlign.build(align, method.CDS_in_frame[locus.name])\n",
      "                        #align = codon_aln\n",
      "                    method_files = glob.glob(method.id+'_*')\n",
      "                    self.alignments[locus.name] = align\n",
      "                method.timeit.append(time.time())\n",
      "                method.timeit.append(method.timeit[2]-method.timeit[1])\n",
      "                for f in method_files:\n",
      "                    os.remove(f)\n",
      "            self.used_methods += alignment_methods\n",
      "            \n",
      "    def write_alns(self, format = 'fasta'):\n",
      "        if len(self.alignments.keys()) == 0:\n",
      "            raise IOError('Align the records first')\n",
      "        else:\n",
      "            for key in self.alignments:\n",
      "                AlignIO.write(self.alignments[key], key+'_aln.'+format, format)\n",
      "                \n",
      "    def write_trimmed_alns(self, format = 'fasta'):\n",
      "        if len(self.trimmed_alignments.keys()) == 0:\n",
      "            raise IOError('Align and trimmed the records first')\n",
      "        else:\n",
      "            for key in self.trimmed_alignments.keys():\n",
      "                AlignIO.write(self.trimmed_alignments[key], key+'_trimmed_aln.'+format, format)\n",
      "            \n",
      "    def tree(self, raxml_methods):\n",
      "        # to do: determine the program used and the resulting expected tree file name\n",
      "        \n",
      "        for raxml_method in raxml_methods:\n",
      "            raxml_method.timeit.append(time.time())\n",
      "            raxml_method.platform = platform_report() \n",
      "            raxml_method.platform.append('Program and version: '+ raxml_method.cmd + ': ' +\n",
      "                                         os.popen(raxml_method.cmd + ' -version').readlines()[2])\n",
      "            for trimmed_alignment in raxml_method.command_lines.keys():\n",
      "                for cline in raxml_method.command_lines[trimmed_alignment]:\n",
      "                    stdout, stderr = cline()\n",
      "                t = None\n",
      "                if raxml_method.preset == 'fa':\n",
      "                    t = Tree('RAxML_bipartitions.'+raxml_method.id+'_'+trimmed_alignment+'0')\n",
      "                elif raxml_method.preset == 'fD_fb':\n",
      "                    t = Tree('RAxML_bipartitions.'+raxml_method.id+'_'+trimmed_alignment+'1')\n",
      "            \n",
      "                for n in t.traverse():\n",
      "                    n.add_feature('tree_method_id', str(raxml_method.id)+'_'+trimmed_alignment)\n",
      "                t.dist = 0\n",
      "                t.add_feature('tree_method_id', str(raxml_method.id)+'_'+trimmed_alignment)\n",
      "           \n",
      "                loci_names = [i.name for i in  self.loci]       \n",
      "                concat_names = [c.name for c in self.concatenations]\n",
      "                if trimmed_alignment in loci_names:\n",
      "                        \n",
      "                        for leaf in t:\n",
      "                            records = self.records\n",
      "                            feature = ''\n",
      "                            feature_source = ''\n",
      "                            record = ''\n",
      "                            for r in records:\n",
      "                                if r.id in leaf.name:\n",
      "                                    record = r\n",
      "                                    for f in r.features:\n",
      "                                        if f.type == 'source':\n",
      "                                            feature_source = f\n",
      "                                        elif f.qualifiers['feature_id'][0] == leaf.name:\n",
      "                                            feature = f\n",
      "                            for a in record.annotations.keys():\n",
      "                                label = 'annotation_'+a\n",
      "                                leaf.add_feature(label, record.annotations[a])\n",
      "                            for f_source_qual in feature_source.qualifiers.keys():\n",
      "                                label = 'source_'+f_source_qual\n",
      "                                leaf.add_feature(label, feature_source.qualifiers[f_source_qual][0])\n",
      "                            for f_qual in feature.qualifiers.keys():\n",
      "                                leaf.add_feature(f_qual, feature.qualifiers[f_qual][0])\n",
      "                        self.trees[trimmed_alignment] = [t,t.write(features=[])]\n",
      "                        \n",
      "                elif trimmed_alignment in concat_names:\n",
      "                        s = filter(lambda i: i.name == trimmed_alignment, self.concatenations)[0]\n",
      "                        for leaf in t:\n",
      "                            records = self.records\n",
      "                            feature = ''\n",
      "                            feature_source = ''\n",
      "                            record = ''\n",
      "                            for r in records:\n",
      "                                for feature in r.features:\n",
      "                                    if not feature.type == 'source':\n",
      "                                        qual_dict = get_qualifiers_dictionary(self, feature.qualifiers['feature_id'])\n",
      "                                        if s.otu_meta in qual_dict.keys() and qual_dict[s.otu_meta] == leaf.name:\n",
      "                                            for key in qual_dict.keys():\n",
      "                                                leaf.add_feature(key, qual_dict[key])\n",
      "                        self.trees[s.name] = [t,t.write(features=[])]\n",
      "            raxml_method.timeit.append(time.time())\n",
      "            raxml_method.timeit.append(raxml_method.timeit[2]-raxml_method.timeit[1])\n",
      "            for file_name in os.listdir(os.curdir):\n",
      "                        if raxml_method.id in file_name:\n",
      "                            os.remove(file_name)\n",
      "        self.used_methods += raxml_methods\n",
      "        \n",
      "    def clear_tree_annotations(self):\n",
      "        for tree in self.trees.keys():\n",
      "            t = Tree(self.trees[tree][1])\n",
      "            t.dist = 0\n",
      "            self.trees[tree][0] = t\n",
      "\n",
      "    def write_nexml(self, output_name):\n",
      "        D = dendropy.DataSet()\n",
      "        tree_list = []\n",
      "        \n",
      "        loci_names = []\n",
      "        for locus in self.loci:\n",
      "            loci_names.append(locus.name)\n",
      "        \n",
      "        for tree_name in self.trees.keys():\n",
      "            #get aligned and trimmd aligned sequences as leaf features\n",
      "            t = self.trees[tree_name][0]\n",
      "            for l in t:\n",
      "                aln_name = l.tree_method_id.split('_')[1]\n",
      "                \n",
      "                otu_feature = 'feature_id'\n",
      "                if aln_name in [c.name for c in self.concatenations]:\n",
      "                    for c in self.concatenations:\n",
      "                        if c.name == aln_name:\n",
      "                            otu_feature = c.otu_meta\n",
      "                            \n",
      "                if not aln_name in [c.name for c in self.concatenations]:\n",
      "                    leaf_feature_value = getattr(l, otu_feature)\n",
      "                    alignment = self.alignments[aln_name]\n",
      "                    for record in alignment:\n",
      "                        if record.id == leaf_feature_value:\n",
      "                            l.add_feature('aligned_sequence',str(record.seq))\n",
      "                    t_aln = db.trimmed_alignments[aln_name]\n",
      "                    \n",
      "                leaf_feature_value = getattr(l, otu_feature)\n",
      "                for record in t_aln:\n",
      "                    if record.id == leaf_feature_value:\n",
      "                        l.add_feature('aligned_trimmed_sequence',str(record.seq))\n",
      "                    \n",
      "            tree_string = self.trees[tree_name][0].write(features=[])\n",
      "            tree = dendropy.Tree()\n",
      "            tree.read_from_string(tree_string, schema='newick', extract_comment_metadata = True)\n",
      "            tree_list.append(tree)\n",
      "        TL = dendropy.TreeList(tree_list)    \n",
      "        D.add_tree_list(TL)\n",
      "        \n",
      "        #for aln_name in self.alignments.keys():\n",
      "        #    if aln_name in loci_names:\n",
      "        #        char_type = ''\n",
      "        #        matrix = None\n",
      "        #        for locus in self.loci:\n",
      "        #            if locus.name == aln_name:\n",
      "        #                char_type = locus.char_type\n",
      "        #        if char_type == 'dna':\n",
      "        #            matrix = dendropy.DnaCharacterMatrix()\n",
      "        #        elif char_type == 'prot':\n",
      "        #            matrix = dendropy.ProteinCharacterMatrix()\n",
      "        #        matrix_string = self.alignments[aln_name].format('fasta')\n",
      "        #        matrix.read_from_string(matrix_string,'fasta')\n",
      "        #        D.add_char_matrix(matrix)\n",
      "            \n",
      "        D.write_to_path(\n",
      "            output_name,\n",
      "            'nexml',\n",
      "            suppress_annotations=False,\n",
      "            annotations_as_nhx=False,\n",
      "            exclude_trees=False)\n",
      "\n",
      "\n",
      "\n",
      "    def annotate(self, fig_folder,\n",
      "    \n",
      "                 root_meta,\n",
      "                 root_value,\n",
      "    \n",
      "                 leaf_labels_txt_meta,\n",
      "                 leaf_node_color_meta=None,\n",
      "                 leaf_label_colors=None,\n",
      "    \n",
      "                 node_bg_meta=None,\n",
      "                 node_bg_color=None,\n",
      "                 \n",
      "                 node_support_dict=None,\n",
      "                 \n",
      "                 heat_map_meta = None, #list\n",
      "                 heat_map_colour_scheme=2,\n",
      "                 \n",
      "                 multifruc=None\n",
      "                 ): \n",
      "    \n",
      "            print '<html>'\n",
      "            ts = TreeStyle()\n",
      "            ts.show_leaf_name = False\n",
      "            ts.scale = 1000\n",
      "            if node_support_dict:\n",
      "                ts.legend_position=1\n",
      "                ts.legend.add_face(TextFace('Node support: ', fsize=10), column=0)\n",
      "                i = 1\n",
      "                for color in node_support_dict.keys():\n",
      "                    ts.legend.add_face(CircleFace(radius = 4, color = color), column=i)\n",
      "                    i +=1 \n",
      "                    ts.legend.add_face(TextFace(' '+str(node_support_dict[color][0])+'-'+str(node_support_dict[color][1]),\n",
      "                                                fsize=10), column=i)\n",
      "                    i += 1\n",
      "            \n",
      "            #if heat_map_meta:\n",
      "            #    ts.legend_position=1\n",
      "            #    ts.legend.add_face(TextFace('Heatmap columns: ', fsize=10), column=0)\n",
      "            #    i = 1\n",
      "            #    for meta in heat_map_meta:\n",
      "            #        ts.legend.add_face(TextFace(' '+str(meta)+', ',\n",
      "            #                                    fsize=20), column=i)\n",
      "            #        i += 1\n",
      "                \n",
      "            for tree in self.trees.keys():\n",
      "                                       \n",
      "                # set outgroup leaves, labels and label colors\n",
      "                outgroup_list = []\n",
      "                all_heatmap_profile_values = []\n",
      "                leaves_for_heatmap = []\n",
      "                \n",
      "                for leaf in self.trees[tree][0]:\n",
      "                    qualifiers_dictionary = get_qualifiers_dictionary(self, leaf.feature_id)\n",
      "                    leaf_label = ''\n",
      "                    for meta in leaf_labels_txt_meta:\n",
      "                        leaf_label += qualifiers_dictionary[meta]+' '\n",
      "                    leaf_label = leaf_label[:-1]\n",
      "                    fgcolor = 'black'\n",
      "                    if leaf_label_colors:\n",
      "                        for colour_name in leaf_label_colors.keys():\n",
      "                            if colour_name in qualifiers_dictionary[leaf_node_color_meta]:\n",
      "                                fgcolor = leaf_label_colors[colour_name]\n",
      "                    leaf_face = TextFace(leaf_label, fgcolor=fgcolor)\n",
      "                    leaf.add_face(leaf_face,0)\n",
      "                    if not root_value == 'mid' and root_value in qualifiers_dictionary[root_meta]:\n",
      "                        outgroup_list.append(leaf)\n",
      "                        \n",
      "                    if heat_map_meta:\n",
      "                        include = True\n",
      "                        for i in heat_map_meta:\n",
      "                            if not i in qualifiers_dictionary:\n",
      "                                include = False\n",
      "                        if include:\n",
      "                            profile = []\n",
      "                            deviation = []\n",
      "                            for meta in heat_map_meta:\n",
      "                                if meta in qualifiers_dictionary.keys():\n",
      "                                    profile.append(float(qualifiers_dictionary[meta]))\n",
      "                                    all_heatmap_profile_values.append(float(qualifiers_dictionary[meta]))\n",
      "                                    deviation.append(0.0)\n",
      "                            leaf.add_features(profile=profile)\n",
      "                            leaf.add_features(deviation=deviation)\n",
      "                            leaves_for_heatmap.append(leaf)\n",
      "                for leaf in leaves_for_heatmap:\n",
      "                    leaf.add_face(ProfileFace(max_v=float(max(all_heatmap_profile_values)),\n",
      "                                              min_v=float(min(all_heatmap_profile_values)), \n",
      "                                              center_v=float(float(max(all_heatmap_profile_values)+min(all_heatmap_profile_values))/2),\n",
      "                                              width=50, height=30,\n",
      "                                              style='heatmap',\n",
      "                                              colorscheme=heat_map_colour_scheme),\n",
      "                                    column=1, position=\"aligned\")\n",
      "                        \n",
      "                        \n",
      "                #set outgroup\n",
      "                if outgroup_list == ['mid']:\n",
      "                    try:\n",
      "                        R = self.trees[tree][0].get_midpoint_outgroup()\n",
      "                        self.trees[tree][0].set_outgroup(R)\n",
      "                        print 'rooting tree '+tree+' at midpoint'\n",
      "                    except:\n",
      "                        print 'root in '+tree+' already set correctly?'\n",
      "                    \n",
      "                elif len(outgroup_list) == 1:\n",
      "                    try:\n",
      "                        self.trees[tree][0].set_outgroup(outgroup_list[0])\n",
      "                    except:\n",
      "                        print 'root in '+tree+' already set correctly?'\n",
      "                elif len(outgroup_list) > 1:\n",
      "                    try:\n",
      "                        R = self.trees[tree][0].get_common_ancestor(outgroup_list)\n",
      "                        self.trees[tree][0].set_outgroup(R)\n",
      "                    except:\n",
      "                        print 'root in '+tree+' already set correctly?'\n",
      "                elif len(outgroup_list)==0:\n",
      "                    try:\n",
      "                        R = self.trees[tree][0].get_midpoint_outgroup()\n",
      "                        self.trees[tree][0].set_outgroup(R)\n",
      "                        print 'rooting tree '+tree+' at midpoint'\n",
      "                    except:\n",
      "                        print 'root in '+tree+' already set correctly?'\n",
      "    \n",
      "                # ladderize\n",
      "                self.trees[tree][0].ladderize()\n",
      "            \n",
      "                if multifruc:\n",
      "                    keep = []\n",
      "                    for n in self.trees[tree][0].traverse():\n",
      "                        if n.support < multifruc and not n.is_leaf():\n",
      "                            n.delete()\n",
      "    \n",
      "                # node bg colors\n",
      "                if node_bg_color:\n",
      "                    for key in node_bg_color.keys():\n",
      "                        for node in self.trees[tree][0].get_monophyletic(values=[key], target_attr=node_bg_meta):\n",
      "                            ns = NodeStyle(bgcolor=node_bg_color[key])\n",
      "                            node.set_style(ns)\n",
      "    \n",
      "                # node support\n",
      "                if node_support_dict:\n",
      "                    for node in self.trees[tree][0].traverse():\n",
      "                        for key in node_support_dict.keys():\n",
      "                            if (node.support <= node_support_dict[key][0] and\n",
      "                                node.support > node_support_dict[key][1]):\n",
      "                                node.add_face(CircleFace(radius = 5, color = key),column=0, position = \"float\")\n",
      "\n",
      "                    \n",
      "                        \n",
      "                    \n",
      "                self.trees[tree][0].render(fig_folder + \"/\"+self.trees[tree][0].get_leaves()[0].tree_method_id+'.png',w=1000, tree_style=ts)\n",
      "                print('<A href=file://'+\n",
      "                       fig_folder + \"/\" + self.trees[tree][0].get_leaves()[0].tree_method_id+'.png'+\n",
      "                       '>'+self.trees[tree][0].get_leaves()[0].tree_method_id+\n",
      "                       '</A><BR>')\n",
      "            print '</html>'\n",
      "            print fig_folder\n",
      "\n",
      "    def trim(self):\n",
      "        for aln in self.alignments.keys():\n",
      "            AlignIO.write(self.alignments[aln],aln+'_trimmed_aln.fasta','fasta')\n",
      "            stdout = os.popen('trimal -in '+ aln +'_trimmed_aln.fasta -gappyout').read()\n",
      "            align = AlignIO.read(StringIO(stdout), \"fasta\",  alphabet=IUPAC.ambiguous_dna)\n",
      "            for record in align:\n",
      "                record.description = ''\n",
      "            self.trimmed_alignments[aln] = align\n",
      "            \n",
      "            \n",
      "class AlignmentMethod:\n",
      "    \n",
      "    def __init__(self, db, method_name='MafftLinsi', CDSAlign=True, program_name='mafft',\n",
      "                 cmd='mafft', loci='all',\n",
      "                 cline_args=dict(localpair=True, maxiterate=1000)):\n",
      "        self.id = str(random.randint(10000,99999))+str(time.time())\n",
      "        self.method_name=method_name\n",
      "        self.CDSAlign=CDSAlign\n",
      "        self.program_name=program_name\n",
      "        self.loci = db.loci\n",
      "        if not loci == 'all':\n",
      "            self.loci = []\n",
      "            for locus_name in loci:\n",
      "                for locus in db.loci:\n",
      "                    if locus_name == locus.name:\n",
      "                        self.loci.append(locus)\n",
      "        self.CDS_proteins = {}\n",
      "        self.CDS_in_frame = {}\n",
      "        self.aln_input_strings = {}\n",
      "        self.command_lines = {}\n",
      "        self.timeit = [time.asctime()]\n",
      "        self.platform = []\n",
      "        self.cmd = cmd\n",
      "        # make defalut input files\n",
      "        if db.records_by_locus == {}:\n",
      "            db.extract_by_locus()\n",
      "        for key in db.records_by_locus.keys():\n",
      "            SeqIO.write(db.records_by_locus[key], self.id+'_'+key+'.fasta', 'fasta')    \n",
      "        for locus in self.loci:\n",
      "            # put default input file filename and string in the AlignmentMethod object\n",
      "            input_filename=self.id+'_'+locus.name+'.fasta'\n",
      "            self.aln_input_strings[locus.name] = [open(input_filename,'r').read()]\n",
      "            # If CDS prepare reference protein input file and in frame CDS input file\n",
      "            if locus.feature_type == 'CDS' and locus.char_type == 'dna' and self.CDSAlign: \n",
      "                self.CDS_proteins[locus.name] = []\n",
      "                self.CDS_in_frame[locus.name] = []\n",
      "                for record in db.records:\n",
      "                    for feature in record.features:\n",
      "                        if (not feature.type == 'source' and 'gene' in feature.qualifiers.keys() and\n",
      "                            feature.qualifiers['gene'][0] in locus.aliases):\n",
      "                            S = feature.extract(record.seq)\n",
      "                            # Make in-frame CDS input file seq start in frame\n",
      "                            if 'codon_start' in feature.qualifiers.keys():\n",
      "                                i = feature.qualifiers['codon_start'][0]\n",
      "                                if i > 1:\n",
      "                                    S = S[(int(i)-1):]\n",
      "                            # Make in-frame CDS input file seq end in frame\n",
      "                            if len(S)%3 == 1:\n",
      "                                S = S[:-1]\n",
      "                            elif len(S)%3 == 2:\n",
      "                                S = S[:-2]  \n",
      "                            # make protein input file seq\n",
      "                            P = Seq(feature.qualifiers['translation'][0], IUPAC.protein)\n",
      "                            # Remove 3' positions that are based on partial codons\n",
      "                            while len(P)*3 > len(S):\n",
      "                                P = P[:-1]\n",
      "                            # remove complete termination codon\n",
      "                            if (len(S)/3)-1 == len(P):\n",
      "                                S = S[:-3]\n",
      "                            # make in frame cds record\n",
      "                            feature_record = SeqRecord(seq = S, id = feature.qualifiers['feature_id'][0],\n",
      "                                                               description = '')\n",
      "                            # put it in the object\n",
      "                            self.CDS_in_frame[locus.name].append(feature_record)\n",
      "                            # make protein record\n",
      "                            feature_record = SeqRecord(seq = P, id = feature.qualifiers['feature_id'][0],\n",
      "                                                       description = '')\n",
      "                            # Put the protein records in the AlignmentMethod object\n",
      "                            self.CDS_proteins[locus.name].append(feature_record)\n",
      "                           \n",
      "                                    \n",
      "                # check same number of prot and cds objects                    \n",
      "                if len(db.records_by_locus[locus.name]) > len(self.CDS_proteins[locus.name]):\n",
      "                    raise RuntimeError('For the CDS locus '+locus.name+': more nuc seqs than prot seqs.'+\n",
      "                                       ' You may miss a \\'translate\\' or \\'gene\\' qualifier in some of '+\n",
      "                                       'the features.')\n",
      "                elif len(db.records_by_locus[locus.name]) < len(self.CDS_proteins[locus.name]):\n",
      "                    raise RuntimeError('For the CDS locus '+locus.name+': less nuc seqs than prot seqs.'+\n",
      "                                       ' You may miss a \\'translate\\' or \\'gene\\' qualifier in some of '+\n",
      "                                       'the features.')\n",
      "                unmatched = []\n",
      "                for record in self.CDS_in_frame[locus.name]:\n",
      "                    for prot in self.CDS_proteins[locus.name]:\n",
      "                        if prot.id == record.id:\n",
      "                            if not len(prot.seq)*3 == len(record.seq):\n",
      "                                unmatched.append(record.id)\n",
      "                unmatched_string = ''\n",
      "                if len(unmatched) > 0:\n",
      "                    for u in unmatched:\n",
      "                        unmatched_string += u+' '\n",
      "                    raise RuntimeError('The following CDS/protein pairs are unmatched: '+unmatched_string)\n",
      "                    \n",
      "                SeqIO.write(self.CDS_in_frame[locus.name],\n",
      "                            self.id+'_CDS_in_frame_'+locus.name+'.fasta', 'fasta')\n",
      "                input_filename2=self.id+'_CDS_in_frame_'+locus.name+'.fasta'\n",
      "                SeqIO.write(self.CDS_proteins[locus.name],\n",
      "                            self.id+'_CDS_proteins_'+locus.name+'.fasta', 'fasta')\n",
      "                input_filename=self.id+'_CDS_proteins_'+locus.name+'.fasta'\n",
      "                self.aln_input_strings[locus.name][0] = [open(input_filename,'r').read(),\n",
      "                                                         open(input_filename2,'r').read()]\n",
      "            cline = dict(dict(input=input_filename), **cline_args)\n",
      "            if self.program_name == 'mafft':\n",
      "                self.command_lines[locus.name] = MafftCommandline(cmd=cmd)\n",
      "            elif self.program_name == 'muscle':\n",
      "                self.command_lines[locus.name] = MuscleCommandline(cmd=cmd)\n",
      "            for c in cline.keys():\n",
      "                self.command_lines[locus.name].__setattr__(c,cline[c])\n",
      "            print str(self.command_lines[locus.name])\n",
      "\n",
      "\n",
      "\n",
      "def use_sh_support_as_branch_support(tree_filename):\n",
      "    string = open(tree_filename,'r').read()\n",
      "    string = re.sub(r'\\[',r'[&&NHX:support=',string)\n",
      "    t = Tree(string)\n",
      "    t.dist=0\n",
      "    t.write(outfile=tree_filename)\n",
      "    #t.show()\n",
      "    \n",
      "def transfer_support_same_topo(tree_file_with_support,\n",
      "                               tree_file_without_support):\n",
      "    supported = Tree(tree_file_with_support) \n",
      "    unsupported = Tree(tree_file_without_support)\n",
      "    supported_leaf_names = sorted(supported.get_leaf_names())\n",
      "    unsupported_leaf_names = sorted(unsupported.get_leaf_names())\n",
      "    if not len(unsupported_leaf_names) == len(supported_leaf_names):\n",
      "        raise IOError(tree_file_with_support + ' and ' + tree_file_without_support +\n",
      "                      ' are not the same length')\n",
      "    for i in range(len(supported_leaf_names)):\n",
      "        if not supported_leaf_names[i] == unsupported_leaf_names[i]:\n",
      "            raise IOError('The trees do not share all leaves or leaf names')\n",
      "    same_root = supported.get_leaf_names()[0]\n",
      "    unsupported.set_outgroup(same_root)\n",
      "    supported.set_outgroup(same_root)\n",
      "    for ns in supported.traverse():\n",
      "        ns_leaves = ns.get_leaf_names()\n",
      "        if not unsupported.check_monophyly(values=ns_leaves, target_attr=\"name\"):\n",
      "            raise RuntimeError('trees do not share topology and/or all the leaf names')\n",
      "        else:\n",
      "            unsupported_ancestor = unsupported.get_common_ancestor(ns_leaves)\n",
      "            unsupported_ancestor.support = ns.support\n",
      "    unsupported.write(outfile = tree_file_without_support)    \n",
      "    \n",
      "    \n",
      "def make_raxml_partfile(tree_method, db, trimmed_alignment_name):\n",
      "\n",
      "    concatenation = None\n",
      "    for c in db.concatenations:\n",
      "        if c.name == trimmed_alignment_name:\n",
      "            concatenation = c\n",
      "    \n",
      "    #concatenation = filter(lambda concatenation: concatenation.name == trimmed_alignment_name, db.concatenations)[0]\n",
      "\n",
      "    model = []\n",
      "    for locus in concatenation.loci:\n",
      "        if locus.char_type == 'prot':\n",
      "            m = None\n",
      "            if isinstance(tree_method.matrix,dict):\n",
      "                m = tree_method.matrix[locus.name]\n",
      "            elif isinstance(tree_method.matrix,str):\n",
      "                m = tree_method.matrix\n",
      "            else:\n",
      "                #todo write error\n",
      "                pass\n",
      "            length = len(db.trimmed_alignments[locus.name][0])\n",
      "            model.append([m,locus.name,length])\n",
      "        elif locus.char_type == 'dna':\n",
      "            length = len(db.trimmed_alignments[locus.name][0])\n",
      "            model.append(['DNA',locus.name,length])\n",
      "                    \n",
      "    # make partition file\n",
      "                    \n",
      "    partfile = open(tree_method.id+'_'+concatenation.name+'_partfile','wt')\n",
      "    i = 1\n",
      "    for m in model:\n",
      "        partfile.write(m[0]+', '+m[1]+'='+str(i)+'-'+str(m[2]+i-1)+'\\n')\n",
      "        i += m[2]\n",
      "    partfile.close()\n",
      "    return tree_method.id+'_'+concatenation.name+'_partfile'\n",
      "\n",
      "def make_raxml_input_matrix_file(tree_method, trimmed_alignment_name):\n",
      "    SeqIO.write(tree_method.trimmed_alignments[trimmed_alignment_name],\n",
      "                tree_method.id+'_'+trimmed_alignment_name+'.fasta','fasta')\n",
      "    return tree_method.id+'_'+trimmed_alignment_name+'.fasta'\n",
      "\n",
      "def write_raxml_clines(tree_method, db, trimmed_alignment_name):\n",
      "            \n",
      "    cline_que = 0\n",
      "\n",
      "    support_replicates = 100\n",
      "    ML_replicates = 1\n",
      "    if '-N' in tree_method.cline_args.keys():\n",
      "        ML_replicates = tree_method.cline_args['-N']\n",
      "    elif '-#' in tree_method.cline_args.keys():\n",
      "        support_replicates = tree_method.cline_args['-#']\n",
      "    \n",
      "    partfile = None\n",
      "    \n",
      "    # Check if it is a concatenation and make partfile\n",
      "\n",
      "    for c in db.concatenations:\n",
      "        if c.name == trimmed_alignment_name:\n",
      "            partfile = make_raxml_partfile(tree_method, db, trimmed_alignment_name)\n",
      "    \n",
      "    input_filename = make_raxml_input_matrix_file(tree_method, trimmed_alignment_name)\n",
      "    model = tree_method.model\n",
      "    try:\n",
      "        locus_char_type = filter(lambda locus: locus.name == trimmed_alignment_name, db.loci)[0].char_type\n",
      "    except:\n",
      "        locus_char_type = 'prot'\n",
      "    \n",
      "    if partfile:\n",
      "        model='PROT'+model+'JTT'\n",
      "    else:\n",
      "        if locus_char_type == 'dna':\n",
      "            model = 'GTR'+tree_method.model\n",
      "        elif  locus_char_type == 'prot':\n",
      "            if isinstance(tree_method.matrix,str):\n",
      "                model = 'PROT'+tree_method.model+tree_method.matrix\n",
      "            elif isinstance(tree_method.matrix,dict):\n",
      "                model = 'PROT'+tree_method.model+tree_method.matrix[trimmed_alignment_name]\n",
      "        \n",
      "    presets = {'fa': [{'-f': 'a',\n",
      "                           '-p': random.randint(0,999),\n",
      "                           '-x':  random.randint(0,999),\n",
      "                           '-s': input_filename,\n",
      "                           '-N': support_replicates,\n",
      "                           '-n': tree_method.id+'_'+trimmed_alignment_name+'0',\n",
      "                           '-m': model,\n",
      "                           '-T': tree_method.threads}],\n",
      "                'fD_fb':[{'-f': 'D',\n",
      "                          '-p': random.randint(0,999),\n",
      "                          '-s': input_filename,\n",
      "                          '-N': ML_replicates,\n",
      "                           '-n': tree_method.id+'_'+trimmed_alignment_name+'0',\n",
      "                           '-m': model,\n",
      "                           '-T': tree_method.threads},{'-f': 'b',\n",
      "                                                       '-p': random.randint(0,999),\n",
      "                                                       '-s': input_filename,\n",
      "                                                       '-n': tree_method.id+'_'+trimmed_alignment_name+'1',\n",
      "                                                       '-m': model,\n",
      "                                                       '-T': tree_method.threads,\n",
      "                                                       '-t': 'RAxML_bestTree.'+tree_method.id+'_'+trimmed_alignment_name+'0',\n",
      "                                                       '-z': 'RAxML_rellBootstrap.'+tree_method.id+'_'+trimmed_alignment_name+'0'\n",
      "                                                       }\n",
      "                         ]\n",
      "                }\n",
      "\n",
      "    if partfile:\n",
      "        for preset in presets.keys():\n",
      "            for cline in range(len(presets[preset])):\n",
      "                presets[preset][cline] = dict({'-q': partfile}, **presets[preset][cline])               \n",
      "    return presets[tree_method.preset] \n",
      "\n",
      "class RaxmlTreeReconstructionMethod:\n",
      "    \n",
      "    \n",
      "    def __init__(self, db, method_name='fa', program_name='raxmlHPC-PTHREADS-SSE3',\n",
      "                 cmd='raxmlHPC-PTHREADS-SSE3', preset = 'fa', alns='all', model='GAMMA', matrix='JTT', threads=4,\n",
      "                 cline_args={}):\n",
      "        self.id = str(random.randint(10000,99999))+str(time.time())\n",
      "        self.method_name=method_name\n",
      "        self.program_name=program_name\n",
      "        self.preset = preset\n",
      "        self.cline_args = cline_args\n",
      "        self.model = model\n",
      "        self.matrix = matrix\n",
      "        self.threads = threads\n",
      "        self.trimmed_alignments = db.trimmed_alignments\n",
      "        if not alns == 'all':\n",
      "            self.trimmed_alignments = {}\n",
      "            for aln_name in alns:\n",
      "                if aln_name in db.trimmed_alignments.keys():\n",
      "                    self.trimmed_alignments[aln_name] = db.trimmed_alignments[aln_name]\n",
      "        self.aln_input_strings = {}\n",
      "        self.command_lines = {}\n",
      "        self.timeit = [time.asctime()]\n",
      "        self.platform = []\n",
      "        self.cmd = cmd\n",
      "        \n",
      "        for trimmed_alignment in self.trimmed_alignments.keys():\n",
      "            self.command_lines[trimmed_alignment] = []\n",
      "            command_lines = write_raxml_clines(self, db, trimmed_alignment)\n",
      "            for command_line in command_lines:\n",
      "                cline_object = RaxmlCommandline(cmd=cmd)\n",
      "                for c in command_line.keys():\n",
      "                    cline_object.__setattr__(c,command_line[c])\n",
      "                self.command_lines[trimmed_alignment].append(cline_object)\n",
      "                print str(cline_object)\n",
      "\n",
      "\n",
      "\n",
      "from pylab import *\n",
      "import random\n",
      "\n",
      "def draw_boxplot(dictionary, y_axis_label): #'locus':[values]\n",
      "    import numpy as np\n",
      "    import matplotlib.pyplot as plt\n",
      "    items = dictionary.items()\n",
      "    items.sort()\n",
      "    \n",
      "    data = [locus[1] for locus in items]\n",
      "        \n",
      "    fig, ax1 = plt.subplots(figsize=(4,6))\n",
      "    #plt.subplots_adjust(left=0.075, right=0.95, top=0.9, bottom=0.25)\n",
      "\n",
      "    #bp = plt.boxplot(data, widths=0.75, patch_artist=True)\n",
      "    bp = plt.boxplot(data, patch_artist=True)\n",
      "    \n",
      "    for box in bp['boxes']:\n",
      "    # change outline color\n",
      "        box.set( color='black', linewidth=1)\n",
      "        \n",
      "    # change fill color\n",
      "        box.set( facecolor = 'red', alpha=0.85 )\n",
      "        \n",
      "    # change color, linestyle and linewidth of the whiskers\n",
      "    for whisker in bp['whiskers']:\n",
      "        whisker.set(color='gray', linestyle='solid', linewidth=2.0)\n",
      "\n",
      "    # change color and linewidth of the caps\n",
      "    for cap in bp['caps']:\n",
      "        cap.set(color='gray', linewidth=2.0)\n",
      "\n",
      "    # change color and linewidth of the medians\n",
      "    for median in bp['medians']:\n",
      "        #median.set(color='#b2df8a', linewidth=2)\n",
      "        median.set(color='white', linewidth=2)\n",
      "\n",
      "    # change the style of fliers and their fill\n",
      "    for flier in bp['fliers']:\n",
      "        flier.set(marker='o', color='#e7298a', alpha=0.5)\n",
      "    \n",
      "    # Add a light horizontal grid to the plot\n",
      "    ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',\n",
      "              alpha=0.7)\n",
      "    \n",
      "    # Hide these grid behind plot objects\n",
      "    ax1.set_axisbelow(True)\n",
      "    \n",
      "    #set title and axis labels\n",
      "    #ax1.set_title('Sequence length distribution per locus\\n', size=18)\n",
      "    \n",
      "    xlabels = [locus[0] for locus in items]\n",
      "    xticks(range(len(data)+1)[1:], xlabels, size=14, rotation='vertical')\n",
      "    subplots_adjust(left=0.3)\n",
      "    \n",
      "    ax1.set_ylabel(y_axis_label, size=18)\n",
      "    \n",
      "    name = str(random.randint(1000,2000))\n",
      "    fig.savefig(name+'.png')\n",
      "    return name+'.png'\n",
      "    \n",
      "def report_methods(db, figs_folder):\n",
      "        report_lines = ['<html>','<head>','<h1>']\n",
      "    \n",
      "        head = 'reprophylo analysis from '+str(time.asctime())\n",
      "        #=====================================================\n",
      "        report_lines.append(head)\n",
      "        report_lines += ['</h1>','</head>','<body>','']\n",
      "        \n",
      "        report_lines += ['<h2>','Data','</h2>', '']\n",
      "        \n",
      "        title = 'species representation in sequence data'.title()\n",
      "        report_lines += ('<h3>', title, '</h3>', '', '<pre>')\n",
      "        #--------------------------------------------------------\n",
      "        \n",
      "        outfile_name= str(random.randint(1000,2000))\n",
      "        db.species_vs_loci(outfile_name)\n",
      "        with open(outfile_name, 'rb') as csvfile:\n",
      "            sp_vs_lc = list(csv.reader(csvfile, delimiter='\\t', quotechar='|'))\n",
      "            field_sizes = []\n",
      "            for i in range(len(sp_vs_lc[0])):\n",
      "                lengths = []\n",
      "                for row in sp_vs_lc:\n",
      "                    lengths.append(len(row[i]))\n",
      "                field_sizes.append(max(lengths))\n",
      "            for row in sp_vs_lc:\n",
      "                string = ''\n",
      "                for i in range(len(row)):\n",
      "                    string += row[i].ljust(field_sizes[i]+3)\n",
      "                report_lines.append(string)\n",
      "        \n",
      "        os.remove(outfile_name)\n",
      "        \n",
      "        if len(db.records_by_locus.keys())>0:\n",
      "            lengths_dict = {}\n",
      "            for locus_name in db.records_by_locus.keys():\n",
      "                lengths_dict[locus_name] = []\n",
      "                for record in db.records_by_locus[locus_name]:\n",
      "                    lengths_dict[locus_name].append(len(record.seq))\n",
      "            fig_filename = draw_boxplot(lengths_dict, 'Seq length (bp)')\n",
      "            title = 'Distribution of sequence lengths'\n",
      "            report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "            if os.path.isfile(fig_filename):\n",
      "                data_uri = open(fig_filename, 'rb').read().encode('base64').replace('\\n', '')\n",
      "                img_tag = '<img height=600 src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                report_lines.append(img_tag)\n",
      "                os.remove(fig_filename)\n",
      "            \n",
      "            for stat in ('GC_content', 'nuc_degen_prop', 'prot_degen_prop'):\n",
      "                stat_dict = {}\n",
      "                ylabel = 'GC ontent (%)'\n",
      "                if not stat == 'GC_content':\n",
      "                    ylabel = 'Aambiguous positions (prop)'\n",
      "                for locus_name in db.records_by_locus.keys():\n",
      "                    stat_dict[locus_name] = []\n",
      "                    for i in db.records_by_locus[locus_name]:\n",
      "                        for record in db.records:\n",
      "                            for feature in record.features:\n",
      "                                if feature.qualifiers['feature_id'][0] == i.id:\n",
      "                                    if stat in feature.qualifiers.keys():\n",
      "                                        stat_dict[locus_name].append(float(feature.qualifiers[stat][0]))\n",
      "                fig_filename = draw_boxplot(stat_dict, ylabel)\n",
      "                title = 'Distribution of sequence statistic \\\"'+stat+'\\\"'\n",
      "                report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "                if os.path.isfile(fig_filename):\n",
      "                    data_uri = open(fig_filename, 'rb').read().encode('base64').replace('\\n', '')\n",
      "                    img_tag = '<img height=600 src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                    report_lines.append(img_tag)\n",
      "                    os.remove(fig_filename)\n",
      "\n",
      "        \n",
      "        \n",
      "        for c in db.concatenations:\n",
      "            title = ('content of concatenation \\\"' + c.name + '\\\"').title()\n",
      "            report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "            \n",
      "            report_lines.append('Rules for  \\\"' + c.name + '\\\":')\n",
      "            rule_1 = 'OTUs must have the loci: '\n",
      "            for locus in c.concat_must_have_all_of:\n",
      "                rule_1 += locus + ', '\n",
      "            report_lines.append(rule_1)\n",
      "            rule_2 = 'OTUs must have at least one of each group: '\n",
      "            for group in c.concat_must_have_one_of:\n",
      "                rule_2 += str(group) +', '\n",
      "            report_lines += (rule_2, '')\n",
      "            \n",
      "            \n",
      "            otus = c.feature_id_dict.keys()\n",
      "            loci = [locus.name for locus in c.loci]\n",
      "            \n",
      "            otus_max_length = max([len(i) for i in otus])+33\n",
      "            loci_columns_max_length = []\n",
      "            \n",
      "            for locus in loci:\n",
      "                lengths = [len(locus)]\n",
      "                for otu in otus:\n",
      "                    if locus in c.feature_id_dict[otu].keys():\n",
      "                        lengths.append(len(c.feature_id_dict[otu][locus]))\n",
      "                    else:\n",
      "                        lengths.append(0)\n",
      "                loci_columns_max_length.append(max(lengths)+3)\n",
      "                \n",
      "            concat_header = ''.ljust(otus_max_length)\n",
      "            for i in range(len(loci)):\n",
      "                concat_header += loci[i].ljust(loci_columns_max_length[i])\n",
      "            report_lines += (concat_header, '~'*len(concat_header))\n",
      "                \n",
      "            for otu in otus:\n",
      "                otu_species = ''\n",
      "                for locus in loci:\n",
      "                    if locus in c.feature_id_dict[otu].keys():\n",
      "                        feature_qualifiers = get_qualifiers_dictionary(db, c.feature_id_dict[otu][locus])\n",
      "                        if 'source_organism' in feature_qualifiers.keys():\n",
      "                            otu_species = feature_qualifiers['source_organism']\n",
      "                    \n",
      "                concat_line = (otu+' '+otu_species).ljust(otus_max_length)\n",
      "                for i in range(len(loci)):\n",
      "                    if loci[i] in c.feature_id_dict[otu].keys():\n",
      "                        concat_line += c.feature_id_dict[otu][loci[i]].ljust(loci_columns_max_length[i])\n",
      "                    else:\n",
      "                        concat_line += ''.ljust(loci_columns_max_length[i])\n",
      "                report_lines.append(concat_line)\n",
      "\n",
      "        report_lines += ['</pre>', '<h2>','Methods','</h2>', '<pre>', '']\n",
      "        \n",
      "        for method in db.used_methods:\n",
      "            \n",
      "            if isinstance(method, AlignmentMethod):\n",
      "                title = 'Seuqence Alignment Method \\\"'+method.method_name+'\\\", method ID: '+method.id\n",
      "                report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "                #--------------------------------------------------------\n",
      "                align_line = 'Included loci :'\n",
      "                for locus in [locus.name for locus in method.loci]:\n",
      "                    align_line += locus + ', '\n",
      "                report_lines.append(align_line)\n",
      "                report_lines.append('Total execution time: '+str(method.timeit[3])+' sec\\'')\n",
      "                report_lines.append('Performed on: '+str(method.timeit[0]))\n",
      "                report_lines += method.platform\n",
      "                report_lines.append('')\n",
      "\n",
      "                report_lines.append('Command lines:')\n",
      "                for cline in method.command_lines.keys():\n",
      "                    report_lines.append('Alignment \\\"'+cline+'\\\":')\n",
      "                    report_lines.append('<pre style=\"white-space:normal;\">')\n",
      "                    report_lines.append(str(method.command_lines[cline]))\n",
      "                    report_lines.append('</pre>')\n",
      "                    \n",
      "            elif isinstance(method, RaxmlTreeReconstructionMethod):\n",
      "                title = 'Raxml Tree Reconstruction Method \\\"'+method.method_name+'\\\", method ID: '+method.id\n",
      "                report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "                #--------------------------------------------------------\n",
      "                tree_line = 'Included alignments :'\n",
      "                for aln in method.trimmed_alignments.keys():\n",
      "                    tree_line += aln + ', '\n",
      "                report_lines.append(tree_line)\n",
      "                report_lines.append('Total execution time: '+str(method.timeit[3])+' sec\\'')\n",
      "                report_lines.append('Performed on: '+str(method.timeit[0]))\n",
      "                report_lines += method.platform\n",
      "                report_lines.append('')\n",
      "\n",
      "                report_lines.append('Command lines:')\n",
      "                for aln in method.command_lines.keys():\n",
      "                    report_lines.append('Alignment \\\"'+aln+'\\\":')\n",
      "                    for cline in method.command_lines[aln]:\n",
      "                        report_lines.append('<pre style=\"white-space:normal;\">')\n",
      "                        report_lines.append(str(cline))\n",
      "                        report_lines.append('</pre>')\n",
      "                    report_lines.append('')\n",
      "                \n",
      "        \n",
      "        \n",
      "        report_lines += ('</pre>', '','') \n",
      "        \n",
      "        report_lines += ('<h1>Trees</h1>','')\n",
      "        \n",
      "        for tree in db.trees.keys():\n",
      "            report_lines += ('<h2>'+tree+'</h2>','<pre style=\"white-space:normal;\">','Tree Method ID: '+db.trees[tree][0].get_leaves()[0].tree_method_id,'</pre>')\n",
      "            \n",
      "            report_lines += ('<h3>newick format</h3>','','<pre style=\"white-space:normal;\">',db.trees[tree][0].write(),'</pre>','')\n",
      "            report_lines += ('<h3>nhx format</h3>','','<pre>',db.trees[tree][1],'</pre>','','','','')\n",
      "            \n",
      "            \n",
      "            \n",
      "            if os.path.isfile(figs_folder+'/'+db.trees[tree][0].get_leaves()[0].tree_method_id+'.png'):\n",
      "                data_uri = open(figs_folder+'/'+db.trees[tree][0].get_leaves()[0].tree_method_id+'.png', 'rb').read().encode('base64').replace('\\n', '')\n",
      "                img_tag = '<img width=500 src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                report_lines.append(img_tag)\n",
      "\n",
      "        \n",
      "        \n",
      "        report_lines.append('</body>')\n",
      "        report_lines.append('</html>')\n",
      "            \n",
      "        return report_lines\n",
      "\n",
      "\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "!python -m doctest -v  reprophylo.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Trying:\r\n",
        "    locus = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    print locus\r\n",
        "Expecting:\r\n",
        "    Locus(char_type=dna, feature_type=CDS, name=coi, aliases=cox1; COX1; coi; COI; CoI)\r\n",
        "**********************************************************************\r\n",
        "File \"reprophylo.py\", line 30, in reprophylo.Locus.__init__\r\n",
        "Failed example:\r\n",
        "    print locus\r\n",
        "Exception raised:\r\n",
        "    Traceback (most recent call last):\r\n",
        "      File \"/usr/lib/python2.7/doctest.py\", line 1289, in __run\r\n",
        "        compileflags, 1) in test.globs\r\n",
        "      File \"<doctest reprophylo.Locus.__init__[1]>\", line 1\r\n",
        "        print locus\r\n",
        "                  ^\r\n",
        "    SyntaxError: invalid syntax\r\n",
        "Trying:\r\n",
        "    coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    location = FeatureLocation(1,100)\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    kept_feature = SeqFeature()\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    kept_feature.location = location\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    kept_feature.type = 'CDS'\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    kept_feature.qualifiers['gene'] = ['CoI']\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    dwindled_feature = SeqFeature()\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    dwindled_feature.location = location\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    dwindled_feature.type = 'rRNA'\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    dwindled_feature.qualifiers['gene'] = ['LSU']\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    s = 'atgc'*1000\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    kept_record = SeqRecord(seq=Seq(s, IUPAC.ambiguous_dna), id='1', description='spam')\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    kept_record.features.append(kept_feature)\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    kept_record.features.append(dwindled_feature)\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    print len(kept_record.features)\r\n",
        "Expecting:\r\n",
        "    2\r\n",
        "**********************************************************************\r\n",
        "File \"reprophylo.py\", line 164, in reprophylo.dwindle_record\r\n",
        "Failed example:\r\n",
        "    print len(kept_record.features)\r\n",
        "Exception raised:\r\n",
        "    Traceback (most recent call last):\r\n",
        "      File \"/usr/lib/python2.7/doctest.py\", line 1289, in __run\r\n",
        "        compileflags, 1) in test.globs\r\n",
        "      File \"<doctest reprophylo.dwindle_record[14]>\", line 1\r\n",
        "        print len(kept_record.features)\r\n",
        "                ^\r\n",
        "    SyntaxError: invalid syntax\r\n",
        "Trying:\r\n",
        "    a = dwindle_record(kept_record, [coi])\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    print len(kept_record.features)\r\n",
        "Expecting:\r\n",
        "    1\r\n",
        "**********************************************************************\r\n",
        "File \"reprophylo.py\", line 167, in reprophylo.dwindle_record\r\n",
        "Failed example:\r\n",
        "    print len(kept_record.features)\r\n",
        "Exception raised:\r\n",
        "    Traceback (most recent call last):\r\n",
        "      File \"/usr/lib/python2.7/doctest.py\", line 1289, in __run\r\n",
        "        compileflags, 1) in test.globs\r\n",
        "      File \"<doctest reprophylo.dwindle_record[16]>\", line 1\r\n",
        "        print len(kept_record.features)\r\n",
        "                ^\r\n",
        "    SyntaxError: invalid syntax\r\n",
        "Trying:\r\n",
        "    coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    location = FeatureLocation(1,100)\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    feature = SeqFeature()\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    feature.location = location\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    feature.type = 'CDS'\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    feature.qualifiers['gene'] = ['CoI']\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    a = keep_feature(feature, [coi])\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    print a\r\n",
        "Expecting:\r\n",
        "    True\r\n",
        "**********************************************************************\r\n",
        "File \"reprophylo.py\", line 123, in reprophylo.keep_feature\r\n",
        "Failed example:\r\n",
        "    print a\r\n",
        "Exception raised:\r\n",
        "    Traceback (most recent call last):\r\n",
        "      File \"/usr/lib/python2.7/doctest.py\", line 1289, in __run\r\n",
        "        compileflags, 1) in test.globs\r\n",
        "      File \"<doctest reprophylo.keep_feature[7]>\", line 1\r\n",
        "        print a\r\n",
        "              ^\r\n",
        "    SyntaxError: invalid syntax\r\n",
        "Trying:\r\n",
        "    L = ['a','b','b']\r\n",
        "Expecting nothing\r\n",
        "ok\r\n",
        "Trying:\r\n",
        "    print list_to_string(L)\r\n",
        "Expecting:\r\n",
        "    a;b;b\r\n",
        "**********************************************************************\r\n",
        "File \"reprophylo.py\", line 248, in reprophylo.list_to_string\r\n",
        "Failed example:\r\n",
        "    print list_to_string(L)\r\n",
        "Exception raised:\r\n",
        "    Traceback (most recent call last):\r\n",
        "      File \"/usr/lib/python2.7/doctest.py\", line 1289, in __run\r\n",
        "        compileflags, 1) in test.globs\r\n",
        "      File \"<doctest reprophylo.list_to_string[1]>\", line 1\r\n",
        "        print list_to_string(L)\r\n",
        "                           ^\r\n",
        "    SyntaxError: invalid syntax\r\n",
        "48 items had no tests:\r\n",
        "    reprophylo\r\n",
        "    reprophylo.AlignmentMethod\r\n",
        "    reprophylo.AlignmentMethod.__init__\r\n",
        "    reprophylo.Concatenation\r\n",
        "    reprophylo.Concatenation.__init__\r\n",
        "    reprophylo.Database\r\n",
        "    reprophylo.Database.__init__\r\n",
        "    reprophylo.Database.add_concatenation\r\n",
        "    reprophylo.Database.add_feature_to_record\r\n",
        "    reprophylo.Database.add_qualifier\r\n",
        "    reprophylo.Database.add_qualifier_from_source\r\n",
        "    reprophylo.Database.align\r\n",
        "    reprophylo.Database.annotate\r\n",
        "    reprophylo.Database.clear_tree_annotations\r\n",
        "    reprophylo.Database.copy_paste_from_features_to_source\r\n",
        "    reprophylo.Database.copy_paste_within_feature\r\n",
        "    reprophylo.Database.extract_by_locus\r\n",
        "    reprophylo.Database.if_this_then_that\r\n",
        "    reprophylo.Database.make_concatenation_alignments\r\n",
        "    reprophylo.Database.read_denovo\r\n",
        "    reprophylo.Database.read_embl_genbank\r\n",
        "    reprophylo.Database.species_vs_loci\r\n",
        "    reprophylo.Database.tree\r\n",
        "    reprophylo.Database.trim\r\n",
        "    reprophylo.Database.write\r\n",
        "    reprophylo.Database.write_alns\r\n",
        "    reprophylo.Database.write_by_locus\r\n",
        "    reprophylo.Database.write_nexml\r\n",
        "    reprophylo.Database.write_trimmed_alns\r\n",
        "    reprophylo.Locus\r\n",
        "    reprophylo.Locus.__str__\r\n",
        "    reprophylo.RaxmlTreeReconstructionMethod\r\n",
        "    reprophylo.RaxmlTreeReconstructionMethod.__init__\r\n",
        "    reprophylo.draw_boxplot\r\n",
        "    reprophylo.generate_pickle_filename\r\n",
        "    reprophylo.get_qualifiers_dictionary\r\n",
        "    reprophylo.is_embl_or_gb\r\n",
        "    reprophylo.lines_to_line\r\n",
        "    reprophylo.make_raxml_input_matrix_file\r\n",
        "    reprophylo.make_raxml_partfile\r\n",
        "    reprophylo.parse_input\r\n",
        "    reprophylo.platform_report\r\n",
        "    reprophylo.report_methods\r\n",
        "    reprophylo.seq_format_from_suffix\r\n",
        "    reprophylo.transfer_support_same_topo\r\n",
        "    reprophylo.type_to_single_line_str\r\n",
        "    reprophylo.use_sh_support_as_branch_support\r\n",
        "    reprophylo.write_raxml_clines\r\n",
        "**********************************************************************\r\n",
        "4 items had failures:\r\n",
        "   1 of   2 in reprophylo.Locus.__init__\r\n",
        "   2 of  17 in reprophylo.dwindle_record\r\n",
        "   1 of   8 in reprophylo.keep_feature\r\n",
        "   1 of   2 in reprophylo.list_to_string\r\n",
        "29 tests in 52 items.\r\n",
        "24 passed and 5 failed.\r\n",
        "***Test Failed*** 5 failures.\r\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "# create html table from csv\n",
      "# Author(s): Chris Trombley <ctroms@gmail.com>\n",
      "# Version 2 - added css class to all columns except header\n",
      "\n",
      "#!/usr/bin/python\n",
      "# create html table from csv\n",
      "\n",
      "import sys\n",
      "import csv\n",
      "\n",
      "\n",
      "# Open the CSV file for reading\n",
      "reader = csv.reader(open('Tetillidae_genes_per_species.csv'), delimiter='\\t')\n",
      "\n",
      "# Create the HTML file for output\n",
      "htmlfile = open('Tetillidae_genes_per_species.csv.html',\"w\")\n",
      "\n",
      "# initialize rownum variable\n",
      "rownum = 0\n",
      "\n",
      "# write <table> tag\n",
      "htmlfile.write('<table>')\n",
      "\n",
      "# generate table contents\n",
      "for row in reader: # Read a single row from the CSV file\n",
      "\n",
      "\t# write header row. assumes first row in csv contains header\n",
      "\tif rownum == 0:\n",
      "\t\thtmlfile.write('<tr>') # write <tr> tag\n",
      "  \t\tfor column in row:\n",
      "  \t\t\thtmlfile.write('<th>' + column + '</th>')\n",
      "  \t\thtmlfile.write('</tr>')\n",
      "\n",
      "  \t#write all other rows\t\n",
      "  \telse:\n",
      "  \t\thtmlfile.write('<tr>')\t\n",
      "  \t\tfor column in row:\n",
      "  \t\t\thtmlfile.write('<td>' + column + '</td>')\n",
      "  \t\thtmlfile.write('</tr>')\n",
      "\t\n",
      "\t#increment row count\t\n",
      "\trownum += 1\n",
      "\n",
      "# write </table> tag\n",
      "htmlfile.write('</table>')\n",
      "\n",
      "# print results to shell\n",
      "print \"Created \" + str(rownum) + \" row table.\"\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Created 41 row table.\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def random_rgb():\n",
      "    import random\n",
      "    r = lambda: random.randint(0,255)\n",
      "    return ('#%02X%02X%02X' % (r(),r(),r()))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print random_rgb()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#302792\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tree reconstruction tools in reprophylo\n",
      "I think that a tool that will simplify the RAxML command line while providing all the possible options will be more appreciated as a starting point than a small range of programs for which you have to write the command line from scratch. In addition, I think that instead of TreeReconstructionMethod object, as I did for alignments, I should make program specific object, ie, RaxmlMethodObject. The reason for this is that the user would usually want to fit the best alignment approach for each gene, but will want to use a uniform tree reconstruction method ( or several of them, but on all the datasets for each method).  \n",
      "  \n",
      "I plan to simplify the raxml command line by excluding keywards that are not related to tree reconstruction because other operations do not make sense, coming after sequence alignment in the reprophylo workflow (everything will be accesible, but not everything will be bundled into predesigned analyses)  \n",
      "The bundles will be either 'tree reconstruction' bundles, 'node support' bundels and 'both' bundles. It will be possible to add optional keywords to the bundes. In the description below I don't show all the keywards that will be included. e.g., -m (model) is always required and I don't write it here.\n",
      "\n",
      "\n",
      "\n",
      "# RAxML keyword blocks\n",
      "  \n",
      "## ML search combined with branch support\n",
      "*This section include one liners or a two liners that would be meaningless when ran individually*  \n",
      "\n",
      "#### Single best tree search With rappid bootstrap\n",
      "-f a -p 12345 -x 12345 -N (number or AUTO_MRE)  \n",
      "#### Multipe best tree searches With REL bootstrap\n",
      "-f D -p 12345 -N (number)  \n",
      "**then**  \n",
      "-f b -t RAxML_bestTree.name -z RAxML_rellBootstrap.name -m (model)  \n",
      "#### Fast tree with sh-like support and branch length\n",
      "-f F -p 12345  \n",
      "**then**  \n",
      "-f J -p 12345 -t RAxML_fastTree  \n",
      "*comment: if not followed by bootstrap analysis, do  use_sh_support_as_branch_support*\n",
      "#### Very fast tree with sh-like support and branch length\n",
      "-f E -p 12345  \n",
      "**then**  \n",
      "-f J -p 12345 -t RAxML_fastTree  \n",
      "*comment: if not followed by bootstrap analysis, do  use_sh_support_as_branch_support*  \n",
      "\n",
      "## Best tree search approaches\n",
      "*In liners that focus on thorogh best tree search, possibly with more than on randomized parsimony/ completely random starting tree*  \n",
      "  \n",
      "#### Best tree default method approach\n",
      "-f d -p 12345 -N (number)  \n",
      "#### Slow best tree search with better likelihood\n",
      "-f o -p 12345 -N (number)  \n",
      "  \n",
      "## Bipartition tree calculation approaches (calc supports and put on best tree)\n",
      "*Methods which focus on branch support calculation and their superimposition on a precalculated best tree or on an nni optimized fast tree with branch-lengths*  \n",
      "  \n",
      "#### Rapid bootstrap\n",
      "-x 12345 -N (number/autoMRE)  \n",
      "**then**  \n",
      "-f b -t RAxML_bestTree/RAxML_fastTreeSH_Support -z RAxML_Bootstrap -m (model)  \n",
      "####  Thorough bootstrap\n",
      "-b 12345 -N (number/autoMRE)  \n",
      "**then**  \n",
      "-f b -t RAxML_bestTree/RAxML_fastTreeSH_Support -z RAxML_Bootstrap -m (model)  \n",
      "  \n",
      "####  IC/TC approach\n",
      "*This approach is based on the level of incongruence between the concatenated tree and the gene trees. I tries replace bootstrap support or posterior probabilities which are inefficient when a lot of data is involved. \n",
      "Salichos and Rokas 2013 http://www.ncbi.nlm.nih.gov/pubmed/23657258  \n",
      "and http://mbe.oxfordjournals.org/content/early/2014/02/07/molbev.msu061.abstract*  \n",
      "  \n",
      "*Technically, from the reprophylo point of view, it differs from other approaches by the fact that it integrates all the gene trees and a concatenated tree, while other approches are performed indipendantly on each alignment. So it needs to pull all the trees calculated at one point and placed in the reprophylo database and intgrate them.*\n",
      "\n",
      "*The analysis is done by calling **-f i** and providing the concat tree to **-t** and the gene trees to **-z**. This will produce the concat tree with the IC and ICA scores as follows **(A,B):0.1325[IC,ICA]**. The scores will then need to be reformated as node features as I have done for the sh-like supports. There are many modifiers to this approach which I have to check carefully in the RAxML manual*  \n",
      "  \n",
      "## Parameter optimization appraches\n",
      "#### optimize br-len and other model parameters\n",
      "-f e -t (RAxML_fastTreeSH_Support or RAxML_fastTree or RAxML_bestTree)  \n",
      "*Comment: This will requlire the use of transfer_support_same_topo on the input and output trees*  \n",
      "  \n",
      "# General analysis modifiers\n",
      "*I excluded keywards that don't make sense in the tree reconstruction phase of reprophylo*  \n",
      "  \n",
      "A-scondary structure model, together with S  \n",
      "B-specify threshold for autoMR  \n",
      "c-specifiy num of CAT categories  \n",
      "d-start at random instead of parsimony randomized tree  \n",
      "D-ML search convergence criterion to stop the analysis (ever needed in raxml?)  \n",
      "e-parameter percision criterion  \n",
      "fI-ML based descision on midpoint for a tree passed with -t    \n",
      "ft-another way to search for a tree, not so sure  \n",
      "fT-optimization via through SPR moves  \n",
      "F-Just CAT, no GAMMA at all  \n",
      "g-constraint tree  \n",
      "H-No so sure  \n",
      "m-models, will need to be read from a control file   \n",
      "M-per partition branch length  \n",
      "n-derived from method.id  \n",
      "O-disable check of undetermined alignments  \n",
      "p-random seed for parsimony. set automatically  \n",
      "P-Used provided AA substitution model  \n",
      "q-partition file, created automatically  \n",
      "r-constraint tree, binary  \n",
      "s-input alignment  \n",
      "S-secondary structue  \n",
      "t-tree file  \n",
      "T-num of cores  \n",
      "u-use median instead of mean for GAMMA based rates  \n",
      "z-pass trees file  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}