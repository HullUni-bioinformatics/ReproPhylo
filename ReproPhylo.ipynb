{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext watermark"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%watermark -g -h -m -v -p ete2,biopython,dendropy,cloud,numpy,matplotlib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPython 2.7.6\n",
        "IPython 1.2.1\n",
        "\n",
        "ete2 2.2rev1056\n",
        "biopython 1.64\n",
        "dendropy 3.12.0\n",
        "cloud 2.8.5\n",
        "numpy 1.8.2\n",
        "matplotlib 1.3.1\n",
        "\n",
        "compiler   : GCC 4.8.2\n",
        "system     : Linux\n",
        "release    : 3.13.0-36-generic\n",
        "machine    : x86_64\n",
        "processor  : x86_64\n",
        "CPU cores  : 8\n",
        "interpreter: 64bit\n",
        "host name  : amir-TECRA-W50-A\n",
        "Git hash   : aa0f118a7bb5ed880a175c6e1c5ee358f84378a6\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file reprophylo.py\n",
      "\n",
      "##############################################################################################\n",
      "if False:\n",
      "    \"\"\"\n",
      "    ReproPhylo version 0.1 \n",
      "    \n",
      "    General purpose phylogenetics package for reproducible and experimental analysis\n",
      "    \n",
      "    Amir Szitenebrg\n",
      "    A.Szitenberg@Hull.ac.uk\n",
      "    Szitenberg@gmail.com\n",
      "    \n",
      "    David H Lunt\n",
      "    D.H.Lunt@Hull.ac.uk\n",
      "    \n",
      "    EvoHull.org\n",
      "    University of Hull\n",
      "    \n",
      "    \n",
      "    Developed with:\n",
      "    CPython 2.7.6\n",
      "    IPython 1.2.1\n",
      "    ete2 2.2rev1056\n",
      "    biopython 1.64\n",
      "    dendropy 3.12.0\n",
      "    cloud 2.8.5\n",
      "    numpy 1.8.2\n",
      "    matplotlib 1.3.1\n",
      "    \n",
      "    RAxML 8\n",
      "    Phylobayes\n",
      "    Trimal\n",
      "    Muscle\n",
      "    Mafft\n",
      "    Pal2nal\n",
      "    \"\"\"\n",
      "##############################################################################################\n",
      "\n",
      "\n",
      "\n",
      "from Bio import SeqIO\n",
      "import os, csv, sys, dendropy, re, time, random, glob, platform, warnings, rpgit, ast\n",
      "import subprocess as sub\n",
      "#import cloud.serialization.cloudpickle as pickle\n",
      "from Bio.Seq import Seq\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from Bio.Alphabet import IUPAC\n",
      "from Bio.SeqRecord import SeqRecord\n",
      "from Bio.SeqFeature import SeqFeature, FeatureLocation, CompoundLocation\n",
      "from Bio.Align.Applications import MafftCommandline, MuscleCommandline\n",
      "from StringIO import StringIO \n",
      "from Bio import AlignIO \n",
      "from Bio.Phylo.Applications import RaxmlCommandline\n",
      "from Bio.Align import MultipleSeqAlignment\n",
      "from Bio.SeqUtils import GC\n",
      "from ete2 import *\n",
      "import __builtin__\n",
      "\n",
      "\n",
      "\n",
      "##############################################################################################\n",
      "if False:\n",
      "    \"\"\"Tools for loci explorations in a GenBank File\"\"\"\n",
      "##############################################################################################\n",
      "\n",
      "\n",
      "\n",
      "def list_loci_in_genbank(genbank_filename, control_filename, loci_report = None):\n",
      "    \n",
      "   stdout = sys.stdout\n",
      "   if  loci_report: \n",
      "        sys.stdout = open(loci_report, 'w')\n",
      "    \n",
      "   genbank_synonyms = ast.literal_eval(open('genbank_synonyms','r').read())\n",
      "    \n",
      "   # Open GenBank file\n",
      "   MelPCgenes = open(genbank_filename, 'rU')\n",
      "   \n",
      "   gene_dict = {} #set up a gene_dict dictionary\n",
      "   \n",
      "   # For each record\n",
      "   for record in SeqIO.parse(MelPCgenes, 'genbank') :\n",
      "   \n",
      "      # Grab the entire sequence\n",
      "      #seq = str(record.seq)  ## what is this actually used for? Nothing seems to happen on disabling it\n",
      "   \n",
      "      # Look at all features for this record\n",
      "      for feature in record.features:\n",
      "         \n",
      "         # If it's a CDS or rRNA...\n",
      "         if feature.type == 'CDS' or feature.type == 'rRNA':\n",
      "   \n",
      "            # If it contains some attribute called 'gene' save that\n",
      "            if 'gene' in feature.qualifiers:\n",
      "               geneName = feature.qualifiers['gene'][0]\n",
      "               geneName.replace(',',';')\n",
      "               if feature.type+','+geneName in gene_dict:\n",
      "                   gene_dict[feature.type+','+geneName]+=1\n",
      "               else:    \n",
      "                   gene_dict[feature.type+','+geneName]=1\n",
      "               #print(geneName)\n",
      "               \n",
      "            # Else if it contains some attribute called 'product' save that instead\n",
      "            elif 'product' in feature.qualifiers:\n",
      "               geneName = feature.qualifiers['product'][0]\n",
      "               geneName.replace(',',';')\n",
      "               if feature.type+','+geneName in gene_dict:\n",
      "                   gene_dict[feature.type+','+geneName]+=1\n",
      "               else:    \n",
      "                   gene_dict[feature.type+','+geneName]=1\n",
      "               #print(geneName)\n",
      "               \n",
      "            # Otherwise, quit.\n",
      "            else:\n",
      "               print 'ERROR when parsing feature: could not find either gene or product'\n",
      "               print feature.qualifiers\n",
      "               quit()\n",
      "   #print(gene_dict)\n",
      "       \n",
      "   #sorting happens via a list\n",
      "   \n",
      "   sorted_gene_names = gene_dict.items()\n",
      "   sorted_gene_names.sort(key = lambda i: i[0].lower())\n",
      "   control_file_lines = {}\n",
      "   \n",
      "   \n",
      "   print('\\n' + \"There are \" + str(len(sorted_gene_names)) + \" gene names (or gene product names) detected\")\n",
      "   print(\"----------------------------------\")\n",
      "   print(\"Gene and count sorted by gene name\")\n",
      "   print(\"----------------------------------\")\n",
      "   \n",
      "   for key, value in sorted_gene_names:\n",
      "           #print key, value\n",
      "           print(str(value) +\" instances of \" + key)\n",
      "           feature_type = key.split(',')[0]\n",
      "           alias = key.split(',')[1]\n",
      "           gen_group = None\n",
      "           for group in genbank_synonyms:\n",
      "               if alias in group:\n",
      "                   gen_group = group\n",
      "           if gen_group:\n",
      "               if gen_group[0] in control_file_lines.keys():\n",
      "                   control_file_lines[gen_group[0]].append(alias)\n",
      "               else:\n",
      "                   control_file_lines[gen_group[0]] = [feature_type, alias]\n",
      "           else:\n",
      "               name = alias.replace(' ','_')\n",
      "               control_file_lines[name] = [feature_type, alias]\n",
      "                    \n",
      "   control_file_handle = open(control_filename, 'wt')\n",
      "   for line in sort(control_file_lines.keys()):  \n",
      "       control_file_handle.write('dna,%s,%s'%(control_file_lines[line][0],line))\n",
      "       for a in control_file_lines[line][1:]: \n",
      "          control_file_handle.write(',%s'%a)\n",
      "       control_file_handle.write('\\n')\n",
      "                            \n",
      "   control_file_handle.close()                 \n",
      "   \n",
      "   print(\"-------------------------------\")\n",
      "   print(\"Gene and count sorted by counts\")\n",
      "   print(\"-------------------------------\")\n",
      "   sorted_gene_names.sort(key = lambda i: int(i[1]), reverse=True)\n",
      "   for key, value in sorted_gene_names:\n",
      "           #print key, value\n",
      "           print(str(value) +\" instances of \" + key)\n",
      "   sys.stdout = stdout\n",
      "   \n",
      "                    \n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##############################################################################################\n",
      "class Locus:\n",
      "##############################################################################################\n",
      "\n",
      "    \"\"\" Configure the loci stored in the ReproPhylo DB.\n",
      "        \n",
      "    >>> locus = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "    >>> print(locus)\n",
      "    Locus(char_type=dna, feature_type=CDS, name=coi, aliases=cox1; COX1; coi; COI; CoI)\n",
      "    \"\"\"\n",
      "\n",
      "    char_type = 'NotSet'\n",
      "    feature_type = 'NotSet'\n",
      "    name = 'NotSet'\n",
      "    aliases = []\n",
      "\n",
      "    def __init__(self, char_type=char_type, feature_type=feature_type,\n",
      "                 name=name, aliases=aliases):\n",
      "\n",
      "        \n",
      "        self.char_type = char_type\n",
      "        self.feature_type = feature_type\n",
      "        self.name = name\n",
      "        self.aliases = aliases\n",
      "        \n",
      "        valid = ['dna','prot']\n",
      "        if not self.char_type in valid:\n",
      "            raise ValueError('self.char_type should be \\'dna\\' or \\'prot\\'')\n",
      "        if not type(self.feature_type) is str:\n",
      "            raise ValueError('self.feature_type should be a string')\n",
      "        if not type(self.name) is str:\n",
      "            raise ValueError('self.name should be a string')\n",
      "        if not type(self.aliases) is list:\n",
      "            raise ValueError('self.aliases should be a list')\n",
      "        else:\n",
      "            for a in self.aliases:\n",
      "                if not type(a) is str:\n",
      "                    raise ValueError('aliases in self.aliases have to be strings')\n",
      "            \n",
      "\n",
      "\n",
      "    def __str__(self):\n",
      "        aliases_str = ('; ').join(self.aliases)\n",
      "        return ('Locus(char_type='+self.char_type+', feature_type='+self.feature_type+\n",
      "                ', name='+self.name+', aliases='+aliases_str+')')\n",
      "\n",
      "    \n",
      "    \n",
      "##############################################################################################\n",
      "class Concatenation:\n",
      "##############################################################################################\n",
      "\n",
      "    \"\"\"This class is used to configure concatenations given loci and rules.\n",
      "    \n",
      "    >>> coi = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "    >>> ssu = Locus('dna', 'rRNA', '18S', ['18S rRNA','SSU rRNA'])\n",
      "    >>> bssu = Locus('dna', 'rRNA', '16S', ['16S rRNA'])\n",
      "    >>> lsu = Locus('dna', 'rRNA', '28S', ['28S rRNA', 'LSU rRNA'])\n",
      "    >>> alg11 = Locus('dna', 'CDS', 'ALG11', ['ALG11'])\n",
      "    >>> loci = [coi, ssu, bssu, lsu, alg11]\n",
      "    >>> concatenation = Concatenation(name='combined', loci=loci,\n",
      "    ...                                otu_meta='OTU_name',\n",
      "    ...                                concat_must_have_all_of=['coi'],\n",
      "    ...                                concat_must_have_one_of =[['16S','28S'],['ALG11','18S']],\n",
      "    ...                                define_trimmed_alns=[\"MuscleDefaults@dummyTrimMethod\"])\n",
      "    >>> print(str(concatenation))\n",
      "    Concatenation named combined, with loci coi,18S,16S,28S,ALG11,\n",
      "    of which coi must exist for all species\n",
      "    and at least one of each group of [ 16S 28S ][ ALG11 18S ] is represented.\n",
      "    Alignments with the following names: MuscleDefaults@dummyTrimMethod are prefered\n",
      "    \"\"\"\n",
      "    \n",
      "    name = 'NotSet'\n",
      "    loci = []\n",
      "    otu_meta = 'NotSet'\n",
      "    concat_must_have_all_of = []\n",
      "    concat_must_have_one_of = []\n",
      "    define_trimmed_alns = [] #should be Locus_name@Alignment_method_name@Trimming_mathod_name\n",
      "    \n",
      "    feature_id_dict = {}\n",
      "    \n",
      "    def __init__(self,\n",
      "                 name = name,\n",
      "                 loci = loci,\n",
      "                 otu_meta = otu_meta,\n",
      "                 concat_must_have_all_of = concat_must_have_all_of,\n",
      "                 concat_must_have_one_of = concat_must_have_one_of,\n",
      "                 define_trimmed_alns = define_trimmed_alns):\n",
      "        self.name = name\n",
      "        self.loci = loci\n",
      "        self.otu_meta = otu_meta\n",
      "        self.concat_must_have_all_of = concat_must_have_all_of\n",
      "        self.concat_must_have_one_of = concat_must_have_one_of\n",
      "        self.feature_id_dict = {}\n",
      "        self.define_trimmed_alns = define_trimmed_alns\n",
      "        self.used_trimmed_alns = {}\n",
      "        seen = []\n",
      "        for locus in loci:\n",
      "            if not isinstance(locus, Locus):\n",
      "                raise TypeError(\"Expecting Locus object in loci list\")\n",
      "            if locus.name in seen:\n",
      "                raise NameError('Locus ' + locus.name + ' apears more than once in self.loci')\n",
      "            else:\n",
      "                seen.append(locus.name)\n",
      "      \n",
      "                \n",
      "                \n",
      "    def __str__(self):\n",
      "        loci_names = [i.name for i in self.loci]\n",
      "        loci_string = ''\n",
      "        for l in loci_names:\n",
      "            loci_string += l+','\n",
      "        loci_string = loci_string[:-1]\n",
      "        must_have = ''\n",
      "        for i in self.concat_must_have_all_of:\n",
      "            must_have += i+','\n",
      "        must_have = must_have[:-1]\n",
      "        trimmed_alignmnets_spec = ''\n",
      "        one_of = ''\n",
      "        for i in self.concat_must_have_one_of:\n",
      "            one_of += '[ '\n",
      "            for j in i:\n",
      "                one_of += j+' '\n",
      "            one_of += ']'\n",
      "        if (self.define_trimmed_alns) > 0:\n",
      "            for i in self.define_trimmed_alns:\n",
      "                trimmed_alignmnets_spec += i\n",
      "        return (\"Concatenation named %s, with loci %s,\\n\"\n",
      "                \"of which %s must exist for all species\\n\"\n",
      "                \"and at least one of each group of %s is represented.\\n\"\n",
      "                \"Alignments with the following names: %s are prefered\"\n",
      "                % (self.name, loci_string, must_have, one_of, trimmed_alignmnets_spec))\n",
      "        \n",
      "        \n",
      "        \n",
      "##############################################################################################\n",
      "if False:\n",
      "    \"\"\"\n",
      "    Reprophylo Database Utilities\n",
      "    \n",
      "    Used in the Database class but are not in the classes methods\n",
      "    \"\"\"\n",
      "##############################################################################################\n",
      "\n",
      "\n",
      "\n",
      "__builtin__.git = False\n",
      "\n",
      "\n",
      "\n",
      "def start_git():\n",
      "    __builtin__.git = True\n",
      "    rpgit.gitInit()\n",
      "    cwd = os.getcwd()\n",
      "    import fnmatch\n",
      "    matches = []\n",
      "    for root, dirnames, filenames in os.walk(cwd):\n",
      "        for filename in fnmatch.filter(filenames, '*.py'):\n",
      "            matches.append(os.path.join(root, filename))\n",
      "        for filename in fnmatch.filter(filenames, '*.ipynb'):\n",
      "            matches.append(os.path.join(root, filename))\n",
      "    for match in matches:\n",
      "        rpgit.gitAdd(match)\n",
      "    comment = \"%i script file(s) from %s\" % (len(matches), time.asctime())\n",
      "    rpgit.gitCommit(comment)\n",
      "    \n",
      "    \n",
      "    \n",
      "def stop_git():\n",
      "    __builtin__.git = False\n",
      "    cwd = os.getcwd()\n",
      "    import fnmatch\n",
      "    matches = []\n",
      "    for root, dirnames, filenames in os.walk(cwd):\n",
      "        for filename in fnmatch.filter(filenames, '*.py'):\n",
      "            matches.append(os.path.join(root, filename))\n",
      "        for filename in fnmatch.filter(filenames, '*.ipynb'):\n",
      "            matches.append(os.path.join(root, filename))\n",
      "    for match in matches:\n",
      "        rpgit.gitAdd(match)\n",
      "    comment = \"%i script file(s) from %s\" % (len(matches), time.asctime())\n",
      "    rpgit.gitCommit(comment)\n",
      "    \n",
      "\n",
      "\n",
      "def platform_report():\n",
      "    \n",
      "    \"\"\" \n",
      "    Prints machine specs, os specs and dependencies at time of execution\n",
      "    \n",
      "    >>> isinstance(platform_report(), list)\n",
      "    True\n",
      "    \"\"\"\n",
      "    import pkg_resources\n",
      "    modules = []\n",
      "    for i in ('ete2','biopython','dendropy','cloud'):\n",
      "        modules.append(i+' version: '+\n",
      "                            pkg_resources.get_distribution(i).version)\n",
      "    return(['Platform: '+platform.platform(aliased=0, terse=0),\n",
      "            'Processor: '+platform.processor(),\n",
      "            'Python build: '+platform.python_build()[0] + platform.python_build()[1],\n",
      "            'Python compiler: '+platform.python_compiler(),\n",
      "            'Python implementation: ' +platform.python_implementation(),\n",
      "            'Python version: ' + platform.python_version()]+\n",
      "             modules+\n",
      "            ['User: ' +platform.uname()[1]])\n",
      "\n",
      "\n",
      "\n",
      "def write_alns(db, format = 'fasta'):\n",
      "    \"\"\"\n",
      "    Writes untrimmed sequence alignment files that are in db in a biopython format\n",
      "    \"\"\"\n",
      "    \n",
      "    if len(db.alignments.keys()) == 0:\n",
      "        raise IOError('Align the records first')\n",
      "    else:\n",
      "        for key in db.alignments:\n",
      "            AlignIO.write(db.alignments[key], key+'_aln.'+format, format)\n",
      "\n",
      "\n",
      "\n",
      "def keep_feature(feature, loci):\n",
      "    \n",
      "    \"\"\" Returns true if a feature's type is in one of the loci and if the gene\n",
      "    or product qualifiers is in the aliases of one of the loci\n",
      "    \n",
      "    # making a dummy feature\n",
      "    >>> coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "    >>> location = FeatureLocation(1,100)\n",
      "    >>> feature = SeqFeature()\n",
      "    >>> feature.location = location\n",
      "    >>> feature.type = 'CDS'\n",
      "    >>> feature.qualifiers['gene'] = ['CoI']\n",
      "    \n",
      "    # testing if fits any of the Database Locus objects\n",
      "    >>> a = keep_feature(feature, [coi])\n",
      "    >>> print(a)\n",
      "    True\"\"\"\n",
      "    \n",
      "    keep = 0\n",
      "    for g in loci:\n",
      "        if not g.name in g.aliases:\n",
      "            g.aliases.append(g.name)\n",
      "        if feature.type == 'source':\n",
      "            keep = 1\n",
      "        elif feature.type == g.feature_type:\n",
      "            qaul = False\n",
      "            if 'gene' in feature.qualifiers.keys():\n",
      "                qual = 'gene'\n",
      "            elif 'product' in feature.qualifiers.keys():\n",
      "                qual = 'product'\n",
      "            if not qual == False and feature.qualifiers[qual][0] in g.aliases:\n",
      "                keep = 1\n",
      "    if keep == 1:\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "  \n",
      "\n",
      "\n",
      "def dwindle_record(record, loci):\n",
      "    \n",
      "    \"\"\" \n",
      "    Retains only features that are called by Locus objects and records with features that are\n",
      "    called by Locus objects\n",
      "    \n",
      "    # Making a dummy locus    \n",
      "    >>> coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "    \n",
      "    # Making a dummy record with a feature that fits a Locus object (kept_feature)\n",
      "    # and a feature that does not (dwindled_feature)\n",
      "    >>> location = FeatureLocation(1,100)\n",
      "    >>> kept_feature = SeqFeature()\n",
      "    >>> kept_feature.location = location\n",
      "    >>> kept_feature.type = 'CDS'\n",
      "    >>> kept_feature.qualifiers['gene'] = ['CoI']\n",
      "    >>> dwindled_feature = SeqFeature()\n",
      "    >>> dwindled_feature.location = location\n",
      "    >>> dwindled_feature.type = 'rRNA'\n",
      "    >>> dwindled_feature.qualifiers['gene'] = ['LSU']\n",
      "    >>> s = 'atgc'*1000\n",
      "    >>> record = SeqRecord(seq=Seq(s, IUPAC.ambiguous_dna), id='1', description='spam')\n",
      "    >>> record.features.append(kept_feature)\n",
      "    >>> record.features.append(dwindled_feature)\n",
      "    >>> print(len(record.features))\n",
      "    2\n",
      "    \n",
      "    # Dwindling the record\n",
      "    >>> a = dwindle_record(record, [coi])\n",
      "    >>> print(len(record.features))\n",
      "    1\n",
      "    \"\"\"\n",
      "    \n",
      "    dwindled_features = []\n",
      "    feature_count = 0\n",
      "    for feature in record.features:\n",
      "        if keep_feature(feature, loci)== True:\n",
      "            if feature.type == 'source' and not 'feature_id' in feature.qualifiers.keys():\n",
      "                feature.qualifiers['feature_id'] = [record.id + '_source']\n",
      "            elif not 'feature_id' in feature.qualifiers.keys():\n",
      "                feature.qualifiers['feature_id'] = [record.id + '_f' + str(feature_count)]\n",
      "                feature_count += 1\n",
      "            if not feature.type == 'source':\n",
      "                feature_seq = feature.extract(record.seq)\n",
      "                degen = len(feature_seq)\n",
      "                for i in ['A','T','G','C','U','a','t','g','c','u']:\n",
      "                    degen -= feature_seq.count(i)\n",
      "                feature.qualifiers['GC_content'] = [str(GC(feature_seq))]\n",
      "                feature.qualifiers['nuc_degen_prop'] = [str(float(degen)/len(feature_seq))]\n",
      "                if 'translation' in feature.qualifiers.keys():\n",
      "                    transl = feature.qualifiers['translation'][0]\n",
      "                    degen = 0\n",
      "                    for i in ['B', 'X', 'Z', 'b', 'x', 'z']:\n",
      "                        degen += transl.count(i)\n",
      "                    feature.qualifiers['prot_degen_prop'] = [str(float(degen)/len(transl))]                    \n",
      "            dwindled_features.append(feature)\n",
      "    record.features = dwindled_features\n",
      "    return record\n",
      "            \n",
      " \n",
      "    \n",
      "def is_embl_or_gb(input_filename):\n",
      "    suffixes = ['.gb','.embl']\n",
      "    gb = False\n",
      "    for s in suffixes:\n",
      "        if s in input_filename:\n",
      "            gb = True\n",
      "    return gb\n",
      "\n",
      "\n",
      "\n",
      "def parse_input(input_filename, fmt):\n",
      "    return SeqIO.parse(input_filename, fmt)\n",
      "\n",
      "\n",
      "\n",
      "def list_to_string(List):\n",
      "    \n",
      "    \"\"\"\n",
      "    Handles list printing as a nice string in the db.write(format=\"csv\") method\n",
      "    \n",
      "    >>> L = ['a','b','b']\n",
      "    >>> print(list_to_string(L))\n",
      "    a;b;b\n",
      "    \"\"\"\n",
      "    \n",
      "    string = ''\n",
      "    for i in List:\n",
      "        if type(i) is str and '\\n' in i:\n",
      "            string += lines_to_line(i).rstrip()+';'\n",
      "        else:\n",
      "            string += str(i)+';'\n",
      "    return string[:-1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def lines_to_line(lines):\n",
      "    \n",
      "    \"\"\"\n",
      "    Replaces newline with space in the db.write(format=\"csv\") method\n",
      "    \"\"\"\n",
      "    \n",
      "    lines = lines.split('\\n')\n",
      "    return (' ').join(lines)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def type_to_single_line_str(var):\n",
      "    \n",
      "    \"\"\"\n",
      "    Returns any type as a one line string for the db.write(format=\"csv\") method\n",
      "    \"\"\"\n",
      "    \n",
      "    if type(var) is str and '\\n' in var:\n",
      "        return lines_to_line(var)\n",
      "    elif type(var) is str or type(var) is int or type(var) is float:\n",
      "        return str(var)\n",
      "    elif type(var) is list and len(var) == 1:\n",
      "        return str(var[0])\n",
      "    elif type(var) is list and len(var) > 0:\n",
      "        return list_to_string(var)\n",
      "    else:\n",
      "        return var\n",
      "\n",
      "\n",
      "\n",
      "def get_qualifiers_dictionary(database, feature_id):\n",
      "    \n",
      "    \"\"\"\n",
      "    Takes sequence record annotation, source qualifiers and feature qualifiers and puts them\n",
      "    in a flat dictionary\n",
      "    \n",
      "        \n",
      "    # Making a dummy locus    \n",
      "    >>> coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "    \n",
      "    # Making a dummy Database\n",
      "    >>> db = Database([coi])\n",
      "    \n",
      "    # making a dummy record\n",
      "    >>> s = 'atgc'*1000\n",
      "    >>> location = FeatureLocation(1,100)\n",
      "    >>> feature = SeqFeature()\n",
      "    >>> feature.location = location\n",
      "    >>> feature.type = 'CDS'\n",
      "    >>> feature.qualifiers['gene'] = ['CoI']\n",
      "    >>> feature.qualifiers['feature_id'] = ['12345']\n",
      "    >>> source = SeqFeature()\n",
      "    >>> source.location = FeatureLocation(0,3999)\n",
      "    >>> source.type = 'source'\n",
      "    >>> source.qualifiers['organism'] = ['Tetillda radiata']\n",
      "    >>> record = SeqRecord(seq=Seq(s, IUPAC.ambiguous_dna), id='1', description='spam')\n",
      "    >>> record.features.append(feature)\n",
      "    >>> record.features.append(source)\n",
      "    >>> record.annotations[\"evidence\"] = 'made up'\n",
      "    >>> db.records = [record]\n",
      "    \n",
      "    # executing get_qualifiers_dictionary()\n",
      "    >>> qual_dict = get_qualifiers_dictionary(db, '12345')\n",
      "    >>> qual_items = qual_dict.items()\n",
      "    >>> qual_items.sort(key = lambda i: i[0])\n",
      "    >>> for key, val in qual_items: print(key.ljust(20,' ') + val.ljust(20,' '))\n",
      "    annotation_evidence made up             \n",
      "    feature_id          12345               \n",
      "    gene                CoI                 \n",
      "    source_organism     Tetillda radiata    \n",
      "    \"\"\"\n",
      "    if type(feature_id) is list:\n",
      "        feature_id = feature_id[0]\n",
      "    record_id = feature_id.split('_')[0]\n",
      "    qualifiers_dictionary={}\n",
      "    for record in database.records:\n",
      "        if record.id in feature_id:\n",
      "            for annotation in record.annotations.keys():\n",
      "                qualifiers_dictionary['annotation_'+annotation]=record.annotations[annotation]\n",
      "            for feature in record.features:\n",
      "                if feature.type == 'source':\n",
      "                    for qualifier in feature.qualifiers.keys():\n",
      "                        qualifiers_dictionary['source_'+qualifier]=feature.qualifiers[qualifier][0]\n",
      "                elif feature.qualifiers['feature_id'][0] == feature_id:\n",
      "                    for qualifier in feature.qualifiers.keys():\n",
      "                        qualifiers_dictionary[qualifier]=feature.qualifiers[qualifier][0]\n",
      "    return qualifiers_dictionary\n",
      "\n",
      "\n",
      "\n",
      "def seq_format_from_suffix(suffix):\n",
      "    \n",
      "    \"\"\"\n",
      "    Guesses input format from suffix\n",
      "    \n",
      "    >>> print(seq_format_from_suffix('gb'))\n",
      "    genbank\n",
      "    \"\"\"\n",
      "    \n",
      "    suffixes = {'fasta': ['fas','fasta','fa','fna'],\n",
      "                'genbank': ['gb','genbank'],\n",
      "                'embl': ['embl']}\n",
      "    found = False\n",
      "    for key in suffixes.keys():\n",
      "        if suffix in suffixes[key]:\n",
      "            found = True\n",
      "            return key\n",
      "    if not found:\n",
      "        raise RuntimeError(suffix+' is not a recognised suffix of an unaligned sequence file')\n",
      "\n",
      "\n",
      "\n",
      "def read_feature_quals_from_tab_csv(csv_filename):\n",
      "    import re\n",
      "    header = open(csv_filename, 'r').readlines()[0].rstrip().split('\\t')\n",
      "    feature_id_col = header.index('feature_id')\n",
      "    taxonomy_col = header.index('taxonomy')\n",
      "    seq_col = header.index('seq')\n",
      "    translation_col = None\n",
      "    if 'translation' in header:\n",
      "        translation_col = header.index('translation')\n",
      "    csv_info = {}\n",
      "    for line in [l.rstrip().split('\\t') for l in open(csv_filename, 'r').readlines()[1:]]:\n",
      "        if not line[0] in csv_info.keys():\n",
      "            csv_info[line[0]] = {'source':{},\n",
      "                                 'taxonomy':[],\n",
      "                                 'features':{}\n",
      "                                 }\n",
      "        if csv_info[line[0]]['taxonomy'] == []:\n",
      "            csv_info[line[0]]['taxonomy'] = line[taxonomy_col].split(';')\n",
      "        csv_info[line[0]]['features'][line[feature_id_col]] = {}\n",
      "        get_source = False\n",
      "        if csv_info[line[0]]['source'] == {}:\n",
      "            get_source = True\n",
      "        for i in range(len(header)):\n",
      "            if get_source and 'source:_' in header[i]:\n",
      "                qual_name = re.sub('source:_','',header[i])\n",
      "                if not line[i] == 'null' and not line[i] == '':\n",
      "                    csv_info[line[0]]['source'][qual_name] = line[i].split(';')\n",
      "            elif (not 'source:_' in header[i] and not line[i] == 'null' and not line[i] == '' and\n",
      "                  not i in [seq_col, translation_col, taxonomy_col, feature_id_col]):\n",
      "                csv_info[line[0]]['features'][line[feature_id_col]][header[i]] = line[i].split(';')\n",
      "    return csv_info\n",
      "\n",
      "                \n",
      "            \n",
      "                \n",
      "            \n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "##############################################################################################\n",
      "class Database:\n",
      "##############################################################################################\n",
      "    \n",
      "    \"\"\"\n",
      "    The Database class contians all the data and has methods to analyze it. It allows for\n",
      "    experimental analysis by running alternative analyses and formally comparing the \n",
      "    outputs. The pickle_db() function allows to pickle the database, including the data,\n",
      "    intermediates and results, as well as a description of the methods.It allows for a rerun\n",
      "    of the whole analysis as is, as well as for a reconfiguration of the analysis or addition\n",
      "    of data. If git is installed, it can be called by 'import rpgit'. As a result, git can be \n",
      "    initiated using start_git(). A git repository will be created in the CWD, if it doesn't already exist. \n",
      "    Input datasets, .py, .ipynb and .pkl files in the CWD will be version controlled. \n",
      "    Version control can be paused in the middle of the script\n",
      "    by calling stop_git() and restarted by calling start_git() again.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, loci):\n",
      "        \n",
      "        \"\"\"\n",
      "        # making dummy loci\n",
      "        >>> coi = Locus('dna','CDS','coi',['COX1','cox1'])\n",
      "        >>> ssu = Locus('dna','rRNA','18S',['18S','SSU'])\n",
      "        \n",
      "        # Making a Database object\n",
      "        >>> db = Database([coi,ssu])\n",
      "        >>> print(str(db))\n",
      "        Database object with the loci coi,18S,\n",
      "        \"\"\"\n",
      "        self.records = []\n",
      "        self.loci = loci\n",
      "        self.records_by_locus = {}\n",
      "        self.concatenations = []\n",
      "        self.alignments = {}\n",
      "        self.trimmed_alignments = {}\n",
      "        self.trees = {}\n",
      "        self.used_methods = []\n",
      "        seen = []\n",
      "        if isinstance(loci,list):\n",
      "            for locus in loci:\n",
      "                if not isinstance(locus, Locus):\n",
      "                    raise TypeError(\"Expecting Locus object in loci list. \"+locus+\n",
      "                                    \" not a Locus object\")\n",
      "                if locus.name in seen:\n",
      "                    raise NameError('Locus ' + locus.name + ' apears more than once in self.loci')\n",
      "                else:\n",
      "                    seen.append(locus.name)\n",
      "        elif isinstance(loci,str):\n",
      "            if any(len(line.split(',')) >= 4 for line in open(loci, 'r').readlines()):\n",
      "                pass\n",
      "            else:\n",
      "                raise IOError(\"File %s has no valid loci of format char_type,feature_type,name,aliases\"%loci)\n",
      "                \n",
      "                \n",
      "            loci_dict = {}\n",
      "            loci_list = []\n",
      "            for line in [line.rstrip() for line in open(loci, 'r').readlines() if len(line.rstrip()) > 0]:\n",
      "                if len(line.split(',')) < 4:\n",
      "                    raise IOError(\"The line %s in file %s is missing arguments. Needs at least char_type,feature_type,name,aliases\"%\n",
      "                                  (line.rstrip(), loci))\n",
      "                else:\n",
      "                    group = None\n",
      "                    try:\n",
      "                        group = int(line.rstrip().split(',')[-1])\n",
      "                    except:\n",
      "                        pass\n",
      "                    \n",
      "                    if group:\n",
      "                        locus_exists = False\n",
      "                        for name in loci_dict:\n",
      "                            if 'group' in loci_dict[name].keys() and loci_dict[name]['group'] == group:\n",
      "                                loci_dict[name]['aliases'] += line.split(',')[3:-1]\n",
      "                                locus_exists = True\n",
      "                        if not locus_exists:\n",
      "                            loci_dict[line.split(',')[2]] = {'group': int(line.rstrip().split(',')[-1]),\n",
      "                                                             'char_type': line.split(',')[0],\n",
      "                                                             'feature_type': line.split(',')[1],\n",
      "                                                             'aliases': line.split(',')[3:-1]\n",
      "                                                             }\n",
      "                    else:\n",
      "                        loci_dict[line.split(',')[2]] = {'group': None,\n",
      "                                                         'char_type': line.split(',')[0],\n",
      "                                                         'feature_type': line.split(',')[1],\n",
      "                                                         'aliases': line.split(',')[3:]\n",
      "                                                         }\n",
      "                        \n",
      "                    \n",
      "                    \n",
      "            for name in loci_dict:\n",
      "                loci_list.append(Locus(loci_dict[name]['char_type'],\n",
      "                                       loci_dict[name]['feature_type'],\n",
      "                                       name,\n",
      "                                       loci_dict[name]['aliases']))\n",
      "            self.loci = loci_list\n",
      "            print 'Read the following loci from file %s:'%loci\n",
      "            for l in self.loci:\n",
      "                print str(l)\n",
      "     \n",
      "                \n",
      "                \n",
      "    def __str__(self):\n",
      "        loci_string = ''\n",
      "        for i in self.loci:\n",
      "            loci_string += i.name+','\n",
      "        return 'Database object with the loci '+loci_string\n",
      "\n",
      "\n",
      "\n",
      "    ###################################\n",
      "    # Database methods for reading data\n",
      "    ###################################  \n",
      "\n",
      "\n",
      "\n",
      "    def read_embl_genbank(self, input_filenames_list):\n",
      "        \n",
      "        \"\"\"\n",
      "        Read a file from Genbank of EMBL\n",
      "        \n",
      "        >>> input_filenames = ['data/example.gb']\n",
      "        >>> locus = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "        >>> db = Database([locus])\n",
      "        >>> print(len(db.records))\n",
      "        0\n",
      "        >>> db.read_embl_genbank(input_filenames)\n",
      "        >>> print(len(db.records))\n",
      "        90\n",
      "        \"\"\"\n",
      "        \n",
      "        if __builtin__.git:\n",
      "            import rpgit\n",
      "        else:\n",
      "            warnings.warn('Version control off')    \n",
      "        generators = []\n",
      "        for input_filename in input_filenames_list:\n",
      "            if __builtin__.git:\n",
      "                import rpgit\n",
      "                rpgit.gitAdd(input_filename)\n",
      "            generators.append(parse_input(input_filename, 'gb'))\n",
      "            for generator in generators:\n",
      "                for record in generator:\n",
      "                    dwindled_record = dwindle_record(record, self.loci)\n",
      "                    if len(record.features) > 1:\n",
      "                        self.records.append(dwindled_record)\n",
      "                    elif len(record.features) == 1 and not record.features[0].type == 'source':\n",
      "                        self.records.append(dwindled_record)\n",
      "        if __builtin__.git:\n",
      "            import rpgit\n",
      "            comment = \"%i genbank/embl data file(s) from %s\" % (len(input_filenames_list), time.asctime())\n",
      "            rpgit.gitCommit(comment)          \n",
      "            \n",
      "        \n",
      "        \n",
      "    def read_denovo(self, input_filenames, char_type, format = 'fasta'):\n",
      "        \n",
      "        \"\"\"\n",
      "        Include records from a fasta file. Fasta file records will be given record ids \n",
      "        of the form 'denovo1'. The record.id and record.description will be placed in a\n",
      "        source feature under the 'original_id' and 'original_desc' qualifiers. Denovo sequences\n",
      "        require the use of the add_feature_to_record() method in order to be included in the\n",
      "        anaysis.\n",
      "        \n",
      "\n",
      "        >>> input_filenames = ['data/example.gb']\n",
      "        >>> locus = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "        >>> db = Database([locus])\n",
      "        >>> print(len(db.records))\n",
      "        0\n",
      "        >>> db.read_embl_genbank(input_filenames)\n",
      "        >>> print(len(db.records))\n",
      "        90\n",
      "        >>> input_filenames = ['data/example_denovo.fasta']\n",
      "        >>> db.read_denovo(input_filenames, 'dna')\n",
      "        >>> print(len(db.records))\n",
      "        91\n",
      "        \n",
      "        # Since the denovo sequence has no feature it is not included\n",
      "        >>> db.extract_by_locus()\n",
      "        >>> print(len(db.records_by_locus['coi']))\n",
      "        90\n",
      "        \n",
      "        # Making a feature for the denovo record.\n",
      "        >>> db.add_feature_to_record('denovo0', 'CDS',  qualifiers={'gene': 'coi'})\n",
      "        >>> db.extract_by_locus()\n",
      "        >>> print(len(db.records_by_locus['coi']))\n",
      "        91\n",
      "        \"\"\"\n",
      "        \n",
      "        if __builtin__.git:\n",
      "            import rpgit\n",
      "        else:\n",
      "            warnings.warn('Version control off')\n",
      "        \n",
      "        count = 0\n",
      "        # start the counter where it stoped the last time we read denovo things\n",
      "        for record in self.records:\n",
      "            if 'denovo' in record.id:\n",
      "                serial = int(record.id[6:])\n",
      "                if serial > count:\n",
      "                    count = serial+1\n",
      "        for input_filename in input_filenames:\n",
      "            if __builtin__.git:\n",
      "                import rpgit\n",
      "                rpgit.gitAdd(input_filename)\n",
      "            denovo = SeqIO.parse(input_filename, format)\n",
      "            for record in denovo:\n",
      "                source = SeqFeature(FeatureLocation(0, len(record.seq)), type='source', strand=1)\n",
      "                source.qualifiers['original_id'] = [record.id]\n",
      "                source.qualifiers['original_desc'] = [(' ').join(record.description.split()[1:])]\n",
      "                record.id = 'denovo'+str(count)\n",
      "                record.name = record.id\n",
      "                source.qualifiers['feature_id'] = [record.id+'_source']\n",
      "                record.features = [source]\n",
      "                if char_type == 'prot':\n",
      "                    record.seq.alphabet = IUPAC.protein\n",
      "                elif char_type == 'dna':\n",
      "                    record.seq.alphabet = IUPAC.ambiguous_dna\n",
      "                count += 1\n",
      "                self.records.append(record)\n",
      "                \n",
      "        if __builtin__.git:\n",
      "            import rpgit\n",
      "            comment = \"%i denovo data file(s) from %s\" % (len(input_filenames), time.asctime())\n",
      "            rpgit.gitCommit(comment)\n",
      "               \n",
      "               \n",
      "               \n",
      "    def add_feature_to_record(self, record_id, feature_type, location='full', qualifiers={}):\n",
      "    \n",
      "        \"\"\"\n",
      "        # Making a dummy locus    \n",
      "        >>> coi = Locus('dna','CDS','coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "    \n",
      "        # Making a dummy Database\n",
      "        >>> db = Database([coi])\n",
      "    \n",
      "        # making a dummy record\n",
      "        >>> s = 'atgc'*1000\n",
      "        >>> source = SeqFeature()\n",
      "        >>> source.location = FeatureLocation(0,3999)\n",
      "        >>> source.type = 'source'\n",
      "        >>> record = SeqRecord(seq=Seq(s, IUPAC.ambiguous_dna), id='1', description='spam')\n",
      "        >>> record.features.append(source)\n",
      "        >>> db.records = [record]\n",
      "        >>> print(len(db.records[0].features))\n",
      "        1\n",
      "        \n",
      "        # adding a feature to a record in the db\n",
      "        >>> db.add_feature_to_record('1', 'CDS', qualifiers={'gene': 'madeuplocus'})\n",
      "        >>> print(len(db.records[0].features))\n",
      "        2\n",
      "        \"\"\"\n",
      "    \n",
      "        for record in self.records:\n",
      "            if record.id == record_id:\n",
      "                #determine new feature id\n",
      "                feature_id = None\n",
      "                serials = []\n",
      "                for feature in record.features:\n",
      "                    if 'feature_id' in feature.qualifiers.keys():\n",
      "                        if '_f' in feature.qualifiers['feature_id']:\n",
      "                            f = feature.qualifiers['feature_id']\n",
      "                            serials.append(int(f.split('_')[1][1:]))\n",
      "                serials.sort(reverse = True)\n",
      "                if len(serials) > 0:\n",
      "                    feature_id = record.id + '_f' + str(serials[0]+1)\n",
      "                else:\n",
      "                    feature_id = record.id + '_f0'\n",
      "                feature = None\n",
      "                if location == 'full':\n",
      "                    feature = SeqFeature(FeatureLocation(0, len(record.seq)),\n",
      "                                         type=feature_type,\n",
      "                                         strand=1)\n",
      "                elif isinstance(location, list):\n",
      "                    for i in location:\n",
      "                        if not isinstance(i, list):\n",
      "                            raise RuntimeError('\\'location\\' takes either \\'full\\' or a list of lists')\n",
      "                    if len(location) == 1:\n",
      "                        feature = SeqFeature(FeatureLocation(int(location[0][0])-1,int(location[0][1])),\n",
      "                                             type=feature_type, strand=int(location[0][2]))\n",
      "                    elif len(location) > 1:\n",
      "                        list_of_locations = []\n",
      "                        for i in location:\n",
      "                            start = int(i[0]-1)\n",
      "                            end = int(i[1])\n",
      "                            strand = int(i[2])\n",
      "                            list_of_locations.append(FeatureLocation(start,end,strand=strand))\n",
      "                        feature = SeqFeature(CompoundLocation(list_of_locations),type=feature_type)\n",
      "                feature.qualifiers['feature_id'] = [feature_id]\n",
      "                \n",
      "                \n",
      "                \n",
      "                if len(qualifiers.keys()) > 0:\n",
      "                    for key in qualifiers.keys():\n",
      "                        feature.qualifiers[key] = [qualifiers[key]]\n",
      "                if (('codon_start' in qualifiers.keys()) and\n",
      "                    ('transl_table' in qualifiers.keys())):\n",
      "                    cds = feature.extract(record.seq)\n",
      "                    if str(qualifiers['codon_start']) == '2':\n",
      "                        cds = cds[1:]\n",
      "                    elif str(qualifiers['codon_start']) == '3':\n",
      "                        cds = cds[2:]\n",
      "                    translation = cds.translate(table=int(qualifiers['transl_table']))\n",
      "                    if len(translation)*3 < float(0.9)*len(cds):\n",
      "                        raise RuntimeWarning('The translation of feature '+feature_id+' uses less than 90%'+\n",
      "                                             ' of the coding sequence')\n",
      "                    feature.qualifiers['translation'] = [str(translation)]\n",
      "                \n",
      "                \n",
      "                feature_seq = feature.extract(record.seq)\n",
      "                degen = len(feature_seq)\n",
      "                for i in ['A','T','G','C','U','a','t','g','c','u']:\n",
      "                    degen -= feature_seq.count(i)\n",
      "                feature.qualifiers['GC_content'] = [str(GC(feature_seq))]\n",
      "                feature.qualifiers['nuc_degen_prop'] = [str(float(degen)/len(feature_seq))]\n",
      "                if 'translation' in feature.qualifiers.keys():\n",
      "                    transl = feature.qualifiers['translation'][0]\n",
      "                    degen = 0\n",
      "                    for i in ['B', 'X', 'Z', 'b', 'x', 'z']:\n",
      "                        degen += transl.count(i)\n",
      "                    feature.qualifiers['prot_degen_prop'] = [str(float(degen)/len(transl))]   \n",
      "                \n",
      "                record.features.append(feature)\n",
      "\n",
      "\n",
      "\n",
      "    ##############################################\n",
      "    # Database methods for managing concatenations\n",
      "    ##############################################  \n",
      "\n",
      "\n",
      " \n",
      "    def add_concatenation(self, concatenation_object):\n",
      "        \n",
      "        \"\"\"\n",
      "        add a Concatenation object to the Database\n",
      "        \n",
      "        # making dummy loci\n",
      "        >>> coi = Locus('dna','CDS','coi',['COX1','cox1'])\n",
      "        >>> ssu = Locus('dna','rRNA','18S',['18S','SSU'])\n",
      "        >>> lsu = Locus('dna','rRNA','28S',['28S','LSU'])\n",
      "        >>> loci = [coi,ssu,lsu]\n",
      "        \n",
      "        # making dummy Concatenation\n",
      "        >>> combined = Concatenation(name='combined', \n",
      "        ...                          loci=loci, otu_meta='OTU_dict',\n",
      "        ...                          concat_must_have_all_of=['coi'],\n",
      "        ...                          concat_must_have_one_of =[['18S','28S']],\n",
      "        ...                          define_trimmed_alns=[\"MafftLinsi@Gappyout\"])\n",
      "        >>> print(str(combined))\n",
      "        Concatenation named combined, with loci coi,18S,28S,\n",
      "        of which coi must exist for all species\n",
      "        and at least one of each group of [ 18S 28S ] is represented.\n",
      "        Alignments with the following names: MafftLinsi@Gappyout are prefered\n",
      "        \n",
      "        # making a dummy Database\n",
      "        >>> db = Database(loci)\n",
      "        \n",
      "        # Including the Concatenation in the Database\n",
      "        >>> db.add_concatenation(combined)\n",
      "        >>> print(len(db.concatenations))\n",
      "        1\n",
      "        \"\"\"\n",
      "        \n",
      "        if isinstance(concatenation_object, Concatenation):\n",
      "            seen = []\n",
      "            for s in self.concatenations:\n",
      "                seen.append(s.name)\n",
      "            if concatenation_object.name in seen:\n",
      "                raise NameError('Concatenation ' + concatenation_object.name +\n",
      "                                ' apears more than once in self.concatenations')\n",
      "            else:\n",
      "                self.concatenations.append(concatenation_object)\n",
      "        else:\n",
      "            raise TypeError(\"Expecting Concatenation object\")\n",
      "\n",
      "    def make_concatenation_alignments(self):\n",
      "        \n",
      "        \"\"\"\n",
      "        Concatenates a trimmed alignment based on each of the Concatenation objects and adds them\n",
      "        to the db.trimmed_alignments dictionary. While a trimmed alignment of an individual locus will have a key\n",
      "        following the patten \"locus_name@alignment_method_name@trimming_method_name, the key for a concatenated\n",
      "        trimmed alignment will be the Concatenation object name attribute.\n",
      "        \"\"\"\n",
      "        for s in self.concatenations:\n",
      "            \n",
      "            # get a non-redundant list of OTUs stored in 'meta', such as voucher specimen\n",
      "            meta = s.otu_meta\n",
      "            OTU_list = []\n",
      "            for record in self.records:\n",
      "                for feature in record.features:\n",
      "                    if not feature.type == 'source':\n",
      "                        qualifiers_dictionary = get_qualifiers_dictionary(self,\n",
      "                                                                          feature.qualifiers['feature_id'])\n",
      "                        if (meta in qualifiers_dictionary.keys() and\n",
      "                            not qualifiers_dictionary[meta] in OTU_list):\n",
      "                            OTU_list.append(qualifiers_dictionary[meta])\n",
      "                            \n",
      "            # make lists of available feature_ids in each locus\n",
      "            available_features = {}\n",
      "            for locus in s.loci:\n",
      "                available_features[locus.name] = []\n",
      "                for record in self.records_by_locus[locus.name]:\n",
      "                    available_features[locus.name].append(record.id) # record ids are feature ids because taken from db.records_by_locus\n",
      "\n",
      "            # make a dict of individuals that fulfil the concat's first rule\n",
      "            seen_locus_names = []\n",
      "            included_individuals = {}\n",
      "            \n",
      "            for individual in OTU_list:\n",
      "                include = True\n",
      "                for must_have_locus_name in s.concat_must_have_all_of:\n",
      "                    if not must_have_locus_name in seen_locus_names:\n",
      "                        seen_locus_names.append(must_have_locus_name)\n",
      "                    locus_specific_features = []\n",
      "                    for feature_id in available_features[must_have_locus_name]:\n",
      "                        qualifiers_dictionary = get_qualifiers_dictionary(self,feature_id)\n",
      "                        if meta in qualifiers_dictionary.keys() and qualifiers_dictionary[meta] == individual:\n",
      "                            locus_specific_features.append(feature_id)\n",
      "                    if len(locus_specific_features) == 1:\n",
      "                        if not individual in included_individuals.keys():\n",
      "                            included_individuals[individual] = {}\n",
      "                        included_individuals[individual][must_have_locus_name] = locus_specific_features[0]\n",
      "                    elif len(locus_specific_features) > 1:\n",
      "                        raise RuntimeError(individual + ' is not unique for ' + must_have_locus_name)\n",
      "                    else:\n",
      "                        include = False\n",
      "                if individual in included_individuals.keys() and not include:\n",
      "                    included_individuals.pop(individual, None)\n",
      "                            \n",
      "            # check if the individual fullfil the second set rule\n",
      "            for individual in included_individuals.keys():\n",
      "                include = True\n",
      "                for loci_group in s.concat_must_have_one_of:\n",
      "                    count = 0\n",
      "                    for locus_name in loci_group:\n",
      "                        if not locus_name in seen_locus_names:\n",
      "                            seen_locus_names.append(locus_name)\n",
      "                        locus_specific_features = []\n",
      "                        for feature_id in available_features[locus_name]:\n",
      "                            qualifiers_dictionary = get_qualifiers_dictionary(self,feature_id)\n",
      "                            if meta in qualifiers_dictionary.keys() and qualifiers_dictionary[meta] == individual:\n",
      "                                locus_specific_features.append(feature_id)\n",
      "                        if len(locus_specific_features) == 1:\n",
      "                            count += 1\n",
      "                            included_individuals[individual][locus_name] = locus_specific_features[0]\n",
      "                        elif len(locus_specific_features) > 1:\n",
      "                            raise RuntimeError(individual + ' is not unique for ' + locus_name)\n",
      "                    if count == 0:\n",
      "                        include = False\n",
      "                if not include:\n",
      "                    included_individuals.pop(individual, None)\n",
      "\n",
      "            # add loci that are in the set but not addressed in rules\n",
      "            for individual in included_individuals.keys():\n",
      "                for locus in s.loci:\n",
      "                    locus_specific_features = []\n",
      "                    if not locus.name in seen_locus_names:\n",
      "                        for feature_id in available_features[locus.name]:\n",
      "                            qualifiers_dictionary = get_qualifiers_dictionary(self,feature_id)\n",
      "                            if meta in qualifiers_dictionary.keys() and qualifiers_dictionary[meta] == individual:\n",
      "                                locus_specific_features.append(feature_id)\n",
      "                        if len(locus_specific_features) == 1:\n",
      "                            included_individuals[individual][locus.name] = locus_specific_features[0]\n",
      "                        elif len(locus_specific_features) > 1:\n",
      "                            raise RuntimeError(individual + ' is not unique for ' + locus.name)\n",
      "\n",
      "            # build alignment\n",
      "            concat_records = []\n",
      "            alignment = []\n",
      "            keys_of_trimmed_alignments_to_use_in_concat = []\n",
      "            for locus in s.loci:\n",
      "                trimmed_aln = None\n",
      "                all_locus_trimmed_alns_in_db = []\n",
      "                for key in self.trimmed_alignments.keys():\n",
      "                    if locus.name == key.split('@')[0]:\n",
      "                        all_locus_trimmed_alns_in_db.append(key)\n",
      "                if len(all_locus_trimmed_alns_in_db) == 1:\n",
      "                    trimmed_aln = all_locus_trimmed_alns_in_db[0]\n",
      "                elif len(all_locus_trimmed_alns_in_db) == 0:\n",
      "                    raise RuntimeError('Locus '+locus.name+' have no trimmed alignments')\n",
      "                else:\n",
      "                    s.define_trimmed_alns.sort(key = lambda i: i.count('@'), reverse=True)\n",
      "                    for definition in s.define_trimmed_alns:\n",
      "                        if definition.count('@') == 2 and locus.name == definition.split('@')[0]:\n",
      "                            trimmed_aln = definition\n",
      "                        elif definition.count('@') == 1 and bool(any(definition in i for i in all_locus_trimmed_alns_in_db)):\n",
      "                            trimmed_aln = locus.name+'@'+definition\n",
      "                        else:\n",
      "                            raise RuntimeError(\"Could not determine which alignment/trimming alternative to use for locus '\"+\n",
      "                                                locus.name+\"' out of \"+str(locus_trimmed_alns))\n",
      "                if trimmed_aln:\n",
      "                    keys_of_trimmed_alignments_to_use_in_concat.append(trimmed_aln)\n",
      "                else:\n",
      "                    raise RuntimeError('Could not find trimmed aln for locus '+locus.name+' given the rulls '+str(s.define_trimmed_alns))\n",
      "\n",
      "            for individual in included_individuals.keys():\n",
      "                sequence = ''\n",
      "                for key in keys_of_trimmed_alignments_to_use_in_concat:\n",
      "                    locus_name = key.split('@')[0]                    \n",
      "                    length = len(self.trimmed_alignments[key][0])\n",
      "                    s.used_trimmed_alns[key] = length\n",
      "                    if locus_name in included_individuals[individual].keys():\n",
      "                        for record in self.trimmed_alignments[key]:\n",
      "                            if record.id == included_individuals[individual][locus_name]:\n",
      "                                sequence += str(record.seq)\n",
      "                    else:\n",
      "                        sequence += 'N'*length\n",
      "                concat_sequence = SeqRecord(seq = Seq(sequence), id = individual, description = '')\n",
      "                alignment.append(concat_sequence)\n",
      "            self.trimmed_alignments[s.name] = MultipleSeqAlignment(alignment)                \n",
      "            s.feature_id_dict = included_individuals    \n",
      "\n",
      "\n",
      "\n",
      "    ###################################################\n",
      "    # Database methods for modifying feature qualifiers\n",
      "    ###################################################\n",
      "\n",
      "\n",
      "\n",
      "    def write(self, filename, format = 'genbank'):\n",
      "        \n",
      "        \"\"\"\n",
      "        Write the records in the database in any Biopython format, as csv or as nexml.\n",
      "        \n",
      "        The csv table has a line for each feature (ie multiplt lines for records\n",
      "        with multiple non-source featue). Each line will include the records annotations,\n",
      "        the source feature qualifiers and the qualifiers of the feature itself. (ie, in a\n",
      "        record with several features, the record annotations and the source feature qualifiers\n",
      "        will be repeted in several lines, once for each non-source feature in the record.\n",
      "        The csv file is primarily usefull for reviewing and editing feature qualifiers\n",
      "        \n",
      "        The nexml format only includes the trees from the db.trees dictionary, but all \n",
      "        the metadata is included as leaf metadata, including the sequences and the\n",
      "        trimmed and alligned sequences for each leaf.\n",
      "        \"\"\"\n",
      "        if format == 'nexml':\n",
      "            self.write_nexml(filename)\n",
      "        elif format == 'genbank' or format == 'embl':\n",
      "            SeqIO.write(self.records, filename, format)\n",
      "        elif format == 'csv':\n",
      "            # get titles for source and othe features\n",
      "            source_qualifiers = []\n",
      "            feature_qualifiers = []\n",
      "            for record in self.records:\n",
      "                for feature in record.features:\n",
      "                    for key in feature.qualifiers.keys():\n",
      "                        if feature.type == 'source' and not key in source_qualifiers:\n",
      "                            source_qualifiers.append(key)\n",
      "                        elif not feature.type == 'source' and not key in feature_qualifiers:\n",
      "                            feature_qualifiers.append(key)\n",
      "            \n",
      "            with open(filename, 'wb') as csvfile:\n",
      "                linewriter = csv.writer(csvfile, delimiter='\\t',\n",
      "                                        quotechar='|',\n",
      "                                        quoting=csv.QUOTE_MINIMAL)\n",
      "                linewriter.writerow(['record_id','seq']+['source:_'+q for q in source_qualifiers]+['taxonomy']+feature_qualifiers)\n",
      "                for record in self.records:\n",
      "                    seq = ''\n",
      "                    if len(record.seq) <= 10:\n",
      "                        seq = str(record.seq)[0:11]\n",
      "                    else:\n",
      "                        seq = str(record.seq)[0:6] + '...' + str(record.seq)[-5:]\n",
      "                    \n",
      "                    \n",
      "                    line_start = [record.id, seq]\n",
      "                    source = None\n",
      "                    for feature in record.features:\n",
      "                        if feature.type == 'source':\n",
      "                            source = feature\n",
      "                    if not source == None:\n",
      "                        for qual in source_qualifiers:\n",
      "                            if qual in source.qualifiers.keys():\n",
      "                                line_start.append(type_to_single_line_str(source.qualifiers[qual]))\n",
      "                            else:\n",
      "                                line_start.append('null')\n",
      "                    elif source == None:\n",
      "                        for qual in source_qualifiers:\n",
      "                            line_start.append('null')\n",
      "                    if 'taxonomy' in record.annotations.keys():\n",
      "                        line_start.append(type_to_single_line_str(record.annotations['taxonomy']))\n",
      "                    else:\n",
      "                        line_start.append(['null'])\n",
      "                    for feature in record.features:\n",
      "                        if not feature.type == 'source':\n",
      "                            line = list(line_start)\n",
      "                            for qual in feature_qualifiers:\n",
      "                                if qual in feature.qualifiers.keys() and qual == 'translation':\n",
      "                                    trans = feature.qualifiers[qual][0]\n",
      "                                    if len(trans)<=10:\n",
      "                                        line.append(trans)\n",
      "                                    else:\n",
      "                                        line.append(trans[:6] + '...' + trans[-5:])\n",
      "                                elif qual in feature.qualifiers.keys():\n",
      "                                    line.append(type_to_single_line_str(feature.qualifiers[qual]))\n",
      "                                else:\n",
      "                                    line.append('null')\n",
      "                            linewriter.writerow(line)\n",
      "\n",
      "    def correct_metadata_from_file(self,csv_file):\n",
      "        metadata = read_feature_quals_from_tab_csv(csv_file)\n",
      "        for record in self.records:\n",
      "            record_corrected_metadata = metadata[record.id]\n",
      "            record.annotations['taxonomy'] = metadata[record.id]['taxonomy']\n",
      "            for feature in record.features:\n",
      "                if feature.type == 'source':\n",
      "                    feature.qualifiers = metadata[record.id]['source']\n",
      "                else:\n",
      "                    feature_id = feature.qualifiers['feature_id']\n",
      "                    translation = None\n",
      "                    if 'translation' in feature.qualifiers.keys():\n",
      "                        translation = feature.qualifiers['translation']\n",
      "                    feature.qualifiers = metadata[record.id]['features'][feature_id[0]]\n",
      "                    feature.qualifiers['feature_id'] = feature_id\n",
      "                    if translation:\n",
      "                        feature.qualifiers['translation'] = translation\n",
      "                \n",
      "                \n",
      "    def if_this_then_that(self, IF_THIS, IN_THIS, THEN_THAT, IN_THAT, mode = 'whole'):\n",
      "        \n",
      "        \n",
      "        \"\"\"\n",
      "        Searches db.records for features that have the value IF_THIS in the qualifier IN_THIS\n",
      "        and places the value THEN_THAT in the qualifier IN_THAT, which either exists or is new.\n",
      "        \n",
      "        The IF_THIS value can either match completely (mode = 'whole') or just to a part (mode = 'part')\n",
      "        of the target qualifier value\n",
      "        \n",
      "        The following demonstartes all the feature qualifier editing methods\n",
      "        \n",
      "        # Make a dummy db with a locus and with records\n",
      "        >>> input_filenames = ['data/example.gb']\n",
      "        >>> locus = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "        >>> db = Database([locus])\n",
      "        >>> db.read_embl_genbank(input_filenames)\n",
      "        \n",
      "        # copying a source qualifier into the feature qualifiers so that it\n",
      "        # will be available for editing (source qualifiers are kept imutable)\n",
      "        >>> db.add_qualifier_from_source('organism')\n",
      "        \n",
      "        # populate a new qualifier based on the data in another\n",
      "        # Here we will take only the the genus name from the organism qualifier\n",
      "        # and put it in a new qualifier\n",
      "        # We use mode='part' because our search phrase (the genus name)\n",
      "        # fits only the start of the organism name\n",
      "        >>> tetillid_genera = ['Tetilla', 'Cinachyrella', 'Craniella']\n",
      "        >>> for g in tetillid_genera:\n",
      "        ...     db.if_this_then_that(g, 'organism', g, 'genus', mode='part')\n",
      "        \n",
      "        # Now we will add a sample id to all the sequences which belong to\n",
      "        # sample TAU_25617\n",
      "        >>> db.add_qualifier(['JX177913.1_f0',\n",
      "        ...                  'JX177935.1_f0',\n",
      "        ...                  'JX177965.1_f0'],\n",
      "        ...                  'specimen_voucher',\n",
      "        ...                  'TAU_25617')\n",
      "    \n",
      "        # We are using a copy paste approch to unite the data from \n",
      "        # differen qualifiers under on qualifiers\n",
      "        >>> db.copy_paste_within_feature('variant', 'strain_or_variant')\n",
      "        >>> db.copy_paste_within_feature('strain', 'strain_or_variant')\n",
      "        \n",
      "        # Now we print the qualifier of a random feature as an example\n",
      "        >>> qual_dict = get_qualifiers_dictionary(db, 'JX177913.1_f0')\n",
      "        >>> qual_items = qual_dict.items()\n",
      "        >>> qual_items.sort(key = lambda i: i[0])\n",
      "        >>> for key, val in qual_items:\n",
      "        ...     print(key.ljust(40,' ') + type_to_single_line_str(val)[:5]+'...')\n",
      "        GC_content                              37.28...\n",
      "        annotation_accessions                   JX177...\n",
      "        annotation_data_file_division           INV...\n",
      "        annotation_date                         05-SE...\n",
      "        annotation_gi                           39293...\n",
      "        annotation_keywords                     ...\n",
      "        annotation_organism                     Cinac...\n",
      "        annotation_references                   locat...\n",
      "        annotation_sequence_version             1...\n",
      "        annotation_source                       mitoc...\n",
      "        annotation_taxonomy                     Eukar...\n",
      "        codon_start                             2...\n",
      "        db_xref                                 GI:39...\n",
      "        feature_id                              JX177...\n",
      "        gene                                    cox1...\n",
      "        genus                                   Cinac...\n",
      "        nuc_degen_prop                          0.0...\n",
      "        organism                                Cinac...\n",
      "        product                                 cytoc...\n",
      "        prot_degen_prop                         0.0...\n",
      "        protein_id                              AFM91...\n",
      "        source_country                          Panam...\n",
      "        source_db_xref                          taxon...\n",
      "        source_feature_id                       JX177...\n",
      "        source_identified_by                    Ilan,...\n",
      "        source_mol_type                         genom...\n",
      "        source_note                             autho...\n",
      "        source_organelle                        mitoc...\n",
      "        source_organism                         Cinac...\n",
      "        source_specimen_voucher                 DH_S2...\n",
      "        specimen_voucher                        TAU_2...\n",
      "        transl_table                            4...\n",
      "        translation                             MIGSG...\n",
      "        \n",
      "        # Note that GC content and the porportion of degenerate positions\n",
      "        # have been automatically included. They will be plotted in the report\n",
      "        \"\"\"\n",
      "        \n",
      "        for record in self.records:\n",
      "            for feature in record.features:\n",
      "                if not feature.type == 'source':\n",
      "                    if IN_THIS in feature.qualifiers.keys():\n",
      "                        if not type(feature.qualifiers[IN_THIS]) is list:\n",
      "                            feature.qualifiers[IN_THIS] = [feature.qualifiers[IN_THIS]]\n",
      "                        for i in feature.qualifiers[IN_THIS]:\n",
      "                            if mode == 'whole':\n",
      "                                if i == IF_THIS:\n",
      "                                    feature.qualifiers[IN_THAT] = [THEN_THAT]\n",
      "                            elif mode == 'part':\n",
      "                                if IF_THIS in i:\n",
      "                                    feature.qualifiers[IN_THAT] = [THEN_THAT]\n",
      "    \n",
      "\n",
      "\n",
      "    def add_qualifier(self, feature_ids, name, value):\n",
      "        if not type(value) is list:\n",
      "                    value = [value]\n",
      "        for record in self.records:\n",
      "            for feature in record.features:\n",
      "                if feature.qualifiers['feature_id'][0] in feature_ids:\n",
      "                    feature.qualifiers[name] = value\n",
      "\n",
      "\n",
      "\n",
      "    def add_qualifier_from_source(self, qualifier):\n",
      "        for record in self.records:\n",
      "            source = None\n",
      "            for feature in record.features:\n",
      "                if feature.type == 'source':\n",
      "                    source = feature\n",
      "            value = None\n",
      "            if not source == None:\n",
      "              if qualifier in source.qualifiers.keys():\n",
      "                    value = source.qualifiers[qualifier]       \n",
      "            if not value == None:\n",
      "                if not type(value) is list:\n",
      "                    value = [value]\n",
      "                for feature in record.features:\n",
      "                    if not feature.type == 'source':\n",
      "                        feature.qualifiers[qualifier] = value\n",
      "    \n",
      "\n",
      "\n",
      "    def copy_paste_within_feature(self, from_qualifier, to_qualifier):\n",
      "        for record in self.records:\n",
      "            for feature in record.features:\n",
      "                if not feature.type == 'source':\n",
      "                    if from_qualifier in feature.qualifiers.keys():\n",
      "                        feature.qualifiers[to_qualifier] = feature.qualifiers[from_qualifier]\n",
      "                  \n",
      "                        \n",
      "                        \n",
      "    def copy_paste_from_features_to_source(self, from_feature_qual, to_source_qual):\n",
      "        for record in self.records:\n",
      "            source = None\n",
      "            values_from_features = []\n",
      "            for feature in record.features:\n",
      "                if not feature.type == 'source':\n",
      "                    if from_feature_qual in feature.qualifiers.keys():\n",
      "                        if not feature.qualifiers[from_feature_qual] in values_from_features:\n",
      "                            values_from_features += feature.qualifiers[from_feature_qual]\n",
      "                else:\n",
      "                    source = feature\n",
      "            if source == None:\n",
      "                source = SeqFeature(FeatureLocation(0, len(record.seq)), type='source', strand=1)\n",
      "                source.qualifiers['feature_id'] = record.id + '_source'\n",
      "                record.features = [source] + record.features\n",
      "            if not to_source_qual in source.qualifiers.keys():\n",
      "                source.qualifiers[to_source_qual] = values_from_features\n",
      "           \n",
      "                \n",
      "                \n",
      "    def species_vs_loci(self, outfile_name):\n",
      "        \n",
      "        \"\"\"\n",
      "        Makes a csv file showing the count of each unique value in the source_organism qualifier\n",
      "        for each locus\n",
      "        \"\"\"\n",
      "        \n",
      "        species_vs_loci = {}\n",
      "        for record in self.records:\n",
      "            organism = 'undef'\n",
      "            for feature in record.features:\n",
      "                if feature.type == 'source':\n",
      "                    if 'organism' in feature.qualifiers.keys():\n",
      "                        organism = feature.qualifiers['organism'][0]\n",
      "            if not organism in species_vs_loci.keys():\n",
      "                species_vs_loci[organism] = {}    \n",
      "            for feature in record.features:\n",
      "                if not feature.type == 'source':\n",
      "                    for locus in self.loci:\n",
      "                        if not locus.name in locus.aliases:\n",
      "                            locus.aliases.append(locus.name)\n",
      "                        if 'gene' in feature.qualifiers.keys():\n",
      "                            if feature.qualifiers['gene'][0] in locus.aliases:\n",
      "                                if not locus.name in species_vs_loci[organism].keys():\n",
      "                                    species_vs_loci[organism][locus.name] = 1\n",
      "                                else:\n",
      "                                    species_vs_loci[organism][locus.name] += 1\n",
      "                        elif 'product' in feature.qualifiers.keys():\n",
      "                            if feature.qualifiers['product'][0] in locus.aliases:\n",
      "                                if not locus.name in species_vs_loci[organism].keys():\n",
      "                                    species_vs_loci[organism][locus.name] = 1\n",
      "                                else:\n",
      "                                    species_vs_loci[organism][locus.name] += 1\n",
      "        with open(outfile_name, 'wb') as csvfile:\n",
      "            linewriter = csv.writer(csvfile, delimiter='\\t',\n",
      "                                    quotechar='|',\n",
      "                                    quoting=csv.QUOTE_MINIMAL)\n",
      "            loci_names = []\n",
      "            for g in self.loci:\n",
      "                loci_names.append(g.name)\n",
      "            linewriter.writerow(['species']+loci_names)\n",
      "            for organism in species_vs_loci.keys():\n",
      "                line = [organism]\n",
      "                for name in loci_names:\n",
      "                    if name in species_vs_loci[organism].keys():\n",
      "                        line.append(str(species_vs_loci[organism][name]))\n",
      "                    else:\n",
      "                        line.append('0')\n",
      "                linewriter.writerow(line)\n",
      "                \n",
      "                \n",
      "                \n",
      "    ######################################                \n",
      "    # Database methods to analyze the data\n",
      "    ######################################\n",
      "    \n",
      "    \n",
      "        \n",
      "    def extract_by_locus(self):\n",
      "        \n",
      "        \"\"\"\n",
      "        \n",
      "        >>> input_filenames = ['data/example.gb']\n",
      "        >>> coi = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "        >>> lsu = Locus('dna', 'rRNA', '28S', ['28s','28S','LSU rRNA','28S ribosomal RNA','28S large subunit ribosomal RNA'])\n",
      "        >>> db = Database([coi, lsu])\n",
      "        >>> db.read_embl_genbank(input_filenames)\n",
      "        >>> db.extract_by_locus()\n",
      "        >>> print(len(db.records_by_locus['coi']))\n",
      "        90\n",
      "        >>> print(len(db.records_by_locus['28S']))\n",
      "        48\n",
      "        \"\"\"\n",
      "        \n",
      "        data_by_locus = {}\n",
      "        for locus in self.loci:\n",
      "            if not locus.name in locus.aliases:\n",
      "                locus.aliases.append(locus.name)\n",
      "            records = []\n",
      "            for record in self.records:\n",
      "                for feature in record.features:\n",
      "                    if (feature.type == locus.feature_type and\n",
      "                        \n",
      "                        (('gene' in feature.qualifiers.keys() and\n",
      "                          feature.qualifiers['gene'][0] in locus.aliases) \n",
      "                         or\n",
      "                         ('product' in feature.qualifiers.keys() and \n",
      "                          feature.qualifiers['product'][0] in locus.aliases))\n",
      "                        \n",
      "                        ):\n",
      "                        if locus.char_type == 'dna':\n",
      "                            S = feature.extract(record.seq)\n",
      "                        elif locus.char_type == 'prot':\n",
      "                            S = Seq(feature.qualifiers['translation'][0], IUPAC.protein)\n",
      "                        feature_record = SeqRecord(seq = S, id = feature.qualifiers['feature_id'][0],\n",
      "                                                   description = '')\n",
      "                        records.append(feature_record)\n",
      "            data_by_locus[locus.name] = records\n",
      "        self.records_by_locus = data_by_locus\n",
      "\n",
      "    def exclude(self, start_from_max=True, **kwargs):\n",
      "        keep_safe = self.records_by_locus\n",
      "        self.extract_by_locus()\n",
      "        locus_names = [i.name for i in self.loci]\n",
      "        for key, value in kwargs.iteritems():\n",
      "            if key in locus_names:\n",
      "                if value == 'all':\n",
      "                    self.records_by_locus[key] = []\n",
      "                else:\n",
      "                    subset = []\n",
      "                    locus_feature_ids = [i.id.split('_')[0] for i in self.records_by_locus[key]]\n",
      "                    if not all(i.split('_')[0] in locus_feature_ids for i in value):\n",
      "                        print [i.split('_')[0] for i in value if not i.split('_')[0] in locus_feature_ids]\n",
      "                        warnings.warn('Not all records to exclude exist in locus. Typos?')\n",
      "                    if start_from_max:\n",
      "                        for record in keep_safe[key]:\n",
      "                            if not record.id.split('_')[0] in [i.split('_')[0] for i in value]:\n",
      "                                subset.append(record)\n",
      "                        self.records_by_locus[key] = subset\n",
      "                    else:\n",
      "                        for record in self.records_by_locus[key]:\n",
      "                            if not record.id.split('_')[0] in [i.split('_')[0] for i in value]:\n",
      "                                subset.append(record)\n",
      "                        self.records_by_locus[key] = subset\n",
      "            else:\n",
      "                warnings.warn('Locus name %s not recognised'%key)\n",
      "\n",
      "    def include(self, start_from_null=True, **kwargs):\n",
      "        keep_safe = self.records_by_locus\n",
      "        self.extract_by_locus()\n",
      "        locus_names = [i.name for i in self.loci]\n",
      "        for key, value in kwargs.iteritems():\n",
      "            if key in locus_names:\n",
      "                if value == 'all':\n",
      "                    pass\n",
      "                else:\n",
      "                    subset = []\n",
      "                    locus_feature_ids = [i.id.split('_')[0] for i in self.records_by_locus[key]]\n",
      "                    if not all(i.split('_')[0] in locus_feature_ids for i in value):\n",
      "                        print [i.split('_')[0] for i in value if not i.split('_')[0] in locus_feature_ids]\n",
      "                        warnings.warn('Not all records to include exist in locus. Typos?')\n",
      "                    for record in self.records_by_locus[key]:\n",
      "                        if record.id.split('_')[0] in [i.split('_')[0] for i in value]:\n",
      "                            subset.append(record)\n",
      "                    self.records_by_locus[key] = subset\n",
      "                    if not start_from_null:\n",
      "                        self.records_by_locus[key] = subset+keep_safe[key]\n",
      "            else:\n",
      "                warnings.warn('Locus name %s not recognised'%key)\n",
      "\n",
      "    def filter_by_seq_length(self, min_length=0, max_length=None):\n",
      "        if self.records_by_locus == {}:\n",
      "            self.extract_by_locus()\n",
      "        for key in self.records_by_locus.keys():\n",
      "            subset = [r for r in self.records_by_locus[key] if len(r) >= min_length]\n",
      "            if max_length:\n",
      "                subset = [r for r in subset if len(r) <= max_length]\n",
      "            self.records_by_locus[key] = subset\n",
      "                \n",
      "\n",
      "\n",
      "    def write_by_locus(self, format = 'fasta'):\n",
      "        \n",
      "        \"\"\"\n",
      "        Write the unaligned sequences into file in any Biopython format, one file per locus\n",
      "        \"\"\"\n",
      "        \n",
      "        if self.records_by_locus == {}:\n",
      "            self.extract_by_locus\n",
      "        for key in self.records_by_locus.keys():\n",
      "            SeqIO.write(self.records_by_locus[key], key+'.'+format, format)\n",
      "\n",
      "\n",
      "\n",
      "    def align(self, alignment_methods=[], pal2nal='./pal2nal.pl'):\n",
      "        \n",
      "            \"\"\"\n",
      "            Configured by an AlnConf object\n",
      "            \"\"\"\n",
      "\n",
      "            seen_loci = []\n",
      "            for method in alignment_methods:\n",
      "                method.timeit.append(time.time())\n",
      "                method.platform = platform_report()\n",
      "                if method.program_name == 'muscle':\n",
      "                    method.platform.append('Program and version: '+os.popen(method.cmd + ' -version').read())\n",
      "                elif method.program_name == 'mafft':\n",
      "                    method.platform.append('Program and version: Get mafft to spit version to stdout')\n",
      "                for locus in method.loci:\n",
      "                    if locus.name in seen_loci:\n",
      "                        #raise RuntimeError('locus '+locus.name+' is in more than one AlnConf objects')\n",
      "                        pass\n",
      "                    else:\n",
      "                        seen_loci.append(locus.name)\n",
      "                    stdout, stderr = method.command_lines[locus.name]()\n",
      "                    align = AlignIO.read(StringIO(stdout), \"fasta\",  alphabet=IUPAC.protein)\n",
      "                    if method.CDSAlign and locus.feature_type == 'CDS' and locus.char_type == 'dna':\n",
      "                        for seq in align:\n",
      "                            found = 0\n",
      "                            for s in method.CDS_in_frame[locus.name]:\n",
      "                                if s.id == seq.id:\n",
      "                                    found = 1\n",
      "                            if found == 0:\n",
      "                                raise RuntimeError(seq.id + ' is not in the CDS sequences')\n",
      "                        for s in method.CDS_in_frame[locus.name]:\n",
      "                            found = 0\n",
      "                            for seq in align:\n",
      "                                if s.id == seq.id:\n",
      "                                    found = 1\n",
      "                            if found == 0:\n",
      "                                raise RuntimeError(seq.id + ' is not in the protein sequences')\n",
      "                        for seq in method.CDS_in_frame[locus.name]:    \n",
      "                            for prot in align:\n",
      "                                if prot.id == seq.id:\n",
      "                                    i = 0\n",
      "                                    for p in str(prot.seq):\n",
      "                                        if not p == '-':\n",
      "                                            i += 1\n",
      "                                    if not i*3 == len(seq.seq):\n",
      "                                        raise RuntimeError('nuc and prot seqs have unmatched lengths for '+seq.id)\n",
      "                        aln_filename = method.id+'_'+locus.name+'.aln'\n",
      "                        AlignIO.write(align, aln_filename, 'fasta')\n",
      "                        cds_filename = method.id+'_CDS_in_frame_'+locus.name+'.fasta'\n",
      "                        stdout = os.popen('perl '+pal2nal+' '+aln_filename+' '+cds_filename + ' -nostderr').read()\n",
      "                        align = AlignIO.read(StringIO(stdout), \"clustal\",  alphabet=IUPAC.ambiguous_dna)\n",
      "                        #from Bio import CodonAlign\n",
      "                        #codon_aln = CodonAlign.build(align, method.CDS_in_frame[locus.name])\n",
      "                        #align = codon_aln\n",
      "                    method_files = glob.glob(method.id+'_*')\n",
      "                    self.alignments[locus.name+'@'+method.method_name] = align\n",
      "                method.timeit.append(time.time())\n",
      "                method.timeit.append(method.timeit[2]-method.timeit[1])\n",
      "                for f in method_files:\n",
      "                    os.remove(f)\n",
      "            self.used_methods += alignment_methods\n",
      "\n",
      "\n",
      "\n",
      "    def write_alns(self, format = 'fasta'):\n",
      "        if len(self.alignments.keys()) == 0:\n",
      "            raise IOError('Align the records first')\n",
      "        else:\n",
      "            for key in self.alignments:\n",
      "                AlignIO.write(self.alignments[key], key+'_aln.'+format, format)\n",
      "\n",
      "\n",
      "\n",
      "    def write_trimmed_alns(self, format = 'fasta'):\n",
      "        if len(self.trimmed_alignments.keys()) == 0:\n",
      "            raise IOError('Align and trimmed the records first')\n",
      "        else:\n",
      "            for key in self.trimmed_alignments.keys():\n",
      "                AlignIO.write(self.trimmed_alignments[key], key+'_trimmed_aln.'+format, format)\n",
      "\n",
      "\n",
      "\n",
      "    def tree(self, raxml_methods):\n",
      "        # to do: determine the program used and the resulting expected tree file name\n",
      "        \n",
      "        for raxml_method in raxml_methods:\n",
      "            raxml_method.timeit.append(time.time())\n",
      "            raxml_method.platform = platform_report() \n",
      "            raxml_method.platform.append('Program and version: '+ raxml_method.cmd + ': ' +\n",
      "                                         os.popen(raxml_method.cmd + ' -version').readlines()[2])\n",
      "            for trimmed_alignment in raxml_method.command_lines.keys():\n",
      "                for cline in raxml_method.command_lines[trimmed_alignment]:\n",
      "                    stdout, stderr = cline()\n",
      "                t = None\n",
      "                if raxml_method.preset == 'fa':\n",
      "                    t = Tree('RAxML_bipartitions.'+raxml_method.id+'_'+trimmed_alignment+'0')\n",
      "                elif raxml_method.preset == 'fD_fb':\n",
      "                    t = Tree('RAxML_bipartitions.'+raxml_method.id+'_'+trimmed_alignment+'1')\n",
      "                elif raxml_method.preset == 'fd_b_fb':\n",
      "                    t = Tree('RAxML_bipartitions.'+raxml_method.id+'_'+trimmed_alignment+'2')\n",
      "                    \n",
      "            \n",
      "                for n in t.traverse():\n",
      "                    n.add_feature('tree_method_id', str(raxml_method.id)+'_'+trimmed_alignment)\n",
      "                t.dist = 0\n",
      "                t.add_feature('tree_method_id', str(raxml_method.id)+'_'+trimmed_alignment)\n",
      "                \n",
      "           \n",
      "                loci_names = [i.name for i in  self.loci]       \n",
      "                concat_names = [c.name for c in self.concatenations]\n",
      "                if trimmed_alignment.partition('@')[0] in loci_names:\n",
      "                        \n",
      "                        for leaf in t:\n",
      "                            records = self.records\n",
      "                            feature = ''\n",
      "                            feature_source = ''\n",
      "                            record = ''\n",
      "                            for r in records:\n",
      "                                if r.id in leaf.name:\n",
      "                                    record = r\n",
      "                                    for f in r.features:\n",
      "                                        if f.type == 'source':\n",
      "                                            feature_source = f\n",
      "                                        elif f.qualifiers['feature_id'][0] == leaf.name:\n",
      "                                            feature = f\n",
      "                            for a in record.annotations.keys():\n",
      "                                label = 'annotation_'+a\n",
      "                                leaf.add_feature(label, record.annotations[a])\n",
      "                            for f_source_qual in feature_source.qualifiers.keys():\n",
      "                                label = 'source_'+f_source_qual\n",
      "                                leaf.add_feature(label, feature_source.qualifiers[f_source_qual][0])\n",
      "                            for f_qual in feature.qualifiers.keys():\n",
      "                                leaf.add_feature(f_qual, feature.qualifiers[f_qual][0])\n",
      "                        for l in t:\n",
      "                            t.add_feature('tree_id', trimmed_alignment+'@'+raxml_method.method_name)\n",
      "                        self.trees[trimmed_alignment+'@'+raxml_method.method_name] = [t,t.write(features=[])]\n",
      "                        \n",
      "                elif trimmed_alignment in concat_names:\n",
      "                        s = filter(lambda i: i.name == trimmed_alignment, self.concatenations)[0]\n",
      "                        for leaf in t:\n",
      "                            records = self.records\n",
      "                            feature = ''\n",
      "                            feature_source = ''\n",
      "                            record = ''\n",
      "                            for r in records:\n",
      "                                for feature in r.features:\n",
      "                                    if not feature.type == 'source':\n",
      "                                        qual_dict = get_qualifiers_dictionary(self, feature.qualifiers['feature_id'])\n",
      "                                        if s.otu_meta in qual_dict.keys() and qual_dict[s.otu_meta] == leaf.name:\n",
      "                                            for key in qual_dict.keys():\n",
      "                                                leaf.add_feature(key, qual_dict[key])\n",
      "                        for l in t:\n",
      "                            t.add_feature('tree_id', s.name+'@mixed@mixed@'+raxml_method.method_name)\n",
      "                        self.trees[s.name+'@mixed@mixed@'+raxml_method.method_name] = [t,t.write(features=[])]\n",
      "            raxml_method.timeit.append(time.time())\n",
      "            raxml_method.timeit.append(raxml_method.timeit[2]-raxml_method.timeit[1])\n",
      "            for file_name in os.listdir(os.curdir):\n",
      "                        if raxml_method.id.partition('_')[0] in file_name:\n",
      "                            os.remove(file_name)\n",
      "        self.used_methods += raxml_methods\n",
      "\n",
      "\n",
      "\n",
      "    def clear_tree_annotations(self):\n",
      "        for tree in self.trees.keys():\n",
      "            t = Tree(self.trees[tree][1])\n",
      "            t.dist = 0\n",
      "            self.trees[tree][0] = t\n",
      "\n",
      "\n",
      "\n",
      "    def write_nexml(self, output_name):\n",
      "        D = dendropy.DataSet()\n",
      "        tree_list = []\n",
      "        \n",
      "        loci_names = []\n",
      "        for locus in self.loci:\n",
      "            loci_names.append(locus.name)\n",
      "        \n",
      "        for tree_name in self.trees.keys():\n",
      "            #get aligned and trimmd aligned sequences as leaf features\n",
      "            t = self.trees[tree_name][0]\n",
      "            for l in t:\n",
      "                loc_name = tree_name.split('@')[0]\n",
      "                trim_aln_name = tree_name.rpartition('@')[0]\n",
      "                aln_name = None\n",
      "                if loc_name in loci_names:\n",
      "                    aln_name = tree_name.rsplit('@',2)[0]\n",
      "                else:\n",
      "                    trim_aln_name = trim_aln_name.split('@')[0]\n",
      "                \n",
      "                otu_feature = 'feature_id'\n",
      "                if not aln_name: # then it is a concatenation\n",
      "                    for c in self.concatenations:\n",
      "                        if c.name == loc_name:\n",
      "                            otu_feature = c.otu_meta\n",
      "                            \n",
      "                if aln_name: # Then it is a locus\n",
      "                    leaf_feature_value = getattr(l, otu_feature)\n",
      "                    alignment = self.alignments[aln_name]\n",
      "                    for record in alignment:\n",
      "                        if record.id == leaf_feature_value:\n",
      "                            l.add_feature('aligned_sequence',str(record.seq))\n",
      "                t_aln = self.trimmed_alignments[trim_aln_name]\n",
      "                    \n",
      "                leaf_feature_value = getattr(l, otu_feature)\n",
      "                for record in t_aln:\n",
      "                    if record.id == leaf_feature_value:\n",
      "                        l.add_feature('aligned_trimmed_sequence',str(record.seq))\n",
      "                    \n",
      "            tree_string = self.trees[tree_name][0].write(features=[])\n",
      "            tree = dendropy.Tree()\n",
      "            tree.read_from_string(tree_string, schema='newick', extract_comment_metadata = True)\n",
      "            tree_list.append(tree)\n",
      "        TL = dendropy.TreeList(tree_list)    \n",
      "        D.add_tree_list(TL)\n",
      "            \n",
      "        D.write_to_path(\n",
      "            output_name,\n",
      "            'nexml',\n",
      "            suppress_annotations=False,\n",
      "            annotations_as_nhx=False,\n",
      "            exclude_trees=False)\n",
      "\n",
      "\n",
      "\n",
      "    def annotate(self, fig_folder,\n",
      "    \n",
      "                 root_meta,\n",
      "                 root_value,\n",
      "    \n",
      "                 leaf_labels_txt_meta,\n",
      "                 leaf_node_color_meta=None,\n",
      "                 leaf_label_colors=None,\n",
      "    \n",
      "                 node_bg_meta=None,\n",
      "                 node_bg_color=None,\n",
      "                 \n",
      "                 node_support_dict=None,\n",
      "                 \n",
      "                 heat_map_meta = None, #list\n",
      "                 heat_map_colour_scheme=2,\n",
      "                 \n",
      "                 multifurc=None\n",
      "                 ): \n",
      "    \n",
      "            print '<html>'\n",
      "            ts = TreeStyle()\n",
      "            ts.show_leaf_name = False\n",
      "            ts.scale = 1000\n",
      "            if node_support_dict:\n",
      "                ts.legend_position=1\n",
      "                ts.legend.add_face(TextFace('Node support: ', fsize=10), column=0)\n",
      "                i = 1\n",
      "                for color in node_support_dict.keys():\n",
      "                    ts.legend.add_face(CircleFace(radius = 4, color = color), column=i)\n",
      "                    i +=1 \n",
      "                    ts.legend.add_face(TextFace(' '+str(node_support_dict[color][0])+'-'+str(node_support_dict[color][1]),\n",
      "                                                fsize=10), column=i)\n",
      "                    i += 1\n",
      "                \n",
      "            for tree in self.trees.keys():\n",
      "                                       \n",
      "                # set outgroup leaves, labels and label colors\n",
      "                outgroup_list = []\n",
      "                all_heatmap_profile_values = []\n",
      "                leaves_for_heatmap = []\n",
      "                \n",
      "                for leaf in self.trees[tree][0]:\n",
      "                    qualifiers_dictionary = get_qualifiers_dictionary(self, leaf.feature_id)\n",
      "                    leaf_label = ''\n",
      "                    for meta in leaf_labels_txt_meta:\n",
      "                        leaf_label += qualifiers_dictionary[meta]+' '\n",
      "                    leaf_label = leaf_label[:-1]\n",
      "                    fgcolor = 'black'\n",
      "                    if leaf_label_colors:\n",
      "                        for colour_name in leaf_label_colors.keys():\n",
      "                            if colour_name in qualifiers_dictionary[leaf_node_color_meta]:\n",
      "                                fgcolor = leaf_label_colors[colour_name]\n",
      "                    leaf_face = TextFace(leaf_label, fgcolor=fgcolor)\n",
      "                    leaf.add_face(leaf_face,0)\n",
      "                    if not root_value == 'mid' and root_meta in qualifiers_dictionary.keys() and root_value in qualifiers_dictionary[root_meta]:\n",
      "                        outgroup_list.append(leaf)\n",
      "                        \n",
      "                    if heat_map_meta:\n",
      "                        include = True\n",
      "                        for i in heat_map_meta:\n",
      "                            if not i in qualifiers_dictionary:\n",
      "                                include = False\n",
      "                        if include:\n",
      "                            profile = []\n",
      "                            deviation = []\n",
      "                            for meta in heat_map_meta:\n",
      "                                if meta in qualifiers_dictionary.keys():\n",
      "                                    profile.append(float(qualifiers_dictionary[meta]))\n",
      "                                    all_heatmap_profile_values.append(float(qualifiers_dictionary[meta]))\n",
      "                                    deviation.append(0.0)\n",
      "                            leaf.add_features(profile=profile)\n",
      "                            leaf.add_features(deviation=deviation)\n",
      "                            leaves_for_heatmap.append(leaf)\n",
      "                for leaf in leaves_for_heatmap:\n",
      "                    leaf.add_face(ProfileFace(max_v=float(max(all_heatmap_profile_values)),\n",
      "                                              min_v=float(min(all_heatmap_profile_values)), \n",
      "                                              center_v=float(float(max(all_heatmap_profile_values)+min(all_heatmap_profile_values))/2),\n",
      "                                              width=50, height=30,\n",
      "                                              style='heatmap',\n",
      "                                              colorscheme=heat_map_colour_scheme),\n",
      "                                    column=1, position=\"aligned\")\n",
      "                        \n",
      "                        \n",
      "                #set outgroup\n",
      "                if outgroup_list == ['mid']:\n",
      "                    try:\n",
      "                        R = self.trees[tree][0].get_midpoint_outgroup()\n",
      "                        self.trees[tree][0].set_outgroup(R)\n",
      "                        print 'rooting tree '+tree+' at midpoint'\n",
      "                    except:\n",
      "                        print 'root in '+tree+' already set correctly?'\n",
      "                    \n",
      "                elif len(outgroup_list) == 1:\n",
      "                    try:\n",
      "                        self.trees[tree][0].set_outgroup(outgroup_list[0])\n",
      "                    except:\n",
      "                        print 'root in '+tree+' already set correctly?'\n",
      "                elif len(outgroup_list) > 1:\n",
      "                    try:\n",
      "                        R = self.trees[tree][0].get_common_ancestor(outgroup_list)\n",
      "                        self.trees[tree][0].set_outgroup(R)\n",
      "                    except:\n",
      "                        print 'root in '+tree+' already set correctly?'\n",
      "                elif len(outgroup_list)==0:\n",
      "                    try:\n",
      "                        R = self.trees[tree][0].get_midpoint_outgroup()\n",
      "                        self.trees[tree][0].set_outgroup(R)\n",
      "                        print 'rooting tree '+tree+' at midpoint'\n",
      "                    except:\n",
      "                        print 'root in '+tree+' already set correctly?'\n",
      "    \n",
      "                # ladderize\n",
      "                self.trees[tree][0].ladderize()\n",
      "            \n",
      "                if multifurc:\n",
      "                    for n in self.trees[tree][0].traverse():\n",
      "                        if n.support < multifurc and not n.is_leaf():\n",
      "                            n.delete()\n",
      "    \n",
      "                # node bg colors\n",
      "                if node_bg_color:\n",
      "                    for key in node_bg_color.keys():\n",
      "                        for node in self.trees[tree][0].get_monophyletic(values=[key], target_attr=node_bg_meta):\n",
      "                            ns = NodeStyle(bgcolor=node_bg_color[key])\n",
      "                            node.set_style(ns)\n",
      "    \n",
      "                # node support\n",
      "                if node_support_dict:\n",
      "                    for node in self.trees[tree][0].traverse():\n",
      "                        for key in node_support_dict.keys():\n",
      "                            if (node.support <= node_support_dict[key][0] and\n",
      "                                node.support > node_support_dict[key][1]):\n",
      "                                node.add_face(CircleFace(radius = 5, color = key),column=0, position = \"float\")             \n",
      "                    \n",
      "                self.trees[tree][0].render(fig_folder + \"/\"+self.trees[tree][0].get_leaves()[0].tree_method_id+'.png',w=1000, tree_style=ts)\n",
      "                print('<A href=file://'+\n",
      "                       fig_folder + \"/\" + self.trees[tree][0].get_leaves()[0].tree_method_id+'.png'+\n",
      "                       '>'+self.trees[tree][0].get_leaves()[0].tree_method_id+\n",
      "                       '</A><BR>')\n",
      "            print '</html>'\n",
      "            print fig_folder\n",
      "\n",
      "\n",
      "\n",
      "#    def trim(self):\n",
      "#        for aln in self.alignments.keys():\n",
      "#            AlignIO.write(self.alignments[aln],aln+'_aln.fasta','fasta')\n",
      "#            stdout = os.popen('trimal -in '+ aln +'_aln.fasta -gappyout').read()\n",
      "#            align = AlignIO.read(StringIO(stdout), \"fasta\",  alphabet=IUPAC.ambiguous_dna)\n",
      "#            for record in align:\n",
      "#                record.description = ''\n",
      "#            self.trimmed_alignments[aln+'@'+'dummyTrimMethod'] = align\n",
      "            \n",
      "    def trim(self, list_of_Conf_objects):\n",
      "        for m in list_of_Conf_objects:\n",
      "            if isinstance(m, TrimalConf):\n",
      "                import subprocess as sub\n",
      "                for aln in m.command_lines.keys():\n",
      "                    p = sub.Popen(m.command_lines[aln], shell=True, stdout=sub.PIPE)\n",
      "                    stdout, stderr = p.communicate()\n",
      "                    #stdout = os.system(m.command_lines[aln]).stdout\n",
      "                    alphabet = IUPAC.ambiguous_dna\n",
      "                    locus_name = aln.split('@')[0]\n",
      "                    for locus in self.loci:\n",
      "                        if locus.name == locus_name and locus.char_type == 'prot':\n",
      "                            alphabet = IUPAC.protein\n",
      "                    self.trimmed_alignments[aln] = AlignIO.read(StringIO(stdout), \"fasta\",  alphabet=alphabet)\n",
      "            for file_name in os.listdir(os.curdir):\n",
      "                        if m.id.partition('_')[0] in file_name:\n",
      "                            os.remove(file_name)\n",
      "            \n",
      "\n",
      "\n",
      "##############################################################################################\n",
      "class AlnConf:\n",
      "##############################################################################################\n",
      "    \n",
      "    \"\"\"\n",
      "    >>> coi = Locus('dna', 'CDS', 'coi', ['cox1','COX1','coi','COI','CoI'])\n",
      "    >>> lsu = Locus('dna', 'rRNA', '28S', ['28s','28S','LSU rRNA','28S ribosomal RNA','28S large subunit ribosomal RNA'])\n",
      "    >>> db = Database([coi, lsu])\n",
      "    \n",
      "    # cline_str = muscle = AlnConf(db, method_name='MuscleDefaults',\n",
      "    #                                      cmd='muscle', program_name='muscle',\n",
      "    #                                     cline_args=dict())\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, db, method_name='MafftLinsi', CDSAlign=True, program_name='mafft',\n",
      "                 cmd='mafft', loci='all',\n",
      "                 cline_args=dict(localpair=True, maxiterate=1000)):\n",
      "        self.id = str(random.randint(10000,99999))+str(time.time())\n",
      "        self.method_name=method_name\n",
      "        self.CDSAlign=CDSAlign\n",
      "        self.program_name=program_name\n",
      "        self.loci = db.loci\n",
      "        if not loci == 'all':\n",
      "            self.loci = []\n",
      "            for locus_name in loci:\n",
      "                for locus in db.loci:\n",
      "                    if locus_name == locus.name:\n",
      "                        self.loci.append(locus)\n",
      "        self.CDS_proteins = {}\n",
      "        self.CDS_in_frame = {}\n",
      "        self.aln_input_strings = {}\n",
      "        self.command_lines = {}\n",
      "        self.timeit = [time.asctime()]\n",
      "        self.platform = []\n",
      "        self.cmd = cmd\n",
      "        # make defalut input files\n",
      "        if db.records_by_locus == {}:\n",
      "            db.extract_by_locus()\n",
      "        for key in db.records_by_locus.keys():\n",
      "            SeqIO.write(db.records_by_locus[key], self.id+'_'+key+'.fasta', 'fasta')    \n",
      "        for locus in self.loci:\n",
      "            # put default input file filename and string in the AlnConf object\n",
      "            input_filename=self.id+'_'+locus.name+'.fasta'\n",
      "            self.aln_input_strings[locus.name] = [open(input_filename,'r').read()]\n",
      "            # If CDS prepare reference protein input file and in frame CDS input file\n",
      "            if locus.feature_type == 'CDS' and locus.char_type == 'dna' and self.CDSAlign: \n",
      "                self.CDS_proteins[locus.name] = []\n",
      "                self.CDS_in_frame[locus.name] = []\n",
      "                for record in db.records:\n",
      "                    for feature in record.features:\n",
      "                        if (not feature.type == 'source' and 'gene' in feature.qualifiers.keys() and\n",
      "                            feature.qualifiers['gene'][0] in locus.aliases):\n",
      "                            S = feature.extract(record.seq)\n",
      "                            # Make in-frame CDS input file seq start in frame\n",
      "                            if 'codon_start' in feature.qualifiers.keys():\n",
      "                                i = feature.qualifiers['codon_start'][0]\n",
      "                                if i > 1:\n",
      "                                    S = S[(int(i)-1):]\n",
      "                            # Make in-frame CDS input file seq end in frame\n",
      "                            if len(S)%3 == 1:\n",
      "                                S = S[:-1]\n",
      "                            elif len(S)%3 == 2:\n",
      "                                S = S[:-2]  \n",
      "                            # make protein input file seq\n",
      "                            P = Seq(feature.qualifiers['translation'][0], IUPAC.protein)\n",
      "                            # Remove 3' positions that are based on partial codons\n",
      "                            while len(P)*3 > len(S):\n",
      "                                P = P[:-1]\n",
      "                            # remove complete termination codon\n",
      "                            if (len(S)/3)-1 == len(P):\n",
      "                                S = S[:-3]\n",
      "                            # make in frame cds record\n",
      "                            feature_record = SeqRecord(seq = S, id = feature.qualifiers['feature_id'][0],\n",
      "                                                               description = '')\n",
      "                            # put it in the object\n",
      "                            self.CDS_in_frame[locus.name].append(feature_record)\n",
      "                            # make protein record\n",
      "                            feature_record = SeqRecord(seq = P, id = feature.qualifiers['feature_id'][0],\n",
      "                                                       description = '')\n",
      "                            # Put the protein records in the AlnConf object\n",
      "                            self.CDS_proteins[locus.name].append(feature_record)\n",
      "                           \n",
      "                                    \n",
      "                # check same number of prot and cds objects                    \n",
      "                if len(db.records_by_locus[locus.name]) > len(self.CDS_proteins[locus.name]):\n",
      "                    raise RuntimeError('For the CDS locus '+locus.name+': more nuc seqs than prot seqs.'+\n",
      "                                       ' You may miss a \\'translate\\' or \\'gene\\' qualifier in some of '+\n",
      "                                       'the features.')\n",
      "                elif len(db.records_by_locus[locus.name]) < len(self.CDS_proteins[locus.name]):\n",
      "                    raise RuntimeError('For the CDS locus '+locus.name+': less nuc seqs than prot seqs.'+\n",
      "                                       ' You may miss a \\'translate\\' or \\'gene\\' qualifier in some of '+\n",
      "                                       'the features.')\n",
      "                unmatched = []\n",
      "                for record in self.CDS_in_frame[locus.name]:\n",
      "                    for prot in self.CDS_proteins[locus.name]:\n",
      "                        if prot.id == record.id:\n",
      "                            if not len(prot.seq)*3 == len(record.seq):\n",
      "                                unmatched.append(record.id)\n",
      "                unmatched_string = ''\n",
      "                if len(unmatched) > 0:\n",
      "                    for u in unmatched:\n",
      "                        unmatched_string += u+' '\n",
      "                    raise RuntimeError('The following CDS/protein pairs are unmatched: '+unmatched_string)\n",
      "                    \n",
      "                SeqIO.write(self.CDS_in_frame[locus.name],\n",
      "                            self.id+'_CDS_in_frame_'+locus.name+'.fasta', 'fasta')\n",
      "                input_filename2=self.id+'_CDS_in_frame_'+locus.name+'.fasta'\n",
      "                SeqIO.write(self.CDS_proteins[locus.name],\n",
      "                            self.id+'_CDS_proteins_'+locus.name+'.fasta', 'fasta')\n",
      "                input_filename=self.id+'_CDS_proteins_'+locus.name+'.fasta'\n",
      "                self.aln_input_strings[locus.name][0] = [open(input_filename,'r').read(),\n",
      "                                                         open(input_filename2,'r').read()]\n",
      "            cline = dict(dict(input=input_filename), **cline_args)\n",
      "            if self.program_name == 'mafft':\n",
      "                self.command_lines[locus.name] = MafftCommandline(cmd=cmd)\n",
      "            elif self.program_name == 'muscle':\n",
      "                self.command_lines[locus.name] = MuscleCommandline(cmd=cmd)\n",
      "            for c in cline.keys():\n",
      "                self.command_lines[locus.name].__setattr__(c,cline[c])\n",
      "            print str(self.command_lines[locus.name])\n",
      "            \n",
      "            \n",
      "            \n",
      "    def __str__(self):\n",
      "        loci_string = ''\n",
      "        for n in [i.name for i in self.loci]:\n",
      "            loci_string += n+','\n",
      "        loci_string = loci_string[:-1]\n",
      "        command_lines = ''\n",
      "        for i in self.command_lines.keys():\n",
      "            command_lines += i+': '+str(self.command_lines[i])+'\\n'\n",
      "        date = str(self.timeit[0])\n",
      "        execution = str(self.timeit[3])\n",
      "        plat = str(self.platform).replace(\",\",'\\n').replace(']','').replace(\"'\",'').replace('[','')\n",
      "        return (\"AlnConf named %s with ID %s\",         \n",
      "                \"Loci: %s \",       \n",
      "                \"Executed on: %s\",\n",
      "                \"Commands:\",\n",
      "                \"%s\",\n",
      "                \"Environment:\",    \n",
      "                \"%s\",\n",
      "                \"execution time:\",\n",
      "                \"%s\" %(self.method_name, str(self.id), loci_string, date, command_lines, plat,execution))  \n",
      "\n",
      "##############################################################################################\n",
      "class TrimalConf:\n",
      "##############################################################################################\n",
      "    def __init__(self, db, method_name='gappyout', program_name='trimal',\n",
      "                 cmd='trimal', alns='all', trimal_commands=dict(gappyout=True)):\n",
      "        \n",
      "        self.id = str(random.randint(10000,99999))+str(time.time())\n",
      "        self.method_name=method_name\n",
      "        self.program_name=program_name\n",
      "        self.alignments = db.alignments\n",
      "        if not alns == 'all':\n",
      "            self.alignments = {}\n",
      "            for aln_name in alns:\n",
      "                if aln_name in db.alignments.keys():\n",
      "                    self.alignments[aln_name] = db.alignments[aln_name]\n",
      "        self.command_lines = {}\n",
      "        self.timeit = [time.asctime()]\n",
      "        self.platform = []\n",
      "        self.cmd = cmd\n",
      "        irelevant = ['out', 'clustal', 'fasta', 'nbrf', 'nexus', 'mega',\n",
      "                     'phylip3.2', 'phylip', 'sgt', 'scc', 'sct', 'sfc',\n",
      "                     'sft','sident']\n",
      "        for aln in self.alignments:\n",
      "            input_filename = self.id+'_'+aln+'.fasta'\n",
      "            AlignIO.write(self.alignments[aln], input_filename,'fasta')\n",
      "            self.command_lines[aln] = \"%s -in %s \"%(self.cmd, input_filename)\n",
      "            for kwd in trimal_commands:\n",
      "                if kwd in irelevant:\n",
      "                    warnings.simplefilter('always')\n",
      "                    warnings.warn(\"%s is irelevant in this context and will be ignored\"%kwd)\n",
      "                else:\n",
      "                    if not trimal_commands[kwd] == None and not trimal_commands[kwd] == False:\n",
      "                        if trimal_commands[kwd] == True:\n",
      "                            self.command_lines[aln] += \"-%s \"%(kwd)\n",
      "                        else:\n",
      "                            self.command_lines[aln] += \"-%s %s \"%(kwd, trimal_commands[kwd])\n",
      "            \n",
      "            self.command_lines[aln+'@'+self.method_name] = self.command_lines[aln][:-1]\n",
      "            print self.command_lines[aln+'@'+self.method_name]\n",
      "            self.command_lines.pop(aln, None)\n",
      "        \n",
      "\n",
      "\n",
      "def use_sh_support_as_branch_support(tree_filename):\n",
      "    string = open(tree_filename,'r').read()\n",
      "    string = re.sub(r'\\[',r'[&&NHX:support=',string)\n",
      "    t = Tree(string)\n",
      "    t.dist=0\n",
      "    t.write(outfile=tree_filename)\n",
      "    #t.show()\n",
      "    \n",
      "def transfer_support_same_topo(tree_file_with_support,\n",
      "                               tree_file_without_support):\n",
      "    supported = Tree(tree_file_with_support) \n",
      "    unsupported = Tree(tree_file_without_support)\n",
      "    supported_leaf_names = sorted(supported.get_leaf_names())\n",
      "    unsupported_leaf_names = sorted(unsupported.get_leaf_names())\n",
      "    if not len(unsupported_leaf_names) == len(supported_leaf_names):\n",
      "        raise IOError(tree_file_with_support + ' and ' + tree_file_without_support +\n",
      "                      ' are not the same length')\n",
      "    for i in range(len(supported_leaf_names)):\n",
      "        if not supported_leaf_names[i] == unsupported_leaf_names[i]:\n",
      "            raise IOError('The trees do not share all leaves or leaf names')\n",
      "    same_root = supported.get_leaf_names()[0]\n",
      "    unsupported.set_outgroup(same_root)\n",
      "    supported.set_outgroup(same_root)\n",
      "    for ns in supported.traverse():\n",
      "        ns_leaves = ns.get_leaf_names()\n",
      "        if not unsupported.check_monophyly(values=ns_leaves, target_attr=\"name\"):\n",
      "            raise RuntimeError('trees do not share topology and/or all the leaf names')\n",
      "        else:\n",
      "            unsupported_ancestor = unsupported.get_common_ancestor(ns_leaves)\n",
      "            unsupported_ancestor.support = ns.support\n",
      "    unsupported.write(outfile = tree_file_without_support)    \n",
      "    \n",
      "    \n",
      "def make_raxml_partfile(tree_method, db, trimmed_alignment_name):\n",
      "\n",
      "    concatenation = None\n",
      "    for c in db.concatenations:\n",
      "        if c.name == trimmed_alignment_name:\n",
      "            concatenation = c\n",
      "    \n",
      "    #concatenation = filter(lambda concatenation: concatenation.name == trimmed_alignment_name, db.concatenations)[0]\n",
      "    \n",
      "    model = []\n",
      "    for locus in concatenation.loci:\n",
      "        part_name = None\n",
      "        for trm_aln in concatenation.used_trimmed_alns.keys():\n",
      "            if locus.name == trm_aln.partition('@')[0]:\n",
      "                part_name = trm_aln\n",
      "        if not part_name:\n",
      "            raise RuntimeError('There is no trimmed alignment for locus '+locus.name+' in concatenation '+concatenation.name)\n",
      "        part_length = concatenation.used_trimmed_alns[part_name]\n",
      "        if locus.char_type == 'prot':\n",
      "            m = None\n",
      "            if isinstance(tree_method.matrix,dict):\n",
      "                m = tree_method.matrix[locus.name]\n",
      "            elif isinstance(tree_method.matrix,str):\n",
      "                m = tree_method.matrix\n",
      "            else:\n",
      "                #todo write error\n",
      "                pass\n",
      "            model.append([m,part_name,part_length])\n",
      "        elif locus.char_type == 'dna':\n",
      "            model.append(['DNA',part_name,part_length])\n",
      "                    \n",
      "    # make partition file\n",
      "                    \n",
      "    partfile = open(tree_method.id+'_'+concatenation.name+'_partfile','wt')\n",
      "    i = 1\n",
      "    for m in model:\n",
      "        partfile.write(m[0]+', '+m[1]+'='+str(i)+'-'+str(m[2]+i-1)+'\\n')\n",
      "        i += m[2]\n",
      "    partfile.close()\n",
      "    return tree_method.id+'_'+concatenation.name+'_partfile'\n",
      "\n",
      "def make_raxml_input_matrix_file(tree_method, trimmed_alignment_name):\n",
      "    SeqIO.write(tree_method.trimmed_alignments[trimmed_alignment_name],\n",
      "                tree_method.id+'_'+trimmed_alignment_name+'.fasta','fasta')\n",
      "    return tree_method.id+'_'+trimmed_alignment_name+'.fasta'\n",
      "\n",
      "def write_raxml_clines(tree_method, db, trimmed_alignment_name):\n",
      "            \n",
      "    cline_que = 0\n",
      "\n",
      "    support_replicates = 100\n",
      "    ML_replicates = 1\n",
      "    if '-N' in tree_method.cline_args.keys():\n",
      "        ML_replicates = tree_method.cline_args['-N']\n",
      "    if '-#' in tree_method.cline_args.keys():\n",
      "        support_replicates = tree_method.cline_args['-#']\n",
      "    \n",
      "    partfile = None\n",
      "    \n",
      "    # Check if it is a concatenation and make partfile\n",
      "\n",
      "    for c in db.concatenations:\n",
      "        if c.name == trimmed_alignment_name.partition('@')[0]:\n",
      "            partfile = make_raxml_partfile(tree_method, db, trimmed_alignment_name)\n",
      "    \n",
      "    input_filename = make_raxml_input_matrix_file(tree_method, trimmed_alignment_name)\n",
      "    model = tree_method.model\n",
      "    try:\n",
      "        locus_char_type = filter(lambda locus: locus.name == trimmed_alignment_name.partition('@')[0], db.loci)[0].char_type\n",
      "    except:\n",
      "        locus_char_type = 'prot'\n",
      "    \n",
      "    if partfile:\n",
      "        model='PROT'+model+'JTT'\n",
      "    else:\n",
      "        if locus_char_type == 'dna':\n",
      "            model = 'GTR'+tree_method.model\n",
      "        elif  locus_char_type == 'prot':\n",
      "            if isinstance(tree_method.matrix,str):\n",
      "                model = 'PROT'+tree_method.model+tree_method.matrix\n",
      "            elif isinstance(tree_method.matrix,dict):\n",
      "                model = 'PROT'+tree_method.model+tree_method.matrix[trimmed_alignment_name]\n",
      "        \n",
      "    presets = {'fa': [{'-f': 'a',\n",
      "                           '-p': random.randint(0,999),\n",
      "                           '-x':  random.randint(0,999),\n",
      "                           '-s': input_filename,\n",
      "                           '-N': support_replicates,\n",
      "                           '-n': tree_method.id+'_'+trimmed_alignment_name+'0',\n",
      "                           '-m': model,\n",
      "                           '-T': tree_method.threads}\n",
      "                      ],\n",
      "                'fD_fb':[{'-f': 'D',\n",
      "                          '-p': random.randint(0,999),\n",
      "                          '-s': input_filename,\n",
      "                          '-N': ML_replicates,\n",
      "                           '-n': tree_method.id+'_'+trimmed_alignment_name+'0',\n",
      "                           '-m': model,\n",
      "                           '-T': tree_method.threads},{'-f': 'b',\n",
      "                                                       '-p': random.randint(0,999),\n",
      "                                                       '-s': input_filename,\n",
      "                                                       '-n': tree_method.id+'_'+trimmed_alignment_name+'1',\n",
      "                                                       '-m': model,\n",
      "                                                       '-T': tree_method.threads,\n",
      "                                                       '-t': 'RAxML_bestTree.'+tree_method.id+'_'+trimmed_alignment_name+'0',\n",
      "                                                       '-z': 'RAxML_rellBootstrap.'+tree_method.id+'_'+trimmed_alignment_name+'0'}\n",
      "                         ],\n",
      "                'fd_b_fb':[{'-f': 'd',\n",
      "                          '-p': random.randint(0,999),\n",
      "                          '-s': input_filename,\n",
      "                          '-N': ML_replicates,\n",
      "                           '-n': tree_method.id+'_'+trimmed_alignment_name+'0',\n",
      "                           '-m': model,\n",
      "                           '-T': tree_method.threads},{\n",
      "                                                       '-p': random.randint(0,999),\n",
      "                                                       '-b': random.randint(0,999),\n",
      "                                                       '-s': input_filename,\n",
      "                                                       '-#': support_replicates,\n",
      "                                                       '-n': tree_method.id+'_'+trimmed_alignment_name+'1',\n",
      "                                                       '-m': model,\n",
      "                                                       '-T': tree_method.threads},\n",
      "                                                       {'-f': 'b',\n",
      "                                                       '-p': random.randint(0,999),\n",
      "                                                       '-s': input_filename,\n",
      "                                                       '-n': tree_method.id+'_'+trimmed_alignment_name+'2',\n",
      "                                                       '-m': model,\n",
      "                                                       '-T': tree_method.threads,\n",
      "                                                       '-t': 'RAxML_bestTree.'+tree_method.id+'_'+trimmed_alignment_name+'0',\n",
      "                                                       '-z': 'RAxML_bootstrap.'+tree_method.id+'_'+trimmed_alignment_name+'1'}\n",
      "                         ]\n",
      "                }\n",
      "\n",
      "    if partfile:\n",
      "        for preset in presets.keys():\n",
      "            for cline in range(len(presets[preset])):\n",
      "                presets[preset][cline] = dict({'-q': partfile}, **presets[preset][cline])               \n",
      "    return presets[tree_method.preset] \n",
      "\n",
      "\n",
      "class RaxmlConf:\n",
      "    \n",
      "    \n",
      "    def __init__(self, db, method_name='fa', program_name='raxmlHPC-PTHREADS-SSE3',\n",
      "                 cmd='raxmlHPC-PTHREADS-SSE3', preset = 'fa', alns='all', model='GAMMA', matrix='JTT', threads=4,\n",
      "                 cline_args={}):\n",
      "        self.id = str(random.randint(10000,99999))+str(time.time())\n",
      "        self.method_name=method_name\n",
      "        self.program_name=program_name\n",
      "        self.preset = preset\n",
      "        self.cline_args = cline_args\n",
      "        self.model = model\n",
      "        self.matrix = matrix\n",
      "        self.threads = threads\n",
      "        self.trimmed_alignments = db.trimmed_alignments\n",
      "        if not alns == 'all':\n",
      "            self.trimmed_alignments = {}\n",
      "            for aln_name in alns:\n",
      "                if aln_name in db.trimmed_alignments.keys():\n",
      "                    self.trimmed_alignments[aln_name] = db.trimmed_alignments[aln_name]\n",
      "        self.aln_input_strings = {}\n",
      "        self.command_lines = {}\n",
      "        self.timeit = [time.asctime()]\n",
      "        self.platform = []\n",
      "        self.cmd = cmd\n",
      "        \n",
      "        for trimmed_alignment in self.trimmed_alignments.keys():\n",
      "            self.command_lines[trimmed_alignment] = []\n",
      "            command_lines = write_raxml_clines(self, db, trimmed_alignment)\n",
      "            for command_line in command_lines:\n",
      "                cline_object = RaxmlCommandline(cmd=cmd)\n",
      "                for c in command_line.keys():\n",
      "                    cline_object.__setattr__(c,command_line[c])\n",
      "                self.command_lines[trimmed_alignment].append(cline_object)\n",
      "                print str(cline_object)\n",
      "\n",
      "\n",
      "\n",
      "from pylab import *\n",
      "import random\n",
      "\n",
      "def draw_boxplot(dictionary, y_axis_label, figs_folder): #'locus':[values]\n",
      "    import numpy as np\n",
      "    import matplotlib.pyplot as plt\n",
      "    items = dictionary.items()\n",
      "    items.sort()\n",
      "    \n",
      "    data = [locus[1] for locus in items]\n",
      "        \n",
      "    fig, ax1 = plt.subplots()\n",
      "    #plt.subplots_adjust(left=0.075, right=0.95, top=0.9, bottom=0.25)\n",
      "\n",
      "    #bp = plt.boxplot(data, widths=0.75, patch_artist=True)\n",
      "    bp = plt.boxplot(data, patch_artist=True)\n",
      "    \n",
      "    for box in bp['boxes']:\n",
      "    # change outline color\n",
      "        box.set( color='black', linewidth=1)\n",
      "        \n",
      "    # change fill color\n",
      "        box.set( facecolor = 'red', alpha=0.85 )\n",
      "        \n",
      "    # change color, linestyle and linewidth of the whiskers\n",
      "    for whisker in bp['whiskers']:\n",
      "        whisker.set(color='gray', linestyle='solid', linewidth=2.0)\n",
      "\n",
      "    # change color and linewidth of the caps\n",
      "    for cap in bp['caps']:\n",
      "        cap.set(color='gray', linewidth=2.0)\n",
      "\n",
      "    # change color and linewidth of the medians\n",
      "    for median in bp['medians']:\n",
      "        #median.set(color='#b2df8a', linewidth=2)\n",
      "        median.set(color='white', linewidth=2)\n",
      "\n",
      "    # change the style of fliers and their fill\n",
      "    for flier in bp['fliers']:\n",
      "        flier.set(marker='o', color='#e7298a', alpha=0.5)\n",
      "    \n",
      "    # Add a light horizontal grid to the plot\n",
      "    ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',\n",
      "              alpha=0.7)\n",
      "    \n",
      "    # Hide these grid behind plot objects\n",
      "    ax1.set_axisbelow(True)\n",
      "    \n",
      "    #set title and axis labels\n",
      "    #ax1.set_title('Sequence length distribution per locus\\n', size=18)\n",
      "    \n",
      "    xlabels = [locus[0] for locus in items]\n",
      "    xticks(range(len(data)+1)[1:], xlabels, size=14, rotation='vertical')\n",
      "    #subplots_adjust(left=0.3, bottom=0.8)\n",
      "    \n",
      "    ax1.set_ylabel(y_axis_label, size=18)\n",
      "    \n",
      "    name = str(random.randint(1000,2000))\n",
      "    fig.savefig(figs_folder + '/' + name +'.png')\n",
      "    close('all')\n",
      "    return figs_folder + '/' + name+'.png'\n",
      "    \n",
      "def report_methods(db, figs_folder):\n",
      "        report_lines = ['<html>','<head>','<h1>']\n",
      "    \n",
      "        head = 'reprophylo analysis from '+str(time.asctime())\n",
      "        #=====================================================\n",
      "        report_lines.append(head)\n",
      "        report_lines += ['</h1>','</head>','<body>','']\n",
      "        \n",
      "        report_lines += ['<h2>','Data','</h2>', '']\n",
      "        \n",
      "        title = 'species representation in sequence data'.title()\n",
      "        report_lines += ('<h3>', title, '</h3>', '', '<pre>')\n",
      "        #--------------------------------------------------------\n",
      "        \n",
      "        outfile_name= str(random.randint(1000,2000))\n",
      "        db.species_vs_loci(outfile_name)\n",
      "        with open(outfile_name, 'rb') as csvfile:\n",
      "            sp_vs_lc = list(csv.reader(csvfile, delimiter='\\t', quotechar='|'))\n",
      "            field_sizes = []\n",
      "            for i in range(len(sp_vs_lc[0])):\n",
      "                lengths = []\n",
      "                for row in sp_vs_lc:\n",
      "                    lengths.append(len(row[i]))\n",
      "                field_sizes.append(max(lengths))\n",
      "            for row in sp_vs_lc:\n",
      "                string = ''\n",
      "                for i in range(len(row)):\n",
      "                    string += row[i].ljust(field_sizes[i]+3)\n",
      "                report_lines.append(string)\n",
      "        \n",
      "        os.remove(outfile_name)\n",
      "        \n",
      "        \n",
      "        if len(db.records_by_locus.keys())>0:\n",
      "            scale = str(len(db.records_by_locus.keys())*200)\n",
      "            lengths_dict = {}\n",
      "            for locus_name in db.records_by_locus.keys():\n",
      "                lengths_dict[locus_name] = []\n",
      "                for record in db.records_by_locus[locus_name]:\n",
      "                    lengths_dict[locus_name].append(len(record.seq))\n",
      "            fig_filename = draw_boxplot(lengths_dict, 'Seq length (bp)', figs_folder)\n",
      "            title = 'Distribution of sequence lengths'\n",
      "            report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "            if os.path.isfile(fig_filename):\n",
      "                data_uri = open(fig_filename, 'rb').read().encode('base64').replace('\\n', '')\n",
      "                img_tag = '<img height=400 width='+scale+' src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                report_lines.append(img_tag)\n",
      "                os.remove(fig_filename)\n",
      "            \n",
      "            for stat in ('GC_content', 'nuc_degen_prop', 'prot_degen_prop'):\n",
      "                stat_dict = {}\n",
      "                ylabel = 'GC ontent (%)'\n",
      "                if not stat == 'GC_content':\n",
      "                    ylabel = 'Aambiguous positions (prop)'\n",
      "                for locus_name in db.records_by_locus.keys():\n",
      "                    stat_dict[locus_name] = []\n",
      "                    for i in db.records_by_locus[locus_name]:\n",
      "                        for record in db.records:\n",
      "                            for feature in record.features:\n",
      "                                if feature.qualifiers['feature_id'][0] == i.id:\n",
      "                                    if stat in feature.qualifiers.keys():\n",
      "                                        stat_dict[locus_name].append(float(feature.qualifiers[stat][0]))\n",
      "                fig_filename = draw_boxplot(stat_dict, ylabel, figs_folder)\n",
      "                title = 'Distribution of sequence statistic \\\"'+stat+'\\\"'\n",
      "                report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "                if os.path.isfile(fig_filename):\n",
      "                    data_uri = open(fig_filename, 'rb').read().encode('base64').replace('\\n', '')\n",
      "                    img_tag = '<img height=400 width='+scale+' src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                    report_lines.append(img_tag)\n",
      "                    os.remove(fig_filename)\n",
      "                    \n",
      "        composed_concatenations = []\n",
      "        for c in db.concatenations:\n",
      "            if c.name in db.trimmed_alignments.keys():\n",
      "                composed_concatenations.append(c)\n",
      "        \n",
      "        \n",
      "        for c in composed_concatenations:\n",
      "            \n",
      "            title = ('content of concatenation \\\"' + c.name + '\\\"').title()\n",
      "            report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "            \n",
      "            report_lines.append('Rules for  \\\"' + c.name + '\\\":')\n",
      "            rule_1 = 'OTUs must have the loci: '\n",
      "            for locus in c.concat_must_have_all_of:\n",
      "                rule_1 += locus + ', '\n",
      "            report_lines.append(rule_1)\n",
      "            rule_2 = 'OTUs must have at least one of each group: '\n",
      "            for group in c.concat_must_have_one_of:\n",
      "                rule_2 += str(group) +', '\n",
      "            report_lines += (rule_2, '')\n",
      "            \n",
      "            \n",
      "            otus = c.feature_id_dict.keys()\n",
      "            loci = [locus.name for locus in c.loci]\n",
      "            \n",
      "            otus_max_length = max([len(i) for i in otus])+33\n",
      "            loci_columns_max_length = []\n",
      "            \n",
      "            for locus in loci:\n",
      "                lengths = [len(locus)]\n",
      "                for otu in otus:\n",
      "                    if locus in c.feature_id_dict[otu].keys():\n",
      "                        lengths.append(len(c.feature_id_dict[otu][locus]))\n",
      "                    else:\n",
      "                        lengths.append(0)\n",
      "                loci_columns_max_length.append(max(lengths)+3)\n",
      "                \n",
      "            concat_header = ''.ljust(otus_max_length)\n",
      "            for i in range(len(loci)):\n",
      "                concat_header += loci[i].ljust(loci_columns_max_length[i])\n",
      "            report_lines += (concat_header, '~'*len(concat_header))\n",
      "                \n",
      "            for otu in otus:\n",
      "                otu_species = ''\n",
      "                for locus in loci:\n",
      "                    if locus in c.feature_id_dict[otu].keys():\n",
      "                        feature_qualifiers = get_qualifiers_dictionary(db, c.feature_id_dict[otu][locus])\n",
      "                        if 'source_organism' in feature_qualifiers.keys():\n",
      "                            otu_species = feature_qualifiers['source_organism']\n",
      "                    \n",
      "                concat_line = (otu+' '+otu_species).ljust(otus_max_length)\n",
      "                for i in range(len(loci)):\n",
      "                    if loci[i] in c.feature_id_dict[otu].keys():\n",
      "                        concat_line += c.feature_id_dict[otu][loci[i]].ljust(loci_columns_max_length[i])\n",
      "                    else:\n",
      "                        concat_line += ''.ljust(loci_columns_max_length[i])\n",
      "                report_lines.append(concat_line)\n",
      "\n",
      "        report_lines += ['</pre>', '<h2>','Methods','</h2>', '<pre>', '']\n",
      "        \n",
      "        \n",
      "        for method in db.used_methods:\n",
      "            if isinstance(method,list) and(method[0] == 'AlnConf' or method[0] == 'RaxmlConf'):\n",
      "                title = method[0]\n",
      "                report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "                for i in method[1:]:\n",
      "                    report_lines.append('<strong>'+str(i[0])+'</strong>')\n",
      "                    report_lines.append(str(i[1]).replace(',','<br>'))\n",
      "            \n",
      "            elif isinstance(method, AlnConf):\n",
      "                title = 'Seuqence Alignment Method \\\"'+method.method_name+'\\\", method ID: '+method.id\n",
      "                report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "                #--------------------------------------------------------\n",
      "                align_line = 'Included loci :'\n",
      "                for locus in [locus.name for locus in method.loci]:\n",
      "                    align_line += locus + ', '\n",
      "                report_lines.append(align_line)\n",
      "                report_lines.append('Total execution time: '+str(method.timeit[3])+' sec\\'')\n",
      "                report_lines.append('Performed on: '+str(method.timeit[0]))\n",
      "                report_lines += method.platform\n",
      "                report_lines.append('')\n",
      "\n",
      "                report_lines.append('Command lines:')\n",
      "                for cline in method.command_lines.keys():\n",
      "                    report_lines.append('Alignment \\\"'+cline+'\\\":')\n",
      "                    report_lines.append('<pre style=\"white-space:normal;\">')\n",
      "                    report_lines.append(str(method.command_lines[cline]))\n",
      "                    report_lines.append('</pre>')\n",
      "                    \n",
      "            elif isinstance(method, RaxmlConf):\n",
      "                title = 'Raxml Tree Reconstruction Method \\\"'+method.method_name+'\\\", method ID: '+method.id\n",
      "                report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "                #--------------------------------------------------------\n",
      "                tree_line = 'Included alignments :'\n",
      "                for aln in method.trimmed_alignments.keys():\n",
      "                    tree_line += aln + ', '\n",
      "                report_lines.append(tree_line)\n",
      "                report_lines.append('Total execution time: '+str(method.timeit[3])+' sec\\'')\n",
      "                report_lines.append('Performed on: '+str(method.timeit[0]))\n",
      "                report_lines += method.platform\n",
      "                report_lines.append('')\n",
      "\n",
      "                report_lines.append('Command lines:')\n",
      "                for aln in method.command_lines.keys():\n",
      "                    report_lines.append('Alignment \\\"'+aln+'\\\":')\n",
      "                    for cline in method.command_lines[aln]:\n",
      "                        report_lines.append('<pre style=\"white-space:normal;\">')\n",
      "                        report_lines.append(str(cline))\n",
      "                        report_lines.append('</pre>')\n",
      "                    report_lines.append('')\n",
      "                \n",
      "        \n",
      "        \n",
      "        report_lines += ('</pre>', '','') \n",
      "        \n",
      "        \n",
      "        if len(db.alignments.keys())>0:                    \n",
      "            title = 'Alignment statistics before trimming'\n",
      "            report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "            report_lines += ['</pre>', '<h4>','Trimal\\'s Residue Similarity Score (-scc)','</h4>', '<pre>', '']\n",
      "            fig_file = draw_trimal_scc(db, 2, figs_folder, trimmed=False)\n",
      "            if os.path.isfile(fig_file):\n",
      "                    data_uri = open(fig_file, 'rb').read().encode('base64').replace('\\n', '')\n",
      "                    img_tag = '<img src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                    report_lines.append(img_tag)\n",
      "                    os.remove(fig_file)\n",
      "            report_lines += ['</pre>', '<h4>','Trimal\\'s column gap gcore (-sgc)','</h4>', '<pre>', '']\n",
      "            fig_file = draw_trimal_scc(db, 2, figs_folder, trimmed=False, alg='-sgc')\n",
      "            if os.path.isfile(fig_file):\n",
      "                    data_uri = open(fig_file, 'rb').read().encode('base64').replace('\\n', '')\n",
      "                    img_tag = '<img src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                    report_lines.append(img_tag)\n",
      "                    os.remove(fig_file)\n",
      "               \n",
      "        if len(db.trimmed_alignments.keys())>0:          \n",
      "            title = 'Alignment statistics after trimming'\n",
      "            report_lines += ('</pre>', '<h3>', title, '</h3>', '<pre>', '')\n",
      "            report_lines += ['</pre>', '<h4>','\"Trimal\\'s Residue Similarity Score (-scc)','</h4>', '<pre>', '']\n",
      "            fig_file = draw_trimal_scc(db, 2, figs_folder, trimmed=True)\n",
      "            if os.path.isfile(fig_file):\n",
      "                    data_uri = open(fig_file, 'rb').read().encode('base64').replace('\\n', '')\n",
      "                    img_tag = '<img src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                    report_lines.append(img_tag)\n",
      "                    os.remove(fig_file)\n",
      "            report_lines += ['</pre>', '<h4>','Trimal\\'s column gap gcore (-sgc)','</h4>', '<pre>', '']\n",
      "            fig_file = draw_trimal_scc(db, 2, figs_folder, trimmed=True, alg='-sgc')\n",
      "            if os.path.isfile(fig_file):\n",
      "                    data_uri = open(fig_file, 'rb').read().encode('base64').replace('\\n', '')\n",
      "                    img_tag = '<img src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                    report_lines.append(img_tag)\n",
      "                    os.remove(fig_file)\n",
      "        \n",
      "        if len(db.trees.keys())>1:        \n",
      "            report_lines += ('','<h1>Robinson-Foulds distances </h1>','')\n",
      "            RF_filename, legend = calc_rf(db, figs_folder)\n",
      "            scale = str(len(legend)*60)\n",
      "            if os.path.isfile(RF_filename):\n",
      "                    data_uri = open(RF_filename, 'rb').read().encode('base64').replace('\\n', '')\n",
      "                    img_tag = '<img height='+scale+' width='+scale+' src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                    report_lines.append(img_tag)\n",
      "                    os.remove(RF_filename)\n",
      "            \n",
      "            report_lines.append('<h3>Legend<h3><pre>')\n",
      "            report_lines += legend\n",
      "            report_lines.append('</pre>')\n",
      "        \n",
      "        report_lines += ('<h1>Trees</h1>','')\n",
      "        \n",
      "        \n",
      "        for tree in db.trees.keys():\n",
      "            report_lines += ('<h2>'+tree.split('@')[0]+'</h2>',\n",
      "                             '<h3>Alignment method: '+tree.split('@')[1]+'</h3>',\n",
      "                             '<h3>Trimming method: '+tree.split('@')[2]+'</h3>',\n",
      "                             '<h3>Tree method: '+tree.split('@')[3]+'</h3>',\n",
      "                             '<pre style=\"white-space:normal;\">',\n",
      "                             'Tree Method ID: '+db.trees[tree][0].get_leaves()[0].tree_method_id,'</pre>')\n",
      "            \n",
      "            report_lines += ('<h3>newick format</h3>','','<pre style=\"white-space:normal;\">',db.trees[tree][0].write(),'</pre>','')\n",
      "            report_lines += ('<h3>nhx format</h3>','','<pre>',db.trees[tree][1],'</pre>','','','','')\n",
      "            \n",
      "            \n",
      "            \n",
      "            if os.path.isfile(figs_folder+'/'+db.trees[tree][0].get_leaves()[0].tree_method_id+'.png'):\n",
      "                data_handle = open(figs_folder+'/'+db.trees[tree][0].get_leaves()[0].tree_method_id+'.png','rb')\n",
      "                data_uri = data_handle.read().encode('base64').replace('\\n', '')\n",
      "                data_handle.close()\n",
      "                img_tag = '<img width=500 src=\"data:image/png;base64,{0}\">'.format(data_uri)\n",
      "                report_lines.append(img_tag)\n",
      "                \n",
      "        report_lines.append('</body>')\n",
      "        report_lines.append('</html>')\n",
      "            \n",
      "        return report_lines\n",
      "    \n",
      "        \n",
      "def pickle_db(db, pickle_file_name):\n",
      "        import os\n",
      "        if os.path.exists(pickle_file_name):\n",
      "            os.remove(pickle_file_name)\n",
      "        import cloud.serialization.cloudpickle as pickle\n",
      "        output = open(pickle_file_name,'wb')\n",
      "        pickle.dump(db, output)\n",
      "        output.close()\n",
      "        if __builtin__.git:\n",
      "            import rpgit\n",
      "            rpgit.gitAdd(pickle_file_name)\n",
      "            comment = \"A pickled Database from %s\" % time.asctime()\n",
      "            rpgit.gitCommit(comment) \n",
      "            \n",
      "        return pickle_file_name\n",
      "    \n",
      "def unpickle_db(pickle_file_name):\n",
      "        import cloud.serialization.cloudpickle as pickle\n",
      "        pickle_handle = open(pickle_file_name, 'rb')\n",
      "        pkl_db = pickle.pickle.load(pickle_handle)\n",
      "        new_db = Database(pkl_db.loci)\n",
      "        attr_names = ['alignments',\n",
      "                      'concatenations',\n",
      "                      'records',\n",
      "                      'records_by_locus',\n",
      "                      'trees',\n",
      "                      'trimmed_alignments',\n",
      "                      ]\n",
      "        \n",
      "        for attr_name in attr_names:\n",
      "           setattr(new_db,attr_name,getattr(pkl_db,attr_name))\n",
      "            \n",
      "        for i in pkl_db.used_methods:\n",
      "            if isinstance(i, list) and (i[0] == 'AlnConf' or i[0] == 'RaxmlConf' or i[0] == 'TrimalConf'):\n",
      "                new_db.used_methods.append(i)\n",
      "            else:\n",
      "                \n",
      "                include = ['id',\n",
      "                           'method_name',\n",
      "                           'CDSAlign',\n",
      "                           'program_name',\n",
      "                           'loci',\n",
      "                           'alns'\n",
      "                           'command_lines',\n",
      "                           'timeit',\n",
      "                           'platform',\n",
      "                           'cmd',\n",
      "                           'preset',\n",
      "                           'model',\n",
      "                           'trimmed_alignments']\n",
      "            \n",
      "                method_list = []\n",
      "                if isinstance(i, AlnConf):\n",
      "                    method_list.append('AlnConf')\n",
      "                elif isinstance(i, RaxmlConf):\n",
      "                    method_list.append('RaxmlConf')\n",
      "                elif isinstance(i, TrimalConf):\n",
      "                    method_list.append('TrimalConf')\n",
      "                for attr in include:\n",
      "                    if attr in dir(i):\n",
      "                        method_list.append([attr, str(getattr(i,attr))])\n",
      "                new_db.used_methods.append(method_list)\n",
      "        return new_db\n",
      "\n",
      "def publish(db, folder_name, figures_folder):\n",
      "    \n",
      "    import os, time\n",
      "    folder = None\n",
      "    zip_file = None\n",
      "    if folder_name.endswith('.zip'):\n",
      "        zip_file = folder_name\n",
      "        folder = folder_name[:-4]\n",
      "    else:\n",
      "        folder = folder_name\n",
      "        zip_file = folder_name + '.zip'\n",
      "    if os.path.exists(folder) or os.path.exists(zip_file):\n",
      "        raise IOError(folder_name + ' already exists')\n",
      "    \n",
      "    os.makedirs(folder)\n",
      "    db.write(folder+'/tree_and_alns.nexml','nexml')\n",
      "    db.write(folder+'/sequences_and_metadata.gb','genbank')\n",
      "    report = open(folder+'/report.html','wt')\n",
      "    for line in report_methods(db, figures_folder):\n",
      "        report.write(line + '\\n')\n",
      "    report.close()\n",
      "\n",
      "    for tree in db.trees.keys():\n",
      "        if os.path.isfile(figures_folder+'/'+db.trees[tree][0].get_leaves()[0].tree_method_id+'.png'):\n",
      "            from shutil import copyfile\n",
      "            copyfile(figures_folder+'/'+db.trees[tree][0].get_leaves()[0].tree_method_id+'.png',\n",
      "                     folder+'/'+db.trees[tree][0].get_leaves()[0].tree_method_id+'.png')\n",
      "            \n",
      "         \n",
      "    pickle_name = time.strftime(\"%a_%d_%b_%Y_%X\", time.gmtime())+'.pkl'\n",
      "    pickle_db(db, folder + '/' + pickle_name)\n",
      "\n",
      "    \n",
      "    import zipfile, shutil\n",
      "    \n",
      "    zf = zipfile.ZipFile(zip_file, \"w\")\n",
      "    for dirname, subdirs, files in os.walk(folder):\n",
      "        zf.write(dirname)\n",
      "        for filename in files:\n",
      "            zf.write(os.path.join(dirname, filename))\n",
      "    zf.close()\n",
      "    shutil.rmtree(folder)\n",
      "    \n",
      "def calc_rf(db, figs_folder):\n",
      "    meta = 'feature_id'\n",
      "    if len(db.concatenations) > 0:\n",
      "        meta = db.concatenations[0].otu_meta\n",
      "\n",
      "    trees = db.trees.keys()\n",
      "\n",
      "    data = []\n",
      "\n",
      "    for t1 in trees:\n",
      "        line = []\n",
      "        dupT1 = Tree(db.trees[t1][0].write())\n",
      "        for l in dupT1:\n",
      "            for record in db.records:\n",
      "                for feature in record.features:\n",
      "                    if feature.qualifiers['feature_id'][0] == l.name and meta in feature.qualifiers.keys():\n",
      "                        l.name = feature.qualifiers[meta][0]\n",
      "        for t2 in trees:\n",
      "            dupT2 = Tree(db.trees[t2][0].write())\n",
      "            for l in dupT2:\n",
      "                for record in db.records:\n",
      "                    for feature in record.features:\n",
      "                        if feature.qualifiers['feature_id'][0] == l.name and meta in feature.qualifiers.keys():\n",
      "                            l.name = feature.qualifiers[meta][0]\n",
      "            rf, max_rf, common_leaves, parts_t1, parts_t2 = dupT1.robinson_foulds(dupT2)        \n",
      "            line.append(rf/float(max_rf))        \n",
      "        data.append(line)   \n",
      "    \n",
      "    row_labels = [str(i) for i in range(len(trees))]\n",
      "    column_labels = row_labels\n",
      "    legend = ['#'.ljust(10,' ')+'LOCUS'.ljust(20,' ')+'ALIGNMENT METHOD'.ljust(20,' ')+'TRIMMING METHOD'.ljust(20,' ')+'TREE METHOD'.ljust(20,' ')]\n",
      "    for i in trees:\n",
      "        line = str(trees.index(i)).ljust(10,' ')\n",
      "        for val in i.split('@'):\n",
      "            line += val.ljust(20,' ')\n",
      "        legend.append(line)\n",
      "    fig, ax = plt.subplots()\n",
      "    data = np.array(data)\n",
      "    heatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n",
      "\n",
      "    # put the major ticks at the middle of each cell\n",
      "    ax.set_xticks(np.arange(data.shape[0])+0.5, minor=False)\n",
      "    ax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)\n",
      "\n",
      "    # want a more natural, table-like display\n",
      "    ax.invert_yaxis()\n",
      "    ax.xaxis.tick_top()\n",
      "\n",
      "    ax.set_xticklabels(row_labels, minor=False, size=14, rotation='vertical')\n",
      "    ax.set_yticklabels(column_labels, minor=False, size=14)\n",
      "    #fig.set_size_inches(12.5,12.5)\n",
      "    fig.colorbar(heatmap, cmap=plt.cm.Blues)\n",
      "    name = str(random.randint(1000,2000))\n",
      "    fig.savefig(figs_folder + '/' + name +'.png')\n",
      "    close('all')\n",
      "    return figs_folder + '/' + name+'.png', legend \n",
      "\n",
      "def draw_trimal_scc(db, num_col, figs_folder, trimmed=False, alg = '-scc'):\n",
      "    import pandas as pd\n",
      "    import matplotlib.pyplot as plt\n",
      "    import random, os\n",
      "    from Bio import AlignIO\n",
      "    \n",
      "    # get the alignment objects\n",
      "    #-------------------------#\n",
      "    alignments = db.alignments.items()\n",
      "    if trimmed:\n",
      "        alignments = db.trimmed_alignments.items()\n",
      "    num_alns = len(alignments)\n",
      "\n",
      "    subplots_arrangement = []\n",
      "    #-----------------------#\n",
      "    num_rows = round(float(num_alns)/num_col)\n",
      "    if num_rows < float(num_alns)/num_col:\n",
      "        num_rows += 1\n",
      "        \n",
      "    fig = plt.figure(figsize=(10*num_col,2.3*num_rows), dpi=80, frameon = False)\n",
      "    plt.subplots_adjust(hspace = 0.8)\n",
      "    subplots_arrangement += [num_rows, num_col]\n",
      "\n",
      "    \n",
      "    #Calc with trimal and plot\n",
      "    #------------------------#\n",
      "    for i in range(1,num_alns+1):\n",
      "        import subprocess as sub\n",
      "        subplot_position = subplots_arrangement +[i]\n",
      "        aln_name = alignments[i-1][0]\n",
      "        aln_obj = alignments[i-1][1]\n",
      "        name = str(random.randint(1000,2000))+'_'+aln_name+'_for_trimal_graph.fasta'\n",
      "        AlignIO.write(aln_obj, name, 'fasta')\n",
      "        stderr = open('stderr','wt')\n",
      "        #stdout = os.popen('trimal '+alg+' -in '+name)#.read()\n",
      "        stdout = sub.Popen(\"trimal \"+alg+\" -in \" + name,\n",
      "                       shell=True, stdout=sub.PIPE, stderr=stderr).stdout\n",
      "        stderr.close()\n",
      "        var = pd.read_table(stdout, sep='\\t+', skiprows=3, engine='python')\n",
      "        os.remove('stderr')\n",
      "        if alg == '-scc':\n",
      "            var.columns = ['position', 'variability']\n",
      "        elif alg == '-sgc':\n",
      "            var.columns = ['position', 'pct_gaps', 'gap_score']\n",
      "        \n",
      "        #Plot residue similarity figure, for nucleotides this is identity value \n",
      "        fig.add_subplot(subplot_position[0], subplot_position[1], subplot_position[2])\n",
      "        if alg == '-scc':\n",
      "            var.variability.plot(color='g',lw=2)\n",
      "        elif alg == '-sgc':\n",
      "            var.pct_gaps.plot(color='g',lw=2)\n",
      "        plt.title(aln_name.replace('@',' '), fontsize=14)\n",
      "        plt.axis([1,len(aln_obj[0].seq), 0, 1.1]) #0 to 1 scale for y axis\n",
      "        xlab = \"alignment position\"\n",
      "        ylab = \"similarity score\"\n",
      "        if alg == '-sgc':\n",
      "            ylab = \"percent gaps\"\n",
      "            plt.axis([1, len(aln_obj[0].seq), 0, 110]) #0 to 100 scale for y axis, ie percent\n",
      "        plt.xlabel(xlab, fontsize=10)\n",
      "        plt.ylabel(ylab, fontsize=10)\n",
      "        plt.grid(True)\n",
      "        os.remove(name)\n",
      "    figname = str(random.randint(1000,2000))\n",
      "    fig.savefig(figs_folder + '/' + figname +'.png')\n",
      "    plt.close('all')\n",
      "    return figs_folder + '/' + figname+'.png'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import doctest\n",
      "    doctest.testmod()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting reprophylo.py\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file genbank_synonyms\n",
      "[\n",
      "['coi', 'COX1', 'cox1', 'COI', 'cox I','coxI', 'cytochrome c oxidase subunit I'],\n",
      "['coii', 'COX2', 'cox2', 'COII', 'cox II', 'cytochrome c oxidase subunit II'],\n",
      "['coiii', 'COX3', 'cox3', 'COIII', 'cox III','cytochrome c oxidase subunit III'],\n",
      "['18s','18S','SSU rRNA','18S ribosomal RNA','small subunit 18S ribosomal RNA', '18S rRNA'],\n",
      "['28s','28S','LSU rRNA','28S ribosomal RNA','28S large subunit ribosomal RNA'],\n",
      "['ATP6','atp6'],\n",
      "['ATP8','atp8'],\n",
      "['ATP9','atp9'],\n",
      "['cytb','CYTB','Cytb', 'cytochrome B','cytochrome b'],\n",
      "['Ef1a', 'elongation factor 1-alpha','elongation factor-1 alpha'],\n",
      "['nad1','ND1','nadh1', 'NADH dehydrogenase subunit 1'],\n",
      "['nad2','ND2','nadh2', 'NADH dehydrogenase subunit 2'],\n",
      "['nad3','ND3','nadh3', 'NADH dehydrogenase subunit 3'],\n",
      "['nad4','ND4','nadh4', 'NADH dehydrogenase subunit 4'],\n",
      "['nad4l','ND4L','nadh4l', 'nadh4L', 'NADH dehydrogenase subunit 4l', 'NADH dehydrogenase subunit 4L'],\n",
      "['nad5','ND5','nadh5', 'NADH dehydrogenase subunit 5'],\n",
      "['nad6','ND6','nadh6'],\n",
      "['12S ribosomal RNA','12S rRNA'],\n",
      "['16S ribosomal RNA','16S rRNA'],\n",
      "['C_mos', 'C-mos','c-mos','C-MOS'],\n",
      "['GAPDH','gapdh']\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting genbank_synonyms\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python reprophylo.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reprophylo.py:610: UserWarning: Version control off\r\n",
        "  warnings.warn('Version control off')\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "def random_rgb():\n",
      "    import random\n",
      "    r = lambda: random.randint(0,255)\n",
      "    return ('#%02X%02X%02X' % (r(),r(),r()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print random_rgb()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#39156D\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tree reconstruction tools in reprophylo\n",
      "I think that a tool that will simplify the RAxML command line while providing all the possible options will be more appreciated as a starting point than a small range of programs for which you have to write the command line from scratch. In addition, I think that instead of TreeReconstructionMethod object, as I did for alignments, I should make program specific object, ie, RaxmlMethodObject. The reason for this is that the user would usually want to fit the best alignment approach for each gene, but will want to use a uniform tree reconstruction method ( or several of them, but on all the datasets for each method).  \n",
      "  \n",
      "I plan to simplify the raxml command line by excluding keywards that are not related to tree reconstruction because other operations do not make sense, coming after sequence alignment in the reprophylo workflow (everything will be accesible, but not everything will be bundled into predesigned analyses)  \n",
      "The bundles will be either 'tree reconstruction' bundles, 'node support' bundels and 'both' bundles. It will be possible to add optional keywords to the bundes. In the description below I don't show all the keywards that will be included. e.g., -m (model) is always required and I don't write it here.\n",
      "\n",
      "\n",
      "\n",
      "# RAxML keyword blocks\n",
      "  \n",
      "## ML search combined with branch support\n",
      "*This section include one liners or a two liners that would be meaningless when ran individually*  \n",
      "\n",
      "#### Single best tree search With rappid bootstrap\n",
      "-f a -p 12345 -x 12345 -N (number or AUTO_MRE)  \n",
      "#### Multipe best tree searches With REL bootstrap\n",
      "-f D -p 12345 -N (number)  \n",
      "**then**  \n",
      "-f b -t RAxML_bestTree.name -z RAxML_rellBootstrap.name -m (model)  \n",
      "#### Fast tree with sh-like support and branch length\n",
      "-f F -p 12345  \n",
      "**then**  \n",
      "-f J -p 12345 -t RAxML_fastTree  \n",
      "*comment: if not followed by bootstrap analysis, do  use_sh_support_as_branch_support*\n",
      "#### Very fast tree with sh-like support and branch length\n",
      "-f E -p 12345  \n",
      "**then**  \n",
      "-f J -p 12345 -t RAxML_fastTree  \n",
      "*comment: if not followed by bootstrap analysis, do  use_sh_support_as_branch_support*  \n",
      "\n",
      "## Best tree search approaches\n",
      "*In liners that focus on thorogh best tree search, possibly with more than on randomized parsimony/ completely random starting tree*  \n",
      "  \n",
      "#### Best tree default method approach\n",
      "-f d -p 12345 -N (number)  \n",
      "#### Slow best tree search with better likelihood\n",
      "-f o -p 12345 -N (number)  \n",
      "  \n",
      "## Bipartition tree calculation approaches (calc supports and put on best tree)\n",
      "*Methods which focus on branch support calculation and their superimposition on a precalculated best tree or on an nni optimized fast tree with branch-lengths*  \n",
      "  \n",
      "#### Rapid bootstrap\n",
      "-x 12345 -N (number/autoMRE)  \n",
      "**then**  \n",
      "-f b -t RAxML_bestTree/RAxML_fastTreeSH_Support -z RAxML_Bootstrap -m (model)  \n",
      "####  Thorough bootstrap\n",
      "-b 12345 -N (number/autoMRE)  \n",
      "**then**  \n",
      "-f b -t RAxML_bestTree/RAxML_fastTreeSH_Support -z RAxML_Bootstrap -m (model)  \n",
      "  \n",
      "####  IC/TC approach\n",
      "*This approach is based on the level of incongruence between the concatenated tree and the gene trees. I tries replace bootstrap support or posterior probabilities which are inefficient when a lot of data is involved. \n",
      "Salichos and Rokas 2013 http://www.ncbi.nlm.nih.gov/pubmed/23657258  \n",
      "and http://mbe.oxfordjournals.org/content/early/2014/02/07/molbev.msu061.abstract*  \n",
      "  \n",
      "*Technically, from the reprophylo point of view, it differs from other approaches by the fact that it integrates all the gene trees and a concatenated tree, while other approches are performed indipendantly on each alignment. So it needs to pull all the trees calculated at one point and placed in the reprophylo database and intgrate them.*\n",
      "\n",
      "*The analysis is done by calling **-f i** and providing the concat tree to **-t** and the gene trees to **-z**. This will produce the concat tree with the IC and ICA scores as follows **(A,B):0.1325[IC,ICA]**. The scores will then need to be reformated as node features as I have done for the sh-like supports. There are many modifiers to this approach which I have to check carefully in the RAxML manual*  \n",
      "  \n",
      "## Parameter optimization appraches\n",
      "#### optimize br-len and other model parameters\n",
      "-f e -t (RAxML_fastTreeSH_Support or RAxML_fastTree or RAxML_bestTree)  \n",
      "*Comment: This will requlire the use of transfer_support_same_topo on the input and output trees*  \n",
      "  \n",
      "# General analysis modifiers\n",
      "*I excluded keywards that don't make sense in the tree reconstruction phase of reprophylo*  \n",
      "  \n",
      "A-scondary structure model, together with S  \n",
      "B-specify threshold for autoMR  \n",
      "c-specifiy num of CAT categories  \n",
      "d-start at random instead of parsimony randomized tree  \n",
      "D-ML search convergence criterion to stop the analysis (ever needed in raxml?)  \n",
      "e-parameter percision criterion  \n",
      "fI-ML based descision on midpoint for a tree passed with -t    \n",
      "ft-another way to search for a tree, not so sure  \n",
      "fT-optimization via through SPR moves  \n",
      "F-Just CAT, no GAMMA at all  \n",
      "g-constraint tree  \n",
      "H-No so sure  \n",
      "m-models, will need to be read from a control file   \n",
      "M-per partition branch length  \n",
      "n-derived from method.id  \n",
      "O-disable check of undetermined alignments  \n",
      "p-random seed for parsimony. set automatically  \n",
      "P-Used provided AA substitution model  \n",
      "q-partition file, created automatically  \n",
      "r-constraint tree, binary  \n",
      "s-input alignment  \n",
      "S-secondary structue  \n",
      "t-tree file  \n",
      "T-num of cores  \n",
      "u-use median instead of mean for GAMMA based rates  \n",
      "z-pass trees file  "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}